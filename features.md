| ID | Feature Description | Details | Implementation Notes |
|----|---------------------|---------|----------------------|
| F01 | Studio Environment Crafter | ✓ **COMPLETATO** Analisi approfondita dell'environment Crafter(https://github.com/danijar/crafter): meccaniche di gioco, sistema di obiettivi (22 achievements), spazio delle azioni, rappresentazione degli stati, dinamiche di sopravvivenza (cibo, acqua, riparo, combattimento) | **Feature Semantiche (Approccio 1)**: Estrazione da `info` dict Crafter (inventory, achievements, player_pos, semantic_view). ✓ API verificata. **Pro**: Zero GPU overhead, velocità. **Contro**: Meno spatial awareness. *Alternativa CNN nativo*: 10x lento, migliore feature learning. *Alternativa Vision Transformer*: 3-5x lento, transfer learning advantage. **Scelta**: Approccio 1 per prototipo veloce. |
| F02 | Implementazione Environment | ✓ **COMPLETATO** Integrazione e configurazione dell'environment Crafter come base per l'addestramento, setup delle interfacce e wrapper necessari | **Wrapper CrafterEnv** (`classes/crafter_environment.py`): Estrae feature vector (41 dims) da `env.step()` → `info` dict e `env._player`. Vettore: [inventario(13), pos(2), status(3), achievements(22), fence(1)]. Tutte 17 azioni sempre valide (no masking). Tested su 5 episodi × 100 steps. |
| F03 | Sviluppo NPC con DQN | ✓ **COMPLETATO** Implementazione dell'agente di Reinforcement Learning utilizzando Deep Q-Network: architettura della rete neurale, funzione di reward, replay buffer, strategia di esplorazione | **DQN parametrico** (`classes/agent.py`): `state_size` ora parametrico (default 41 per Crafter, compatibile con 36 per HeRoN battle). Architettura: Dense(128) → Dense(128) → Dense(64) → Dense(action_size). No CNN layers needed (feature semantiche). No masked Q-values (tutte azioni valide in Crafter). Sparse rewards (+1 achievement) richiederà reward shaping in F03 futuro. |
| F04 | Prompt Engineering Helper | ✓ **COMPLETATO**: Modifica delle chiamate all'Helper (LLM zero-shot) per generare sequenze di azioni coerenti invece di singole azioni, design dei prompt per il contesto di Crafter | **CrafterHelper** (`classes/crafter_helper.py`): Genera sequenze di 3-5 azioni con prompt engineering specifico per Crafter. `describe_crafter_state()` converte 41-dim state → human-readable format. `parse_action_sequence()` estrae actions multiple da response LLM. Re-planning trigger (Strategy B) dopo ogni step se achievements/salute/inventario cambiano significativamente. `SequenceExecutor` gestisce esecuzione sequenze con fallback a DQN. Test in `test_f04_helper.py`. |
| F05 | Dataset Generation per Reviewer | ✓ **COMPLETATO**: Creazione dataset contenente: stati dell'environment Crafter, sequenze di azioni suggerite dall'Helper, feedback correttivi basati sugli outcome | **CrafterDatasetGenerator** (`dataset Reviewer/crafter_dataset_generation.py`): Episodic data collector con 50-100 episodi, Helper call ogni 5 step, 500 step/episodio. OutcomeEvaluator con 5 criteri (achievement unlock, resource efficiency, health mgmt, tier progression, sequence coherence). FeedbackGenerator con hand-crafted rule-based feedback su 5 livelli. Export CSV: `prompt`, `response`, `instructions`, `quality_score`, `achievements_unlocked`. Target: 2000-5000 samples. |
| F06 | Fine-Tuning Reviewer | ✓ COMPLETATO: Addestramento del Reviewer (fine-tuned LLM) sul nuovo dataset tramite Reinforcement Learning per adattarlo ai task specifici di Crafter |
| F07 | Analisi Ottimale Lunghezza Sequenze | Determinazione del numero ottimale di azioni da suggerire per chiamata Helper (es. 2 chiamate × 5 mosse vs altre configurazioni) |
| F08 | Integrazione Architettura HeRoN | ✓ **COMPLETATO**: Integrazione completa dei tre componenti (NPC DQN, Helper LLM, Reviewer fine-tuned) nell'environment Crafter con gestione del flusso di comunicazione, reward shaping, e sequence execution con re-planning | **HeRoN/heron_crafter.py** (500+ linee): Training orchestration con three-agent loop. **Probability-Threshold Decay**: `threshold = max(0, threshold - 0.1)` per episode fino a episode 600, dopo disabilita LLM. Se `p > threshold`: Helper→Reviewer→Helper workflow, altrimenti DQN diretto. **Sequence Execution (Strategy B)**: `SequenceExecutor` gestisce 3-5 action sequences; `helper.should_replan()` interrompe su achievement unlock, salute critica, o deplezione risorse → fallback a DQN per azioni rimanenti. **Reward Shaping**: `CrafterRewardShaper` aggiunge bonuses intrinseci a native sparse rewards: +0.1 resource collection, +0.05 health/tier progression, +0.02 tool usage. Shaped reward = native + bonus total. **Metrics**: Rewards (native + shaped separati), achievements unlocked, helper calls, hallucination rate, moves, action scores per episode. Export CSV + 5 PNG plots (rewards, achievements, moves, helper stats, bonus breakdown). **Placeholder Reviewer**: Model paths commented con `TODO: Update after F06` - attualmente codice graceful fallback se modello non disponibile. **Backward Compatibility**: Crafter environment state (41-dim) fully compatible con DQN parametrico. |
| F09 | Addestramento Iterativo | ✓ **COMPLETATO**: Cicli di training iterativo del NPC con feedback continuo da Helper e Reviewer, ottimizzazione degli iperparametri | **Multi-Phase Training System** (`HeRoN/run_iterative_training.py`): Curriculum learning con 3 stage (Early: wood/stone, Mid: crafting/tools, Late: combat/advanced). Progressive episode length 500→2000. **Hyperparameter Scheduling**: Learning rate decay (step/exponential/cosine), epsilon decay (linear/exponential/staged), threshold decay (linear/staged). **Performance-Based Checkpointing**: Best model tracking, periodic checkpoints every 10 episodes. **Early Stopping**: Patience=100, convergence detection. **Adaptive Reward Shaping**: Weights adjust by curriculum stage and achievement rate. **Iterative Refinement Cycle** (`HeRoN/iterative_training.py`): Dataset generation → Reviewer fine-tuning → HeRoN training → Evaluation loop for 3-5 iterations. **CurriculumManager** (`HeRoN/curriculum_manager.py`): Stage-based achievement targets, episode length progression, adaptive weight adjustment. **Critical Fixes**: Epsilon init 1.0→0.01, threshold decay per-episode (not per-step), relative paths for checkpoints. Export: hyperparameter_history.csv, training_config.json, iteration_comparison_report.md. |
| F10 | Sistema di Valutazione | ✓ COMPLETATO: Implementazione metriche di performance: score totale, achievements sbloccati, analisi per singolo obiettivo, confronto con baseline |
| F11 | Testing e Benchmark | Valutazione sistematica delle prestazioni dell'NPC nei 22 obiettivi di Crafter, confronto con agenti baseline e analisi statistica |
| F12 | Validazione Reviewer | Verifica dell'efficacia del Reviewer nel fornire feedback mirati e migliorativi, analisi qualitativa dei suggerimenti |
| F13 | Analisi Risultati | Documentazione dei risultati: miglioramenti ottenuti, limiti identificati, sfide affrontate e soluzioni implementate |
| F14 | Documentazione Tecnica | Stesura documentazione completa: architettura, decisioni implementative, esperimenti condotti, risultati ottenuti |
