\chapter{Architettura HeRoN}

\section{Panoramica dell'Architettura}

HeRoN (Helper-Reviewer-NPC) è un'architettura multi-agente che combina Reinforcement Learning e Large Language Model per migliorare il processo decisionale di agenti intelligenti in ambienti interattivi. L'idea di fondo consiste nell'unire la capacità del Reinforcement Learning di ottimizzare strategie attraverso prove ed errori, il ragionamento semantico e la conoscenza generale dei Large Language Model, e un meccanismo di feedback iterativo per migliorare i suggerimenti.

\subsection{Diagramma Architettura DQN Baseline}

Prima di descrivere l'architettura completa HeRoN, viene presentata l'architettura baseline DQN utilizzata come riferimento per il confronto con l'integrazione LLM.


\textbf{Flusso Operativo DQN}:
\begin{enumerate}
    \item \textbf{Percezione}: Ambiente → Estrazione dello stato (vettore 43-dim)
    \item \textbf{Decisione}: Rete DQN → Q-values → selezione $\epsilon$-greedy
    \item \textbf{Azione}: Esegui azione $a_t$, osserva $r_t, s_{t+1}$
    \item \textbf{Memorizzazione}: Salva $(s_t, a_t, r_{shaped}, s_{t+1}, done)$ in Prioritized Replay
    \item \textbf{Apprendimento}: Campiona batch → calcola TD-loss → aggiorna pesi DQN
    \item \textbf{Stabilizzazione}: Ogni 100 passi, copia pesi DQN → Rete Target
\end{enumerate}


L'architettura DQN baseline apprende esclusivamente tramite interazione diretta con l'ambiente, senza supporto esterno.

\subsection{Diagramma Architettura HeRoN Completa}


L'architettura HeRoN rappresenta un'estensione del DQN baseline, con l'aggiunta di due componenti LLM per la guida strategica e un meccanismo di \textbf{threshold decay} che bilancia l'intervento tra LLM e RL.

\textbf{Meccanismo Threshold Decay}:

Il threshold $\theta$ controlla quando consultare il LLM anziché usare DQN autonomo. Nel progetto sono state implementate tre strategie di attivazione LLM:

\begin{enumerate}
    \item \textbf{HeRoN Initial}: LLM attivo solo nei primi 100 step di ogni episodio (finestra temporale fissa)
    \item \textbf{HeRoN Random}: LLM con probabilità casuale del 50\% ad ogni step (attivazione stocastica)
    \item \textbf{HeRoN Final}: Threshold decay adattivo $\theta(t) = \max(0, 1.0 - k \times t)$ con $k=0.01$ (probabilità LLM crescente 0\%→100\% durante ogni episodio)
\end{enumerate}

Tutte le configurazioni includono un \textbf{cutoff a episodio 100}: dopo i primi 100 episodi, il DQN diventa completamente autonomo.

\textbf{Flusso Decisionale Integrato}:
\begin{enumerate}
    \item Ambiente genera stato → Estrazione dello stato (43-dim)
    \item Threshold check: random(0.73) $>$ threshold(0.65) → \textbf{LLM Path}
    \item Helper riceve descrizione dello stato → genera \texttt{[move\_right], [do],} \\
    \texttt{[move\_left], [do], [noop]}
    \item Reviewer analizza → feedback: \textit{"Health low, prioritize eating"}
    \item Helper re-query con feedback → sequenza raffinata: \texttt{[sleep],} \\
    \texttt{[move\_right], [do], [move\_left], [noop]}
    \item Action Executor esegue \texttt{[sleep]} → $(s_1, r_1, info_1)$
    \item Salva $(s_0, sleep, r_1, s_1, done)$ in Prioritized Replay
    \item Monitor: achievement unlocked? → \textbf{SÌ} → interrompi sequenza, nuova query Helper
    \item DQN training: sample batch → compute loss → aggiorna pesi
\end{enumerate}


L'architettura HeRoN integra i vantaggi del Reinforcement Learning (apprendimento da esperienza) e dei Large Language Model (conoscenza a priori e ragionamento strategico), con una transizione graduale verso l'autonomia dell'agente.

L'architettura HeRoN è composta da tre componenti principali che interagiscono in modo coordinato:

\subsection{NPC (Non-Player Character)}

L'NPC è l'agente che gioca in Crafter usando Reinforcement Learning. In questo progetto, l'NPC implementa l'algoritmo \textbf{Deep Q-Network (DQN)} con le seguenti caratteristiche:

\begin{itemize}
    \item \textbf{Architettura}: Rete neurale feedforward a 3 hidden layers (43-128-128-64-17) per mappare stati a Q-values
    \item \textbf{Double DQN}: Due reti distinte (policy network e target network) per stabilizzare l'apprendimento
    \item \textbf{Prioritized Experience Replay}: Campionamento intelligente delle esperienze passate basato su TD-error
    \item \textbf{Funzionamento}: L'NPC osserva lo stato (43-dim), seleziona un'azione tramite strategia $\epsilon$-greedy, esegue l'azione, riceve reward e aggiorna i pesi della rete neurale
\end{itemize}


\subsection{Helper}

Il componente Helper è un Large Language Model utilizzato in modalità zero-shot che fornisce suggerimenti strategici all'NPC. Nel progetto HeRoN per Crafter, l'Helper si implementa utilizzando un LLM locale (Qwen3-4B-2507) attraverso LM Studio con le seguenti caratteristiche:

\begin{itemize}
    \item \textbf{Generazione di sequenze di azioni}: Diversamente dall'implementazione originale che suggeriva singole azioni, l'Helper in questo progetto genera sequenze di 3-5 azioni coerenti da eseguire una dopo l'altra.
    
    \item \textbf{Contestualizzazione}: L'Helper riceve informazioni dettagliate circa lo stato corrente del gioco:
    \begin{itemize}
        \item Inventario del giocatore (16 item)
        \item Posizione corrente
        \item Statistiche vitali (salute, cibo, acqua)
        \item Achievement sbloccati (22 possibili)
    \end{itemize}
    
    \item \textbf{Prompt Engineering}: Il prompt si presenta come specificatamente progettato per Crafter e include:
    \begin{itemize}
        \item Descrizione del contesto di gioco
        \item Stato corrente dell'agente
        \item Lista delle azioni disponibili
        \item Richiesta di generare una sequenza strategica
    \end{itemize}
    L'Helper risponde con una sequenza di azioni nel formato:
    \begin{verbatim}
    [azione_1], [azione_2], [azione_3], [azione_4], [azione_5]
    \end{verbatim}
    Ad esempio:
    \begin{verbatim}
    [move_right], [do], [move_left], [do], [noop]
    \end{verbatim}
\end{itemize}

\subsection{Reviewer}

Il componente Reviewer è un LLM fine-tuned (basato su T5) che valuta i suggerimenti forniti dall'Helper e genera feedback per migliorarli. Come descritto in dettaglio nel Capitolo~4, il Reviewer si addestra specificamente per il contesto di Crafter su un dataset composto da 150 episodi di gioco, contenente circa 15.000 esempi di coppie (suggerimento, feedback).

Il Reviewer analizza:
\begin{itemize}
    \item Coerenza della sequenza di azioni suggerite
    \item Appropriatezza rispetto allo stato corrente
    \item Potenziali rischi o inefficienze
    \item Priorità strategiche (es. sopravvivenza vs. progressione)
    \item Fornisce feedback strutturato che si utilizza per ri-interrogare l'Helper con più informazioni.
\end{itemize}

\subsection{Gestione del Contesto}

Durante ogni episodio, l'Helper mantiene uno stato conversazionale persistente (\texttt{message\_history}) che accumula descrizioni dello stato di gioco, sequenze di azioni generate, feedback del Reviewer e contesto di gioco (posizione, inventario, achievement). Questa memoria conversazionale consente all'Helper di mantenere coerenza e contestualizzazione lungo l'episodio, influenzando direttamente la qualità dei suggerimenti generati.

La cronologia consente al LLM di mantenere coerenza logica tra azioni successive e di comprendere l'evoluzione dello stato di gioco all'interno dell'episodio.

\subsection{Gestione Intelligente del Contesto (Token-Aware)}

L'Helper implementa un sistema di gestione intelligente del contesto per prevenire l'overflow della finestra di contesto del modello LLM (Qwen3-4B-2507 ha 8192 token di limite):

\begin{enumerate}
    \item \textbf{Monitoraggio token}: L'Helper conta il numero di token nella cronologia utilizzando il tokenizer Qwen2.5 (compatibile con Qwen3)
    
    \item \textbf{Soglia di sicurezza}: Quando il contesto raggiunge 6500 token (80\% del limite), viene attivato un reset intelligente per evitare crash e risposte vuote
    
    \item \textbf{Reset con riassunto episodio}: Invece di scartare tutto il contesto, l'Helper genera un riassunto che include:
    \begin{itemize}
        \item Numero di step eseguiti nell'episodio
        \item Reward totale accumulato
        \item Lista degli achievement sbloccati
        \item Feedback recente del Reviewer (se disponibile)
        \item Descrizione dello stato di gioco corrente
    \end{itemize}
    Questo riassunto diventa il nuovo inizio della cronologia, preservando informazioni strategiche critiche.
\end{enumerate}

\textbf{Vantaggi del reset intelligente}:
\begin{itemize}
    \item \textbf{Continuità strategica}: Il modello comprende il progresso episodico complessivo
    \item \textbf{Efficienza token}: Riduce i token inutili mantenendo informazioni essenziali
    \item \textbf{Riduzione allucinazioni}: Contesto pulito riduce risposte errate o non coerenti
    \item \textbf{Maggiore lunghezza episodio}: Consente episodi più lunghi senza crash
\end{itemize}

\subsection{Meccanismi di Re-planning e Aggiornamento Contesto}

Durante l'esecuzione di una sequenza di azioni, l'Helper monitora determinati eventi per aggiornare intelligentemente il contesto:

\textbf{Trigger di Re-query (Interruzione Sequenza)}:
\begin{itemize}
    \item \textbf{Achievement sbloccato}: Quando il giocatore sblocca un nuovo achievement, l'Helper riceve una nuova query con il contesto aggiornato che include il nuovo achievement nel set di quelli sbloccati
    
    \item \textbf{Salute critica} ($health \leq 5$): Se la salute scende sotto soglia critica, la sequenza viene interrotta e l'Helper si consulta per suggerire azioni di emergenza (mangiare, bere, dormire)
    
    \item \textbf{Salute bassa} ($health < 30\%$): Se la salute è bassa ma non critica, si consulta l'Helper per bilanciare l'esplorazione con la gestione della sopravvivenza
    
    \item \textbf{Risorsa completamente consumata}: Se una risorsa chiave (legno, pietra, carbone) raggiunge 0, viene attivata una nuova query per raccoglierla prioritariamente
\end{itemize}

\subsection{Reset Episodio e Pulizia Contesto}

All'inizio di ogni nuovo episodio, l'Helper esegue una pulizia completa:
\begin{itemize}
    \item Cancellazione della cronologia messaggi (\texttt{message\_history = []})
    \item Reset del tracciamento achievement episodio
    \item Azzeramento del feedback Reviewer recente
    \item Pulizia della cronologia delle sequenze (usata per rilevare loop)
\end{itemize}

Questo previene l'accumulo di contesto da episodi precedenti.

\section{Vantaggi dell'Architettura}

L'architettura HeRoN combina RL e LLM offrendo:
\begin{itemize}
    \item \textbf{Conoscenza a priori}: LLM accelera l'apprendimento con conoscenze generali
    \item \textbf{Ragionamento strategico}: Pianificazione di azioni coerenti a lungo termine
    \item \textbf{Adattabilità}: Unisce esplorazione RL e suggerimenti LLM per nuove situazioni
    \item \textbf{Interpretabilità}: Sequenze di azioni analizzabili per capire la strategia
    \item \textbf{Raffinamento iterativo}: Helper e Reviewer migliorano la qualità dei suggerimenti
\end{itemize}

\section{Sfide dell'Integrazione}

Le principali difficoltà nell’integrazione RL-LLM sono:
\begin{itemize}
    \item \textbf{Overhead computazionale}: LLM più costosi rispetto al DQN
    \item \textbf{Parsing delle risposte}: Gestione di risposte errate o non valide
    \item \textbf{Bilanciamento}: Equilibrio tra dipendenza da LLM e autonomia RL
    \item \textbf{Consistenza}: Garantire sequenze eseguibili e coerenti
\end{itemize}

