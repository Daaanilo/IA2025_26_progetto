\chapter{Architettura HeRoN}

\section{Panoramica dell'Architettura}

HeRoN (Helper-Reviewer-NPC) è un'architettura multi-agente che combina Reinforcement Learning e Large Language Model per migliorare il processo decisionale di agenti intelligenti in ambienti interattivi. L'idea di base è unire la capacità del Reinforcement Learning di ottimizzare strategie attraverso prove ed errori, il ragionamento semantico e la conoscenza generale dei Large Language Model, e un meccanismo di feedback iterativo per migliorare i suggerimenti.

\subsection{Diagramma Architettura DQN Baseline}

Prima di descrivere l'architettura completa HeRoN, parliamo dell'architettura baseline DQN che usiamo come base e per confrontare i benefici dell'integrazione con LLM.


\textbf{Flusso Operativo DQN}:
\begin{enumerate}
    \item \textbf{Percezione}: Environment → State Extraction (43-dim vector)
    \item \textbf{Decisione}: DQN Network → Q-values → $\epsilon$-greedy selection
    \item \textbf{Azione}: Execute action $a_t$, observe $r_t, s_{t+1}$
    \item \textbf{Memorizzazione}: Salva $(s_t, a_t, r_{shaped}, s_{t+1}, done)$ in Prioritized Replay
    \item \textbf{Apprendimento}: Sample batch → compute TD-loss → update DQN weights
    \item \textbf{Stabilizzazione}: Ogni 100 steps, copia DQN weights → Target Network
\end{enumerate}

L'architettura DQN baseline impara solo attraverso prove ed errori, senza aiuto esterno.

\subsection{Diagramma Architettura HeRoN Completa}

L'architettura HeRoN estende il DQN baseline aggiungendo due componenti LLM per una guida strategica, usando un meccanismo di threshold decay che bilancia LLM e RL.


\textbf{Meccanismo Chiave - Threshold Decay}:

Il threshold $\theta$ controlla la probabilità di consultare LLM vs. usare DQN autonomo:
\begin{equation}
\theta(e) = \max(0, 1.0 - 0.01 \times e)
\end{equation}

dove $e$ è il numero dell'episodio corrente. Questo garantisce:
\begin{itemize}
    \item \textbf{Episodi 0-100}: $\theta$: 1.0 → 0.0, transizione graduale da 100\% LLM a 0\% LLM
    \item \textbf{Episodi 100+}: $\theta = 0$, DQN completamente autonomo
    \item \textbf{Vantaggio}: LLM guida nelle fasi iniziali quando DQN è inesperto, poi DQN diventa indipendente
\end{itemize}

\textbf{Flusso Completo (Esempio con LLM Path)}:
\begin{enumerate}
    \item Environment genera stato → State Extraction (43-dim)
    \item Threshold check: random(0.73) $>$ threshold(0.65) → \textbf{LLM Path}
    \item Helper riceve state description → genera \texttt{[move\_right], [do],} \\
    \texttt{[move\_left], [do], [noop]}
    \item Reviewer analizza → feedback: \textit{"Health low, prioritize eating"}
    \item Helper re-query con feedback → sequenza raffinata: \texttt{[eat\_plant], [sleep],} \\
    \texttt{[move\_right], [do], [noop]}
    \item Action Executor esegue \texttt{[eat\_plant]} → $(s_1, r_1, info_1)$
    \item Salva $(s_0, eat\_plant, r_1, s_1, done)$ in Prioritized Replay
    \item Monitor: achievement unlocked? → \textbf{SÌ} → interrompi sequenza, nuova query Helper
    \item DQN training: sample batch → compute loss → update weights
\end{enumerate}

L'architettura HeRoN combina il meglio di RL (apprendimento da esperienza) e LLM (conoscenza a priori + ragionamento strategico), con una transizione graduale verso autonomia completa.

L'architettura HeRoN è composta da tre componenti principali che interagiscono in modo coordinato:

\subsection{NPC (Non-Player Character)}

Il componente NPC è un agente di Reinforcement Learning che rappresenta il "giocatore" nell'environment. Nel contesto di questo progetto, l'NPC è implementato utilizzando l'algoritmo Deep Q-Network (DQN) con le seguenti caratteristiche:

\begin{itemize}
    \item \textbf{Architettura}: Usa una versione chiamata Double DQN con una rete (target network) per rendere l’apprendimento più stabile.
    \item \textbf{Replay Buffer}: Memorizza le esperienze passate dando priorità alle esperienze importanti, con spazio per 10.000 eventi.
    \item \textbf{Parametri}:
    \begin{itemize}
        \item $\alpha = 0.6$: quanto diamo priorità alle esperienze importanti
        \item $\beta = 0.4 \rightarrow 1.0$: peso per correggere il campionamento delle esperienze
        \item $\gamma = 0.99$: quanto valutiamo l'importanza delle ricompense future
        \item $\epsilon = 1.0 \rightarrow 0.05$: probabilità di esplorare nuove azioni, che diminuisce col tempo
    \end{itemize}
    \item \textbf{Input}: Stato dell'ambiente a 43 dimensioni
    \item \textbf{Output}: Q-values per 17 azioni possibili
\end{itemize}

L'NPC impara attraverso l'esperienza diretta, accumulando transizioni $(s, a, r, s')$ nel replay buffer e aggiornando i pesi della rete neurale per massimizzare il reward atteso.

\subsection{Helper}

Il componente Helper è un Large Language Model utilizzato in modalità zero-shot che fornisce suggerimenti strategici all'NPC. Nel progetto HeRoN per Crafter, l'Helper è implementato utilizzando un LLM locale (Qwen3-4B-2507) attraverso LM Studio con le seguenti caratteristiche:

\begin{itemize}
    \item \textbf{Generazione di sequenze di azioni}: Diversamente dall'implementazione originale che suggeriva singole azioni, l'Helper in questo progetto genera sequenze di 3-5 azioni coerenti da eseguire una dopo l'altra.
    
    \item \textbf{Contestualizzazione}: L'Helper riceve informazioni dettagliate sullo stato corrente del gioco:
    \begin{itemize}
        \item Inventario del giocatore (16 item)
        \item Posizione corrente
        \item Statistiche vitali (salute, cibo, acqua)
        \item Achievement sbloccati (22 possibili)
    \end{itemize}
    
    \item \textbf{Prompt Engineering}: Il prompt è stato specificamente progettato per Crafter e include:
    \begin{itemize}
        \item Descrizione del contesto di gioco
        \item Stato corrente dell'agente
        \item Lista delle azioni disponibili
        \item Richiesta di generare una sequenza strategica
    \end{itemize}
    L'Helper risponde con una sequenza di azioni nel formato:
    \begin{verbatim}
    [azione_1], [azione_2], [azione_3], [azione_4], [azione_5]
    \end{verbatim}
    Ad esempio:
    \begin{verbatim}
    [move_right], [do], [move_left], [do], [noop]
    \end{verbatim}
\end{itemize}

\subsection{Reviewer}

Il componente Reviewer è un LLM fine-tuned (basato su T5) che valuta i suggerimenti forniti dall'Helper e genera feedback per migliorarli. Il Reviewer è stato addestrato specificamente per il contesto di Crafter utilizzando un dataset generato attraverso:
\begin{enumerate}
    \item Raccolta di stati dell'environment durante sessioni di gioco
    \item Generazione di suggerimenti dall'Helper per ciascuno stato
    \item Annotazione manuale o semi-automatica di feedback correttivi
    \item Fine-tuning del modello T5 su coppie (suggerimento, feedback)
\end{enumerate}

Il dataset contiene circa 2,500 esempi raccolti da 50 episodi di gioco.

Il Reviewer analizza:
\begin{itemize}
    \item Coerenza della sequenza di azioni suggerite
    \item Appropriatezza rispetto allo stato corrente
    \item Potenziali rischi o inefficienze
    \item Priorità strategiche (es. sopravvivenza vs. progressione)
    \item Fornisce feedback strutturato che usiamo per ri-interrogare l'Helper con più informazioni.
\end{itemize}

\section{Workflow dell'Architettura}

Il workflow di HeRoN durante il training si articola in queste fasi:

\subsection{Fase 1: Decisione di Consultazione}

Ad ogni step dell'episodio, l'architettura decide se consultare i componenti LLM basandosi su una soglia dinamica $\theta$.

%\begin{algorithm}
%\caption{Decisione di consultazione LLM}
%\begin{algorithmic}
%\IF{$random() > \theta$ AND $episode < 600$}
%    \STATE Procedi con workflow LLM
%\ELSE
%    \STATE Usa solo DQN: $a = \arg\max_a Q(s, a)$
%\ENDIF
%\end{algorithmic}
%\end{algorithm}

Quando decidiamo di consultare l'LLM, l'Helper genera una sequenza di azioni basandosi sullo stato corrente.

La soglia $\theta$ decresce linearmente da 1.0 a 0.0 nel corso di 100 episodi:
$$\theta_t = \max(0, \theta_0 - 0.01 \cdot episode)$$

In questo modo, l'NPC si affida di più ai suggerimenti LLM all'inizio dell'allenamento, per poi ridurre questa dipendenza man mano che l'agente RL migliora.

\subsection{Fase 2: Review e Raffinamento (Reviewer)}

Se il Reviewer è disponibile, valuta la sequenza proposta e fornisce un feedback.
L'Helper usa il feedback per migliorare la sequenza e ne crea una seconda versione.
Infine, usiamo la sequenza migliorata se il Reviewer ha dato feedback, altrimenti la prima.

\subsection{Fase 3: Esecuzione e Re-planning}

La sequenza di azioni viene eseguita una dopo l'altra, ma può essere interrotta in caso di eventi importanti:

\begin{itemize}
    \item \textbf{Achievement sbloccato}: Nuova consultazione LLM con contesto aggiornato
    \item \textbf{Salute critica} ($health \leq 5$): Fallback immediato a DQN per azioni di sopravvivenza
    \item \textbf{Salute bassa} ($health < 30\%$): Ri-query del sistema per gestione prioritaria della salute
\end{itemize}

\section{Vantaggi dell'Architettura}

L'architettura HeRoN combina RL e LLM offrendo:
\begin{itemize}
    \item \textbf{Conoscenza a priori}: LLM accelera l'apprendimento con conoscenze generali
    \item \textbf{Ragionamento strategico}: Pianificazione di azioni coerenti a lungo termine
    \item \textbf{Adattabilità}: Unisce esplorazione RL e suggerimenti LLM per nuove situazioni
    \item \textbf{Interpretabilità}: Sequenze di azioni analizzabili per capire la strategia
    \item \textbf{Raffinamento iterativo}: Helper e Reviewer migliorano la qualità dei suggerimenti
\end{itemize}

\section{Sfide dell'Integrazione}

Le principali difficoltà nell’integrazione RL-LLM sono:
\begin{itemize}
    \item \textbf{Overhead computazionale}: LLM più costosi rispetto al DQN
    \item \textbf{Parsing delle risposte}: Gestione di risposte errate o non valide
    \item \textbf{Bilanciamento}: Equilibrio tra dipendenza da LLM e autonomia RL
    \item \textbf{Consistenza}: Garantire sequenze eseguibili e coerenti
\end{itemize}

