\section{Architettura HeRoN}

\subsection{Panoramica dell'Architettura}

HeRoN (Helper-Reviewer-NPC) è un'architettura multi-agente che integra Reinforcement Learning e Large Language Models per migliorare il processo decisionale di agenti intelligenti in ambienti interattivi. L'idea centrale di HeRoN consiste nel combinare la capacità di apprendimento per rinforzo di ottimizzare strategie attraverso trial-and-error, il ragionamento semantico e la conoscenza generale dei Large Language Models, e un meccanismo di feedback iterativo per raffinare i suggerimenti forniti.

L'architettura HeRoN è composta da tre componenti principali che interagiscono in modo coordinato:

\subsubsection{NPC (Non-Player Character)}

Il componente NPC è un agente di Reinforcement Learning che rappresenta il "giocatore" nell'environment. Nel contesto di questo progetto, l'NPC è implementato utilizzando l'algoritmo Deep Q-Network (DQN) con le seguenti caratteristiche:

\begin{itemize}
    \item \textbf{Architettura}: Usa una versione chiamata Double DQN con una rete (target network) per rendere l’apprendimento più stabile.
    \item \textbf{Replay Buffer}: Memorizza le esperienze passate dando priorità alle esperienze importanti, con spazio per 10.000 eventi.
    \item \textbf{Parametri}:
    \begin{itemize}
        \item $\alpha = 0.6$: indica quanto si dà priorità alle esperienze importanti
        \item $\beta = 0.4 \rightarrow 1.0$: peso per correggere il campionamento delle esperienze
        \item $\gamma = 0.99$: fattore che valuta l'importanza delle ricompense future
        \item $\epsilon = 1.0 \rightarrow 0.05$: probabilità di esplorare nuove azioni, che diminuisce nel tempo
    \end{itemize}
    \item \textbf{Input}: Stato dell'ambiente a 43 dimensioni
    \item \textbf{Output}: Q-values per 17 azioni possibili
\end{itemize}

L'NPC apprende attraverso l'esperienza diretta, accumulando transizioni $(s, a, r, s')$ nel replay buffer e aggiornando i pesi della rete neurale per massimizzare il reward atteso.

\subsubsection{Helper}

Il componente Helper è un Large Language Model utilizzato in modalità zero-shot che fornisce suggerimenti strategici all'NPC. Nel progetto HeRoN per Crafter, l'Helper è implementato utilizzando un LLM locale (Llama-3.2-3B-Instruct) attraverso LM Studio con le seguenti caratteristiche:

\begin{itemize}
    \item \textbf{Generazione di sequenze di azioni}: A differenza dell'implementazione originale che suggeriva singole azioni, l'Helper in questo progetto genera sequenze di 3-5 azioni coerenti da eseguire sequenzialmente.
    
    \item \textbf{Contestualizzazione}: L'Helper riceve informazioni dettagliate sullo stato corrente del gioco:
    \begin{itemize}
        \item Inventario del giocatore (16 item)
        \item Posizione corrente
        \item Statistiche vitali (salute, cibo, acqua)
        \item Achievement sbloccati (22 possibili)
    \end{itemize}
    
    \item \textbf{Prompt Engineering}: Il prompt è stato specificamente progettato per Crafter e include:
    \begin{itemize}
        \item Descrizione del contesto di gioco
        \item Stato corrente dell'agente
        \item Lista delle azioni disponibili
        \item Richiesta di generare una sequenza strategica
    \end{itemize}
    L'Helper risponde con una sequenza di azioni nel formato:
    \begin{verbatim}
    [azione_1], [azione_2], [azione_3],
    [azione_4], [azione_5]
    \end{verbatim}
    Ad esempio:
    \begin{verbatim}
    [move_right], [do], [move_left], [do],
    [noop]
    \end{verbatim}
\end{itemize}

\subsubsection{Reviewer}

Il componente Reviewer è un LLM fine-tuned (basato su T5) che valuta i suggerimenti forniti dall'Helper e genera feedback per migliorarli. Il Reviewer è stato addestrato specificamente per il contesto di Crafter utilizzando un dataset generato attraverso:
\begin{enumerate}
    \item Raccolta di stati dell'environment durante sessioni di gioco
    \item Generazione di suggerimenti dall'Helper per ciascuno stato
    \item Annotazione manuale o semi-automatica di feedback correttivi
    \item Fine-tuning del modello T5 su coppie (suggerimento, feedback)
\end{enumerate}

Il dataset tipicamente contiene circa 2,500 esempi raccolti da 50 episodi di gioco.

Il Reviewer analizza:
\begin{itemize}
    \item Coerenza della sequenza di azioni suggerite
    \item Appropriatezza rispetto allo stato corrente
    \item Potenziali rischi o inefficienze
    \item Priorità strategiche (es. sopravvivenza vs. progressione)
    \item Fornisce feedback strutturato che viene utilizzato per ri-interrogare l'Helper con maggiori informazioni.
\end{itemize}

\subsection{Workflow dell'Architettura}

Il workflow di HeRoN durante il training si articola nelle seguenti fasi:

\subsubsection{Fase 1: Decisione di Consultazione}

Ad ogni step dell'episodio, l'architettura decide se consultare i componenti LLM in base a una soglia dinamica $\theta$.

%\begin{algorithm}
%\caption{Decisione di consultazione LLM}
%\begin{algorithmic}
%\IF{$random() > \theta$ AND $episode < 600$}
%    \STATE Procedi con workflow LLM
%\ELSE
%    \STATE Usa solo DQN: $a = \arg\max_a Q(s, a)$
%\ENDIF
%\end{algorithmic}
%\end{algorithm}

Quando viene deciso di consultare l'LLM, l'Helper genera una sequenza di azioni basandosi sullo stato corrente.

La soglia $\theta$ decresce linearmente da 1.0 a 0.0 nel corso di 100 episodi:
$$\theta_t = \max(0, \theta_0 - 0.01 \cdot episode)$$

In questo modo, l’NPC si affida maggiormente ai suggerimenti LLM all’inizio dell’allenamento, per poi diminuire questa dipendenza man mano che l’agente RL migliora.

\subsubsection{Fase 2: Review e Raffinamento (Reviewer)}

Se il Reviewer è disponibile, valuta la sequenza proposta fornisce un feedback.
L’Helper usa il feedback per migliorare la sequenza e ne crea una seconda versione.
Infine, si usa la sequenza migliorata se il Reviewer ha dato feedback, altrimenti si usa la prima.

\subsubsection{Fase 3: Esecuzione e Re-planning}

La sequenza di azioni viene eseguita sequenzialmente, ma può essere interrotta in caso di eventi significativi:

\begin{itemize}
    \item \textbf{Achievement sbloccato}: Nuova consultazione LLM con contesto aggiornato
    \item \textbf{Salute critica} ($health \leq 5$): Fallback immediato a DQN per azioni di sopravvivenza
    \item \textbf{Salute bassa} ($health < 30\%$): Ri-query del sistema per gestione prioritaria della salute
\end{itemize}

\subsection{Vantaggi dell'Architettura}

L'architettura HeRoN combina RL e LLM offrendo:
\begin{itemize}
    \item \textbf{Conoscenza a priori}: LLM accelera l'apprendimento con conoscenze generali
    \item \textbf{Ragionamento strategico}: Pianificazione di azioni coerenti a lungo termine
    \item \textbf{Adattabilità}: Unisce esplorazione RL e suggerimenti LLM per nuove situazioni
    \item \textbf{Interpretabilità}: Sequenze di azioni analizzabili per capire la strategia
    \item \textbf{Raffinamento iterativo}: Helper e Reviewer migliorano la qualità dei suggerimenti
\end{itemize}

\subsection{Sfide dell'Integrazione}

Le principali difficoltà nell’integrazione RL-LLM sono:
\begin{itemize}
    \item \textbf{Overhead computazionale}: LLM più costosi rispetto al DQN
    \item \textbf{Parsing delle risposte}: Gestione di risposte errate o non valide
    \item \textbf{Bilanciamento}: Equilibrio tra dipendenza da LLM e autonomia RL
    \item \textbf{Consistenza}: Garantire sequenze eseguibili e coerenti
\end{itemize}

