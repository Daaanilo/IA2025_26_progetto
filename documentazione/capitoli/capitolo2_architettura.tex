\chapter{Architettura HeRoN}

\section{Panoramica dell'Architettura}

HeRoN (Helper-Reviewer-NPC) è un'architettura multi-agente che combina Reinforcement Learning e Large Language Model per migliorare il processo decisionale di agenti intelligenti in ambienti interattivi. L'idea di fondo consiste nell'unire la capacità del Reinforcement Learning di ottimizzare strategie attraverso prove ed errori, il ragionamento semantico e la conoscenza generale dei Large Language Model, e un meccanismo di feedback iterativo per migliorare i suggerimenti.

\subsection{Diagramma Architettura DQN Baseline}

Prima di descrivere l'architettura completa HeRoN, viene presentata l'architettura baseline DQN utilizzata come riferimento per il confronto con l'integrazione LLM.


\textbf{Flusso Operativo DQN}:
\begin{enumerate}
    \item \textbf{Percezione}: Ambiente → Estrazione dello stato (vettore 43-dim)
    \item \textbf{Decisione}: Rete DQN → Q-values → selezione $\epsilon$-greedy
    \item \textbf{Azione}: Esegui azione $a_t$, osserva $r_t, s_{t+1}$
    \item \textbf{Memorizzazione}: Salva $(s_t, a_t, r_{shaped}, s_{t+1}, done)$ in Prioritized Replay
    \item \textbf{Apprendimento}: Campiona batch → calcola TD-loss → aggiorna pesi DQN
    \item \textbf{Stabilizzazione}: Ogni 100 passi, copia pesi DQN → Rete Target
\end{enumerate}


L'architettura DQN baseline apprende esclusivamente tramite interazione diretta con l'ambiente, senza supporto esterno.

\subsection{Diagramma Architettura HeRoN Completa}


L'architettura HeRoN rappresenta un'estensione del DQN baseline, con l'aggiunta di due componenti LLM per la guida strategica, e l'utilizzo di un meccanismo di threshold decay che bilancia l'intervento tra LLM e RL.


\textbf{Meccanismo Chiave - Threshold Decay}:

Il threshold $\theta$ controlla la probabilità di consultare LLM vs. utilizzare DQN autonomo:
\begin{equation}
\theta(e) = \max(0, 1.0 - 0.01 \times e)
\end{equation}

dove $e$ è il numero dell'episodio corrente. Questo garantisce:
\begin{itemize}
    \item \textbf{Episodi 0-100}: $\theta$: 1.0 → 0.0, transizione graduale da 100\% LLM a 0\% LLM
    \item \textbf{Episodi 100+}: $\theta = 0$, DQN completamente autonomo
    \item \textbf{Vantaggio}: Nelle fasi iniziali, l'intervento LLM supporta il processo decisionale, mentre nelle fasi successive il DQN opera in modo autonomo
\end{itemize}

\textbf{Flusso Completo (Esempio con LLM Path)}:
\begin{enumerate}
    \item Ambiente genera stato → Estrazione dello stato (43-dim)
    \item Threshold check: random(0.73) $>$ threshold(0.65) → \textbf{LLM Path}
    \item Helper riceve descrizione dello stato → genera \texttt{[move\_right], [do],} \\
    \texttt{[move\_left], [do], [noop]}
    \item Reviewer analizza → feedback: \textit{"Health low, prioritize eating"}
    \item Helper re-query con feedback → sequenza raffinata: \texttt{[sleep],} \\
    \texttt{[move\_right], [do], [move\_left], [noop]}
    \item Action Executor esegue \texttt{[sleep]} → $(s_1, r_1, info_1)$
    \item Salva $(s_0, sleep, r_1, s_1, done)$ in Prioritized Replay
    \item Monitor: achievement unlocked? → \textbf{SÌ} → interrompi sequenza, nuova query Helper
    \item DQN training: sample batch → compute loss → aggiorna pesi
\end{enumerate}


L'architettura HeRoN integra i vantaggi del Reinforcement Learning (apprendimento da esperienza) e dei Large Language Model (conoscenza a priori e ragionamento strategico), con una transizione graduale verso l'autonomia dell'agente.

L'architettura HeRoN è composta da tre componenti principali che interagiscono in modo coordinato:

\subsection{NPC (Non-Player Character) - Versione Semplificata}

L'NPC è l'agente che gioca in Crafter usando il Reinforcement Learning. In questo progetto, l'NPC usa l'algoritmo Deep Q-Network (DQN), che funziona così:

\begin{itemize}
    \item \textbf{Architettura}: DQN usa una seconda rete (target network) per rendere l'apprendimento più stabile.
    \item \textbf{Replay Buffer}: Salva le esperienze passate e dà più importanza a quelle utili, con spazio per 5.000 eventi.
    \item \textbf{Parametri principali}:
    \begin{itemize}
        \item $\alpha = 0.6$: priorità alle esperienze importanti
        \item $\beta = 0.4$ che cresce fino a $1.0$: corregge il campionamento
        \item $\gamma = 0.99$: importanza delle ricompense future
        \item $\epsilon = 1.0$ che scende fino a $0.05$: probabilità di provare azioni nuove
    \end{itemize}
    \item \textbf{Input}: Stato dell'ambiente (43 numeri)
    \item \textbf{Output}: Valori Q per 17 azioni possibili
\end{itemize}

In pratica, l'NPC osserva lo stato, esegue un'azione, riceve una ricompensa e aggiorna la sua memoria per imparare a giocare meglio nel tempo.

\subsection{Helper}

Il componente Helper è un Large Language Model utilizzato in modalità zero-shot che fornisce suggerimenti strategici all'NPC. Nel progetto HeRoN per Crafter, l'Helper si implementa utilizzando un LLM locale (Qwen3-4B-2507) attraverso LM Studio con le seguenti caratteristiche:

\begin{itemize}
    \item \textbf{Generazione di sequenze di azioni}: Diversamente dall'implementazione originale che suggeriva singole azioni, l'Helper in questo progetto genera sequenze di 3-5 azioni coerenti da eseguire una dopo l'altra.
    
    \item \textbf{Contestualizzazione}: L'Helper riceve informazioni dettagliate circa lo stato corrente del gioco:
    \begin{itemize}
        \item Inventario del giocatore (16 item)
        \item Posizione corrente
        \item Statistiche vitali (salute, cibo, acqua)
        \item Achievement sbloccati (22 possibili)
    \end{itemize}
    
    \item \textbf{Prompt Engineering}: Il prompt si presenta come specificatamente progettato per Crafter e include:
    \begin{itemize}
        \item Descrizione del contesto di gioco
        \item Stato corrente dell'agente
        \item Lista delle azioni disponibili
        \item Richiesta di generare una sequenza strategica
    \end{itemize}
    L'Helper risponde con una sequenza di azioni nel formato:
    \begin{verbatim}
    [azione_1], [azione_2], [azione_3], [azione_4], [azione_5]
    \end{verbatim}
    Ad esempio:
    \begin{verbatim}
    [move_right], [do], [move_left], [do], [noop]
    \end{verbatim}
\end{itemize}

\subsection{Reviewer}

Il componente Reviewer è un LLM fine-tuned (basato su T5) che valuta i suggerimenti forniti dall'Helper e genera feedback per migliorarli. Come descritto in dettaglio nel Capitolo~4, il Reviewer si addestra specificamente per il contesto di Crafter su un dataset generato da circa 50 episodi di gioco, contenente circa 2,500 esempi di coppie (suggerimento, feedback).

Il Reviewer analizza:
\begin{itemize}
    \item Coerenza della sequenza di azioni suggerite
    \item Appropriatezza rispetto allo stato corrente
    \item Potenziali rischi o inefficienze
    \item Priorità strategiche (es. sopravvivenza vs. progressione)
    \item Fornisce feedback strutturato che si utilizza per ri-interrogare l'Helper con più informazioni.
\end{itemize}

\subsection{Gestione del Contesto}

Durante ogni episodio, l'Helper mantiene uno stato conversazionale persistente (\texttt{message\_history}) che accumula descrizioni dello stato di gioco, sequenze di azioni generate, feedback del Reviewer e contesto di gioco (posizione, inventario, achievement). Questa memoria conversazionale consente all'Helper di mantenere coerenza e contestualizzazione lungo l'episodio, influenzando direttamente la qualità dei suggerimenti generati.

La cronologia consente al LLM di mantenere coerenza logica tra azioni successive e di comprendere l'evoluzione dello stato di gioco all'interno dell'episodio.

\subsection{Gestione Intelligente del Contesto (Token-Aware)}

L'Helper implementa un sistema di gestione intelligente del contesto per prevenire l'overflow della finestra di contesto del modello LLM (Qwen3-4B-2507 ha 8192 token di limite):

\begin{enumerate}
    \item \textbf{Monitoraggio token}: L'Helper conta il numero di token nella cronologia utilizzando il tokenizer Qwen2.5 (compatibile con Qwen3)
    
    \item \textbf{Soglia di sicurezza}: Quando il contesto raggiunge 6500 token (80\% del limite), viene attivato un reset intelligente per evitare crash e risposte vuote
    
    \item \textbf{Reset con riassunto episodio}: Invece di scartare tutto il contesto, l'Helper genera un riassunto che include:
    \begin{itemize}
        \item Numero di step eseguiti nell'episodio
        \item Reward totale accumulato
        \item Lista degli achievement sbloccati
        \item Feedback recente del Reviewer (se disponibile)
        \item Descrizione dello stato di gioco corrente
    \end{itemize}
    Questo riassunto diventa il nuovo inizio della cronologia, preservando informazioni strategiche critiche.
\end{enumerate}

\textbf{Vantaggi del reset intelligente}:
\begin{itemize}
    \item \textbf{Continuità strategica}: Il modello comprende il progresso episodico complessivo
    \item \textbf{Efficienza token}: Riduce i token inutili mantenendo informazioni essenziali
    \item \textbf{Riduzione allucinazioni}: Contesto pulito riduce risposte errate o non coerenti
    \item \textbf{Maggiore lunghezza episodio}: Consente episodi più lunghi senza crash
\end{itemize}

\subsection{Meccanismi di Re-planning e Aggiornamento Contesto}

Durante l'esecuzione di una sequenza di azioni, l'Helper monitora determinati eventi per aggiornare intelligentemente il contesto:

\textbf{Trigger di Re-query (Interruzione Sequenza)}:
\begin{itemize}
    \item \textbf{Achievement sbloccato}: Quando il giocatore sblocca un nuovo achievement, l'Helper riceve una nuova query con il contesto aggiornato che include il nuovo achievement nel set di quelli sbloccati
    
    \item \textbf{Salute critica} ($health \leq 5$): Se la salute scende sotto soglia critica, la sequenza viene interrotta e l'Helper si consulta per suggerire azioni di emergenza (mangiare, bere, dormire)
    
    \item \textbf{Salute bassa} ($health < 30\%$): Se la salute è bassa ma non critica, si consulta l'Helper per bilanciare l'esplorazione con la gestione della sopravvivenza
    
    \item \textbf{Risorsa completamente consumata}: Se una risorsa chiave (legno, pietra, carbone) raggiunge 0, viene attivata una nuova query per raccoglierla prioritariamente
\end{itemize}

\textbf{Aggiornamento dello Stato Episodio}:

Ad ogni passo, l'Helper aggiorna il suo tracciamento interno registrando i nuovi achievement sbloccati, il numero di passi eseguiti, il reward accumulato e il feedback più recente del Reviewer. Questi dati vengono utilizzati durante il reset del contesto per generare un riassunto episodico coerente, assicurando che il LLM comprenda il progresso complessivo anche dopo un reset di contesto.

\subsection{Reset Episodio e Pulizia Contesto}

All'inizio di ogni nuovo episodio, l'Helper esegue una pulizia completa:
\begin{itemize}
    \item Cancellazione della cronologia messaggi (\texttt{message\_history = []})
    \item Reset del tracciamento achievement episodio
    \item Azzeramento del feedback Reviewer recente
    \item Pulizia della cronologia delle sequenze (usata per rilevare loop)
\end{itemize}

Questo previene l'accumulo di contesto da episodi precedenti.

\section{Workflow dell'Architettura}

Il workflow di HeRoN durante il training si articola in queste fasi:

\subsection{Fase 1: Decisione di Consultazione}

Ad ogni step dell'episodio, l'architettura decide se consultare i componenti LLM basandosi su una soglia dinamica $\theta$.

\begin{algorithm}[H]
\caption{Decisione di consultazione LLM}
\begin{algorithmic}
\IF{$random() > \theta$ AND $episode < 100$}
    \STATE Procedi con workflow LLM
\ELSE
    \STATE Usa solo DQN: $a = \arg\max_a Q(s, a)$
\ENDIF
\end{algorithmic}
\end{algorithm}

Quando si decide di consultare l'LLM, l'Helper genera una sequenza di azioni basandosi sullo stato corrente.

La soglia $\theta$ decresce linearmente da 1.0 a 0.0 nel corso di 100 episodi:
$$\theta_t = \max(0, \theta_0 - 0.01 \cdot episode)$$

In questo modo, durante le fasi iniziali dell'allenamento si fa ricorso ai suggerimenti LLM, per poi ridurre questa dipendenza man mano che l'agente RL migliora.

\subsection{Fase 2: Review e Raffinamento (Reviewer)}

Se il Reviewer è disponibile, valuta la sequenza proposta e fornisce un feedback.
L'Helper utilizza il feedback per migliorare la sequenza e ne crea una seconda versione.
Infine, si utilizza la sequenza migliorata se il Reviewer ha dato feedback, altrimenti la prima.

\subsection{Fase 3: Esecuzione e Re-planning}

La sequenza di azioni viene eseguita una dopo l'altra. Durante l'esecuzione, l'Helper monitora determinati eventi trigger che comportano l'interruzione della sequenza corrente e l'attivazione di una nuova query:

\begin{itemize}
    \item \textbf{Achievement sbloccato}: Quando il giocatore sblocca un nuovo achievement, l'Helper riceve una nuova query con il contesto aggiornato
    \item \textbf{Salute critica} ($health \leq 5$): In caso di salute critica, fallback immediato a DQN per azioni di sopravvivenza
    \item \textbf{Salute bassa} ($health < 30\%$): Ri-query del sistema per gestione prioritaria della salute
    \item \textbf{Risorsa chiave consumata}: Se legno, pietra o carbone raggiungono 0, nuova query per raccoglierla prioritariamente
\end{itemize}

\section{Vantaggi dell'Architettura}

L'architettura HeRoN combina RL e LLM offrendo:
\begin{itemize}
    \item \textbf{Conoscenza a priori}: LLM accelera l'apprendimento con conoscenze generali
    \item \textbf{Ragionamento strategico}: Pianificazione di azioni coerenti a lungo termine
    \item \textbf{Adattabilità}: Unisce esplorazione RL e suggerimenti LLM per nuove situazioni
    \item \textbf{Interpretabilità}: Sequenze di azioni analizzabili per capire la strategia
    \item \textbf{Raffinamento iterativo}: Helper e Reviewer migliorano la qualità dei suggerimenti
\end{itemize}

\section{Sfide dell'Integrazione}

Le principali difficoltà nell’integrazione RL-LLM sono:
\begin{itemize}
    \item \textbf{Overhead computazionale}: LLM più costosi rispetto al DQN
    \item \textbf{Parsing delle risposte}: Gestione di risposte errate o non valide
    \item \textbf{Bilanciamento}: Equilibrio tra dipendenza da LLM e autonomia RL
    \item \textbf{Consistenza}: Garantire sequenze eseguibili e coerenti
\end{itemize}

