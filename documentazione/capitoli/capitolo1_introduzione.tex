\chapter{Introduzione}

\section{Contesto}

Questo progetto fa parte del campo del Reinforcement Learning applicato ai videogiochi, un'area di ricerca in rapida crescita che mira a creare agenti intelligenti capaci di imparare strategie ottimali interagendo con ambienti di gioco.

I videogiochi moderni, soprattutto quelli open-world e di sopravvivenza, presentano sfide complesse che richiedono agli agenti di prendere decisioni strategiche a lungo termine, gestire risorse limitate e adattarsi a situazioni dinamiche. Questi ambienti sono perfetti per testare e validare nuove idee di intelligenza artificiale.

\section{Motivazione}

L'architettura HeRoN (Helper-Reviewer-NPC) è un approccio innovativo che combina il Reinforcement Learning tradizionale con il ragionamento dei Large Language Model (LLM). Questa architettura è stata inizialmente validata in environment di tipo JRPG (Japanese Role-Playing Game) a turni, dimostrando la sua efficacia nel migliorare le prestazioni degli agenti RL attraverso suggerimenti strategici forniti da modelli linguistici.

La sfida principale di questo progetto è stata estendere e testare HeRoN in un contesto molto diverso: il gioco Crafter, un open-world di sopravvivenza che richiede pianificazione a lungo termine, gestione delle risorse e adattamento dinamico.

\section{Obiettivi del Progetto}


Gli obiettivi principali del lavoro sono i seguenti:

\subsection{Obiettivi Primari}

\begin{itemize}
    \item \textbf{Adattamento dell'architettura HeRoN}: L'architettura HeRoN viene estesa dall'environment JRPG a turni all'environment Crafter, un survival game open-world in tempo continuo.
    \item \textbf{Fine-tuning del Reviewer}: Il componente Reviewer viene adattato ai nuovi task specifici di Crafter, con generazione di un dataset appropriato e addestramento del modello per fornire feedback efficaci nel contesto del survival game.
    \item \textbf{Modifica del Helper}: Il comportamento del componente Helper viene modificato affinché vengano generate sequenze di azioni coerenti (3-5 azioni) anziché singole decisioni, permettendo una pianificazione più strategica.
    \item \textbf{Implementazione dell'NPC}: Viene sviluppato un agente di Reinforcement Learning basato sull'algoritmo Deep Q-Network (DQN) ottimizzato per le 17 azioni disponibili in Crafter e il suo spazio di stati a 43 dimensioni.
    \item \textbf{Valutazione delle prestazioni}: Le prestazioni dell'architettura HeRoN completa vengono valutate quantitativamente rispetto a baseline tradizionali, misurando il numero di achievement sbloccati nei 22 obiettivi disponibili in Crafter.
\end{itemize}

\subsection{Obiettivi Secondari}

\begin{itemize}
    \item Analisi del numero ottimale di azioni da suggerire per ciascuna chiamata del Helper.
    \item Studio dell'impatto del reward shaping sulle prestazioni dell'agente.
    \item Implementazione di meccanismi di re-planning che interrompono le sequenze di azioni in situazioni critiche (salute bassa, achievement sbloccati).
\end{itemize}

%\subsection{Struttura del Documento}
%
%Il presente documento è organizzato come segue:
%
%\begin{itemize}
%    \item \textbf{Sezione 2 - Architettura HeRoN}: Descrizione dettagliata dell'architettura HeRoN, dei suoi tre componenti (NPC, Helper, Reviewer) e delle loro interazioni.
%    
%    \item \textbf{Sezione 3 - Environment Crafter}: Presentazione dell'environment Crafter, delle sue caratteristiche, dello spazio di stati e azioni, e dei 22 achievement disponibili.
%    
%    \item \textbf{Sezione 4 - Metodologia}: Descrizione della metodologia di implementazione, dalla preparazione dell'environment al training dell'architettura completa.
%    
%    \item \textbf{Sezione 5 - Risultati}: Presentazione e analisi dei risultati sperimentali, con confronti quantitativi tra HeRoN e baseline.
%    
%    \item \textbf{Sezione 6 - Conclusioni}: Sintesi dei risultati ottenuti, discussione delle sfide affrontate e prospettive future.
%\end{itemize}
