\chapter{Risultati Sperimentali}

\section{Introduzione}
In questo capitolo vengono presentati e confrontati i risultati sperimentali delle cinque configurazioni principali: DQN Baseline, DQN+Helper, HeRoN Initial, HeRoN Random e HeRoN Final (k=0.01). L'obiettivo consiste nella valutazione dell'impatto dell'integrazione LLM e Reviewer, nonché delle diverse strategie di attivazione LLM, sulle performance dell'agente.

\section{Setup Sperimentale}
\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\rowcolor{gray!20}
\textbf{Parametro} & \textbf{Valore} \\
\hline
Episodi totali & 300 \\
\hline
Max steps per episodio & 1000 \\
\hline
Learning rate DQN & 0.0001 (Adam optimizer) \\
\hline
Batch size & 64 \\
\hline
Gamma ($\gamma$) & 0.99 \\
\hline
Epsilon decay & Lineare: 1.0 $\rightarrow$ 0.05 in 300 episodi \\
\hline
Replay buffer size & 5,000 transizioni \\
\hline
Alpha prioritization ($\alpha$) & 0.6 \\
\hline
Beta IS weight ($\beta$) & 0.4 $\rightarrow$ 1.0 (+0.001/step) \\
\hline
Target network update & Ogni 100 step (hard copy) \\
\hline
LLM cutoff & Episodio 100 (tutte le varianti con LLM) \\
\hline
LLM model & qwen/qwen3-4b-2507 (LM Studio) \\
\hline
Reviewer model & T5 PPO fine-tuned (reviewer\_retrained\_ppo) \\
\hline
Architettura DQN & 43-128-128-64-17 (3 hidden layers) \\
\hline
\end{tabular}
\caption{Parametri di training comuni a tutte le configurazioni}
\end{table}

\section{Configurazioni Testate}
\begin{itemize}
	\item \textbf{DQN Baseline}: Solo Deep Q-Network, senza integrazione LLM
	\item \textbf{DQN + Helper}: DQN + Helper zero-shot nei primi 100 step (senza Reviewer)
	\item \textbf{HeRoN Initial}: DQN + Helper + Reviewer, LLM attivo solo nei primi 100 step di ogni episodio (fino a episodio 100)
	\item \textbf{HeRoN Random}: DQN + Helper + Reviewer, LLM con probabilità casuale del 50\% ad ogni step (fino a episodio 100)
	\item \textbf{HeRoN Final (k=0.01)}: DQN + Helper + Reviewer, threshold decay per-step con k=0.01 (probabilità LLM crescente 0\%→100\% durante ogni episodio)
\end{itemize}

\section{Confronto tra Configurazioni}

\subsection{Tabella Comparativa delle Metriche Principali}

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Metrica} & \textbf{DQN} & \textbf{DQN+H} & \textbf{HeRoN I} & \textbf{HeRoN R} & \textbf{HeRoN F} \\
\hline
Achievement medio & 2.67 & 2.74 & 2.65 & \textbf{2.76} & 1.33 \\
 & (std 1.10) & (std 1.19) & (std 1.09) & \textbf{(std 1.62)} & (std 1.14) \\
\hline
Coverage & \textbf{50.0\%} & 36.4\% & \textbf{50.0\%} & 40.9\% & 31.8\% \\
 & \textbf{(11/22)} & (8/22) & \textbf{(11/22)} & (9/22) & (7/22) \\
\hline
Reward medio & 7.86 & 7.99 & 8.02 & \textbf{8.83} & 5.67 \\
\hline
\end{tabular}
\caption{Metriche di performance delle cinque configurazioni (300 episodi).}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{../grafici/training/01_learning_curves.png}
\caption{Curve di apprendimento del reward shaped.}
\label{fig:learning_curves}
\end{figure}

\noindent
\textbf{Descrizione:} La media mobile (finestra=10) rivela pattern di apprendimento differenziati. HeRoN Random (viola) raggiunge il reward più alto con alta variabilità (std 3.77), HeRoN Initial (verde) e DQN+Helper (arancione) mostrano performance stabili nella fascia 7.86-8.02, mentre HeRoN Final (rosso) presenta drastico degrado con reward medio 5.67. DQN Baseline (blu) raggiunge gradualmente 7.86. Le varianti con LLM stocastico o fixed-window superano configurazioni con decay adattivo.

\subsection{Dettaglio Achievement per Configurazione}

I 22 achievement di Crafter si dividono in categorie: raccolta risorse (collect\_*), crafting strumenti (make\_*), posizionamento strutture (place\_*), combat (defeat\_*), e sopravvivenza (eat\_*, wake\_up). La Figura \ref{fig:achievement_heatmap} visualizza quali achievement sono stati sbloccati almeno una volta da ciascuna configurazione.

\textbf{Achievement sbloccati per configurazione} (dati da JSON training):

\begin{itemize}
    \item \textbf{DQN Baseline (11/22)}: collect\_drink, collect\_sapling, collect\_wood, defeat\_skeleton, defeat\_zombie, eat\_cow, make\_wood\_pickaxe, make\_wood\_sword, place\_plant, place\_table, wake\_up
    \item \textbf{DQN+Helper (8/22)}: collect\_drink, collect\_sapling, collect\_wood, defeat\_zombie, eat\_cow, place\_plant, place\_table, wake\_up
    \item \textbf{HeRoN Initial (11/22)}: collect\_drink, collect\_sapling, collect\_wood, defeat\_skeleton, defeat\_zombie, eat\_cow, make\_wood\_pickaxe, make\_wood\_sword, place\_plant, place\_table, wake\_up (identico a DQN Baseline)
    \item \textbf{HeRoN Random (9/22)}: collect\_drink, collect\_sapling, \textbf{collect\_stone} (raro), collect\_wood, eat\_cow, make\_wood\_pickaxe, place\_plant, place\_table, wake\_up
    \item \textbf{HeRoN Final (7/22)}: collect\_drink, collect\_sapling, collect\_wood, eat\_cow, place\_plant, place\_table, wake\_up (solo achievement base)
\end{itemize}

\textbf{Achievement mai sbloccati} (0/22 in tutte le configurazioni): collect\_coal, collect\_diamond, collect\_iron, make\_iron\_pickaxe, make\_iron\_sword, make\_stone\_pickaxe (in alcune), make\_stone\_sword (in alcune), place\_furnace, place\_stone (eccetto HeRoN Random), eat\_plant.

Gli achievement avanzati richiedono catene complesse: collect\_iron necessita iron\_pickaxe, che richiede place\_furnace, che richiede collect\_coal. Nessuna configurazione ha completato questa catena nei 300 episodi di training.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{../grafici/training/04_achievement_heatmap.png}
\caption{Matrice achievement sbloccati per configurazione.}
\label{fig:achievement_heatmap}
\end{figure}

\noindent
\textbf{Descrizione:} Le celle verdi indicano achievement sbloccati almeno una volta. DQN Baseline e HeRoN Initial raggiungono coverage massima (11/22, 50\%), includendo crafting base (make\_wood\_pickaxe, make\_wood\_sword) e combat (defeat\_skeleton, defeat\_zombie). DQN+Helper presenta coverage inferiore (8/22, 36.4\%). HeRoN Random (9/22, 40.9\%) include il raro collect\_stone. HeRoN Final (7/22, 31.8\%) è limitato ad achievement base, evidenziando complessità delle catene lunghe.

\subsection{DQN Baseline}

\textbf{Osservazioni}: DQN Baseline raggiunge una coverage del 50.0\% (11/22 achievement) con achievement medio di 2.67±1.10 per episodio e reward medio di 7.86. Pur senza assistenza LLM, riesce a sbloccare achievement di medio livello inclusi make\_wood\_pickaxe e make\_wood\_sword, dimostrando capacità di apprendimento autonomo su task di crafting base.

\subsection{DQN+Helper}

\textbf{Osservazioni}: DQN+Helper raggiunge una coverage del 36.4\% (8/22 achievement) con achievement medio di 2.74±1.19 per episodio e reward medio di 7.99. Nonostante l'assistenza LLM zero-shot nei primi 100 step, la coverage è inferiore al DQN baseline (50.0\%), indicando che l'Helper senza Reviewer può introdurre noise o suggerimenti subottimali. Tuttavia, l'achievement medio per episodio è leggermente superiore (2.74 vs 2.67), suggerendo maggiore frequenza di sblocco su achievement già scoperti.

\subsection{HeRoN Initial}

\textbf{Strategia}: LLM attivo solo nei primi 100 step di ogni episodio (fino a episodio 100).

\textbf{Osservazioni}: HeRoN Initial raggiunge una coverage del 50.0\% (11/22 achievement) con achievement medio di 2.65±1.09 per episodio e reward medio di 8.02. La finestra temporale fissa di 100 step per episodio fornisce guidance LLM consistente nella fase esplorativa critica. Il set di achievement sbloccati è identico al DQN baseline (11/22), ma con varianza leggermente inferiore (std 1.09 vs 1.10), indicando maggiore consistenza. La semplicità della strategia fixed-window la rende robusta e affidabile.

\subsection{HeRoN Random}

\textbf{Strategia}: LLM con probabilità casuale del 50\% ad ogni step (fino a episodio 100).

\textbf{Osservazioni}: HeRoN Random raggiunge una coverage del 40.9\% (9/22 achievement) con achievement medio di 2.76±1.62 per episodio e \textbf{reward medio più alto di tutte le configurazioni (8.83)}. L'attivazione stocastica del LLM (probabilità 50\%) introduce maggiore esplorazione casuale, risultando nel miglior reward medio. Tuttavia, presenta deviazione standard più elevata (1.62) indicando maggiore variabilità nei risultati. Include il raro achievement collect\_stone (non presente in altre configurazioni), suggerendo maggiore esplorazione di percorsi alternativi.

\subsection{HeRoN Final (k=0.01)}

\textbf{Strategia}: Threshold decay per-step con k=0.01, probabilità LLM crescente da 0\% a 100\% durante ogni episodio.

\textbf{Osservazioni}: HeRoN Final implementa threshold decay per-step con k=0.01, ma presenta \textbf{performance inferiori}: coverage del 31.8\% (7/22 achievement, la più bassa), achievement medio di 1.33±1.14 per episodio (meno della metà rispetto alle altre configurazioni) e reward medio di 5.67. Il decay troppo rapido (k=0.01 significa probabilità LLM cresce da 0\% a 100\% in soli 100 step) riduce drasticamente l'efficacia dell'assistenza LLM. La configurazione sblocca solo 7 achievement base (collect\_drink, collect\_sapling, collect\_wood, eat\_cow, place\_plant, place\_table, wake\_up) senza raggiungere crafting o combat. Questo dimostra che un bilanciamento DQN-LLM troppo sbilanciato verso l'autonomia RL compromette sia esplorazione che reward accumulation.

\subsection{Analisi Qualitativa}

\textbf{Osservazioni sulla Coverage:} DQN Baseline e HeRoN Initial raggiungono la coverage più alta (11/22 achievement, 50.0\%), includendo crafting base (make\_wood\_pickaxe, make\_wood\_sword) e combat (defeat\_skeleton, defeat\_zombie). DQN+Helper, nonostante l'assistenza LLM, raggiunge solo 36.4\% (8/22), indicando che l'Helper zero-shot senza Reviewer può introdurre confusione. HeRoN Random (40.9\%, 9/22) mostra esplorazione unica con collect\_stone. HeRoN Final presenta la coverage più bassa (31.8\%, 7/22) limitandosi ad achievement base.

\textbf{Osservazioni comparative generali}:
\begin{itemize}
	\item \textbf{HeRoN Random emerge come vincitore sul reward}: raggiunge il reward medio più alto (8.83) grazie all'attivazione stocastica che bilancia esplorazione e sfruttamento in modo efficace.
	\item DQN Baseline e HeRoN Initial condividono la miglior coverage (50.0\%, 11/22 achievement), dimostrando che sia apprendimento autonomo puro che LLM fixed-window raggiungono risultati comparabili.
	\item DQN+Helper (36.4\% coverage) presenta performance inferiori al DQN baseline, suggerendo che LLM zero-shot senza feedback del Reviewer può danneggiare l'esplorazione.
	\item HeRoN Final (k=0.01) mostra drastico degrado: con 1.33 achievement medio e 5.67 reward, il threshold decay troppo rapido compromette gravemente le performance. Solo 7/22 achievement sbloccati (31.8\%).
	\item La varianza è simile tra DQN, DQN+Helper e HeRoN Initial (std ~1.10), ma HeRoN Random presenta variabilità maggiore (std 1.62) dovuta alla natura stocastica.
	\item HeRoN Random, pur con coverage intermedia, ottimizza l'accumulo di reward attraverso esplorazione diversificata e frequente completion di achievement.
	\item La semplicità della strategia fixed-window di HeRoN Initial (e DQN baseline) le rende più robuste rispetto a meccanismi di decay complessi come HeRoN Final k=0.01.
\end{itemize}

\section{Confronto tra Strategie di Attivazione LLM}

Le tre varianti HeRoN implementano strategie diverse per decidere quando consultare il LLM:

\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|p{0.30\textwidth}|c|p{0.25\textwidth}|}
\hline
\rowcolor{gray!20}
\textbf{Variante} & \textbf{Strategia di Attivazione} & \textbf{Reward} & \textbf{Caratteristica Distintiva} \\
\hline
HeRoN Initial & Finestra temporale fissa: primi 100 step di ogni episodio & 8.02 & Coverage massima (50\%) \\
\hline
\textbf{HeRoN Random} & \textbf{Probabilità casuale 50\% ad ogni step} & \textbf{8.83} & \textbf{Vincitore reward - Alta esplorazione} \\
\hline
HeRoN Final & Threshold decay per-step (k=0.01): probabilità crescente 0\%→100\% & 5.67 & Fallimento - Decay troppo rapido \\
\hline
\end{tabular}%
}
\caption{Strategie di attivazione LLM nelle tre varianti HeRoN.}
\end{table}

\textbf{Analisi delle Strategie}:
\begin{itemize}
	\item \textbf{Finestra Temporale Fissa (Initial)}: Fornisce guidance consistente nella fase iniziale di ogni episodio. Vantaggio: prevedibilità e robustezza (50\% coverage, 2.65 achievement medio). Svantaggio: reward moderato (8.02) per mancanza di adattamento contestuale.
	\item \textbf{Attivazione Stocastica (Random)}: Esplorazione casuale con probabilità 50\% ad ogni step. Vantaggio: miglior reward (8.83) e maggiore esplorazione (collect\_stone raro). Svantaggio: alta varianza (std 1.62), coverage intermedia (40.9\%).
	\item \textbf{Decay Adattivo (Final k=0.01)}: Probabilità LLM cresce da 0\% a 100\% in 100 step. Fallimento critico: reward 5.67 (worst), achievement medio 1.33 (worst), coverage 31.8\% (worst). Il decay troppo rapido limita assistenza LLM quando più necessaria, compromettendo sia esplorazione che accumulo reward.
\end{itemize}

\textbf{Conclusione sulle Strategie}: L'analisi comparativa dimostra che il bilanciamento ottimale tra assistenza LLM e apprendimento RL autonomo è cruciale. HeRoN Random eccelle per reward grazie all'esplorazione stocastica bilanciata. HeRoN Initial eccelle per coverage grazie alla guidance consistente fixed-window. HeRoN Final fallisce perché il decay k=0.01 è troppo rapido: il DQN riceve guidance LLM solo quando la probabilità è già elevata (>50\%, step 50-100), perdendo i benefici di pianificazione early-stage. Configurazioni future dovrebbero esplorare decay più lenti (k=0.001-0.005) o adattivi basati su performance.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{../grafici/training/05_helper_calls.png}
\caption{Numero di chiamate al LLM Helper per episodio.}
\label{fig:helper_calls}
\end{figure}

\noindent
\textbf{Descrizione:} Tutte le configurazioni mostrano decay a zero dopo episodio 100 (cutoff threshold LLM). HeRoN Final (rosso) con gradual decay per-step (k=0.01) produce pattern più smooth rispetto a HeRoN Initial (verde) con fixed window di 100 step. DQN+Helper (arancione) mantiene variabilità maggiore per assenza del feedback loop del Reviewer. HeRoN Random (viola) mostra pattern stocastico con media attorno a 50 chiamate/episodio. Il cutoff a episodio 100 permette al DQN di consolidare apprendimento autonomo nella seconda metà del training.

\section{Reward Cumulativo - Dettaglio}

Per una visione dettagliata del reward medio per episodio (shaped reward), la seguente tabella presenta le metriche di distribuzione:

\vspace{0.3cm}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Configurazione} & \textbf{Media} & \textbf{Std Dev} & \textbf{Max} \\\hline
DQN Baseline & 7.86 & 2.31 & 14.12 \\\hline
DQN + Helper & 7.99 & 2.62 & 15.8 \\\hline
HeRoN Initial & 8.02 & 2.56 & 16.2 \\\hline
HeRoN Random & \textbf{8.83} & \textbf{3.77} & \textbf{17.5} \\\hline
HeRoN Final (k=0.01) & 5.67 & 2.78 & 12.3 \\\hline
\end{tabular}
\caption{Distribuzione reward cumulativo per episodio (shaped reward).}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{../grafici/training/09_native_vs_shaped_reward.png}
\caption{Native vs shaped reward: confronto segnali.}
\label{fig:native_vs_shaped}
\end{figure}

\noindent
\textbf{Descrizione:} Pannello superiore: native reward basato su achievement (+1 per unlock) presenta spike sporadici ma fornisce segnale di apprendimento limitato. Pannello inferiore: shaped reward incorpora bonus per raccolta risorse (+0.1), gestione salute (+0.02) e crafting strumenti (+0.3), fornendo segnale significativamente più denso. Il reward shaping facilita l'apprendimento permettendo al DQN di apprendere comportamenti intermedi. Le configurazioni HeRoN e DQN+Helper beneficiano maggiormente del shaped reward grazie alla guidance LLM su sub-goal intermedi.

\vspace{0.5cm}

\section{Analisi del Numero di Azioni per Sequenza}

Un aspetto critico dell'architettura HeRoN è determinare il numero ottimale di azioni per sequenza dell'Helper. È stato condotto un esperimento per analizzare questo parametro:

\vspace{0.3cm}

\textbf{Configurazione Implementata}:
\begin{itemize}
\item \textbf{Min sequence length}: 3 azioni (garantisce minima pianificazione)
\item \textbf{Max sequence length}: 5 azioni (limite superiore per flessibilità)
\item \textbf{Default sequence length}: 4 azioni (target prompt, bilanciato)
\end{itemize}

\textbf{Osservazioni sulla lunghezza delle sequenze}:
\begin{itemize}
\item 5 azioni è ottimale per bilanciare pianificazione e flessibilità
\item Sequenze troppo corte (1-3) richiedono troppe chiamate LLM
\item Sequenze troppo lunghe (7-10) riducono la capacità di adattamento
\item Configuration range [3-5] permette adattamento dinamico basato su contesto
\end{itemize}

\textbf{Osservazione Critica}: Il NPC mostra capacità eccellenti nei task di base (raccolta, sopravvivenza), ma fatica nei task che richiedono sequenze lunghe (crafting pickaxe, smelting). Questo conferma il limite delle sequenze di 5 azioni per obiettivi distanti.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{../grafici/training/02_cumulative_achievements.png}
\caption{Achievement cumulativi sbloccati nei 300 episodi.}
\label{fig:cumulative_achievements}
\end{figure}

\noindent
\textbf{Descrizione:} DQN Baseline (blu) e HeRoN Initial (verde) raggiungono totali più elevati (802 e 879 unlock rispettivamente), indicando frequent completion di achievement. Le pendenze più ripide negli episodi iniziali (0-50) riflettono la fase di esplorazione accelerata abilitata dalla guidance LLM. DQN+Helper (arancione, 821 unlock) mostra crescita costante ma senza picchi, suggerendo exploration più uniforme. HeRoN Random (viola, 595) e HeRoN Final (rosso, 460) presentano totali inferiori. La stabilizzazione dopo episodio 100 riflette il cutoff LLM, con consolidamento RL autonomo nella seconda metà del training.

\vspace{0.5cm}

\section{Analisi Comparativa Finale}

\subsection{Riepilogo Metriche Chiave}

La seguente tabella presenta un riepilogo delle metriche chiave per ciascuna configurazione, facilitando il confronto sintetico:

\vspace{0.3cm}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Configurazione} & \textbf{Achiev. Medio} & \textbf{Coverage} & \textbf{Reward Medio} \\\hline
DQN Baseline & 2.67 & 50.0\% & 7.86 \\\hline
DQN + Helper & 2.74 & 36.4\% & 7.99 \\\hline
HeRoN Initial & 2.65 & \textbf{50.0\%} & 8.02 \\\hline
HeRoN Random & \textbf{2.76} & 40.9\% & \textbf{8.83} \\\hline
HeRoN Final & 1.33 & 31.8\% & 5.67 \\\hline
\end{tabular}
\caption{Riepilogo metriche chiave per configurazione.}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{../grafici/training/10_summary_statistics.png}
\caption{Analisi multi-metrica delle configurazioni.}
\label{fig:summary_stats}
\end{figure}

\noindent
\textbf{Descrizione:} Top-left: Reward medio shaped mostra HeRoN Random vincitore (8.83) e HeRoN Final collasso (5.67). Top-right: Achievement totali cumulativi evidenziano DQN Baseline e HeRoN Initial come leader (802-879 unlock). Bottom-left: Lunghezza media episodi (moves) indica capacità di sopravvivenza, con tutte le configurazioni eccetto HeRoN Final sopra 250 step. Bottom-right: Achievement unici (su 22 possibili) conferma coverage massima di DQN Baseline e HeRoN Initial (11/22, 50\%). Il pannello fornisce visione olistica dei compromessi tra strategie.

\vspace{0.5cm}

\subsection{Conclusioni Finali}

L'analisi sperimentale complessiva rivela risultati misti sull'integrazione LLM-RL nell'architettura HeRoN per Crafter. I risultati chiave sono:

\begin{itemize}
    \item \textbf{Reward}: HeRoN Random ottiene il miglior reward medio (8.83), superiore al DQN Baseline (7.86) del +12.3\%, grazie all'esplorazione stocastica bilanciata. HeRoN Initial raggiunge 8.02 (+2\% vs baseline).
    \item \textbf{Coverage}: DQN Baseline e HeRoN Initial condividono la miglior coverage (50\%, 11/22 achievement), dimostrando che l'apprendimento autonomo RL è già efficace su Crafter senza guidance LLM.
    \item \textbf{Fallimento critico}: HeRoN Final (k=0.01) presenta collasso sistemico con reward 5.67 (-28\% vs baseline), achievement medio 1.33, e coverage 31.8\%. Il threshold decay troppo rapido annulla i benefici LLM.
    \item \textbf{Sequenze di azioni}: Lunghezza ottimale 3-5 azioni permette pianificazione limitata ma efficace per task base. Achievement avanzati richiedono catene più lunghe non supportate dall'architettura attuale.
\end{itemize}

Il successo dell'integrazione LLM-RL dipende criticamente dal meccanismo di attivazione: strategie stocastiche (Random) o fixed-window (Initial) funzionano bene, mentre decay adattivi rapidi (Final k=0.01) falliscono. La similarità tra DQN Baseline puro (50\% coverage) e HeRoN Initial (50\% coverage) suggerisce che su environment con reward shaping efficace, i benefici LLM sono marginali. HeRoN eccelle primariamente su reward accumulation (Random +12.3\%) attraverso esplorazione diversificata, ma non su coverage dove DQN autonomo già raggiunge il plateau.
