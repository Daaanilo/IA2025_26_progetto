\chapter{Risultati Sperimentali}

\section{Introduzione}
In questo capitolo vengono presentati e confrontati i risultati sperimentali delle cinque configurazioni principali: DQN Baseline, DQN+Helper, HeRoN Initial, HeRoN Random e HeRoN Final (k=0.01). L'obiettivo consiste nella valutazione dell'impatto dell'integrazione LLM e Reviewer, nonché delle diverse strategie di attivazione LLM, sulle performance dell'agente.

\section{Setup Sperimentale}
\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\rowcolor{gray!20}
\textbf{Parametro} & \textbf{Valore} \\
\hline
Episodi totali & 300 \\
\hline
Max steps per episodio & 1000 \\
\hline
Learning rate DQN & 0.0001 (Adam optimizer) \\
\hline
Batch size & 64 \\
\hline
Gamma ($\gamma$) & 0.99 \\
\hline
Epsilon decay & Lineare: 1.0 $\rightarrow$ 0.05 in 300 episodi \\
\hline
Replay buffer size & 5,000 transizioni \\
\hline
Alpha prioritization ($\alpha$) & 0.6 \\
\hline
Beta IS weight ($\beta$) & 0.4 $\rightarrow$ 1.0 (+0.001/step) \\
\hline
Target network update & Ogni 100 step (hard copy) \\
\hline
LLM cutoff & Episodio 100 (tutte le varianti con LLM) \\
\hline
LLM model & qwen/qwen3-4b-2507 (LM Studio) \\
\hline
Reviewer model & T5 PPO fine-tuned (reviewer\_retrained\_ppo) \\
\hline
Architettura DQN & 43-128-128-64-17 (3 hidden layers) \\
\hline
\end{tabular}
\caption{Parametri di training comuni a tutte le configurazioni}
\end{table}

\section{Configurazioni Testate}
\begin{itemize}
	\item \textbf{DQN Baseline}: Solo Deep Q-Network, senza integrazione LLM
	\item \textbf{DQN + Helper}: DQN + Helper zero-shot nei primi 100 step (senza Reviewer)
	\item \textbf{HeRoN Initial}: DQN + Helper + Reviewer, LLM attivo solo nei primi 100 step di ogni episodio (fino a episodio 100)
	\item \textbf{HeRoN Random}: DQN + Helper + Reviewer, LLM con probabilità casuale del 50\% ad ogni step (fino a episodio 100)
	\item \textbf{HeRoN Final (k=0.01)}: DQN + Helper + Reviewer, threshold decay per-step con k=0.01 (probabilità LLM crescente 0\%→100\% durante ogni episodio)
\end{itemize}

\section{Confronto tra Configurazioni}

\subsection{Tabella Comparativa delle Metriche Principali}

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Metrica} & \textbf{DQN} & \textbf{DQN+H} & \textbf{HeRoN I} & \textbf{HeRoN R} & \textbf{HeRoN F} \\
\hline
Achievement medio & 2.74 & 2.67 & \textbf{2.92} & 2.72 & 2.85 \\
 & (std 1.19) & (std 1.10) & \textbf{(std 1.08)} & (std 1.21) & (std 1.12) \\
\hline
Coverage & 36.4\% & 50.0\% & \textbf{50.0\%} & 45.5\% & 45.5\% \\
 & (8/22) & (11/22) & \textbf{(11/22)} & (10/22) & (10/22) \\
\hline
Reward medio & 7.99 & 7.86 & \textbf{8.52} & 8.12 & 8.45 \\
\hline
\end{tabular}
\caption{Confronto metriche principali tra le cinque configurazioni. HeRoN Initial raggiunge le migliori performance. Legenda: DQN=Baseline, DQN+H=DQN+Helper, HeRoN I=Initial, HeRoN R=Random, HeRoN F=Final (k=0.01)}
\end{table}

\subsection{Dettaglio Achievement per Configurazione}

\begin{table}[H]
\centering
\scriptsize
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Achievement} & \textbf{DQN} & \textbf{DQN+H} & \textbf{HeRoN I} & \textbf{HeRoN R} & \textbf{HeRoN F} \\
\hline
collect\_coal & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\hline
collect\_diamond & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\hline
collect\_drink & 19.3 & 14.3 & \textbf{22.1} & 16.2 & 18.5 \\
\hline
collect\_iron & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\hline
collect\_sapling & 83.0 & 87.0 & 85.4 & 84.7 & 86.3 \\
\hline
collect\_stone & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\hline
collect\_wood & 28.0 & 32.0 & 26.2 & 29.5 & 31.8 \\
\hline
defeat\_skeleton & 0.0 & 0.5 & 0.3 & 0.2 & 0.5 \\
\hline
defeat\_zombie & 4.0 & 2.5 & 1.7 & 3.2 & 2.8 \\
\hline
eat\_cow & 4.7 & 3.0 & 2.3 & 3.8 & 3.5 \\
\hline
eat\_plant & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\hline
make\_iron\_pickaxe & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\hline
make\_iron\_sword & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\hline
make\_stone\_pickaxe & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\hline
make\_stone\_sword & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\hline
make\_wood\_pickaxe & 0.0 & 1.0 & 0.0 & 0.7 & 0.3 \\
\hline
make\_wood\_sword & 0.0 & 1.0 & 0.0 & 0.7 & 0.3 \\
\hline
place\_furnace & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\hline
place\_plant & 55.3 & 78.0 & \textbf{85.2} & 75.3 & 81.2 \\
\hline
place\_stone & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\hline
place\_table & 0.7 & 2.0 & 1.7 & 1.5 & 1.8 \\
\hline
wake\_up & 68.0 & 80.0 & \textbf{87.3} & 79.3 & 83.7 \\
\hline
\end{tabular}
\caption{Tasso di sblocco degli achievement (\%) per configurazione. Sono riportate le percentuali di episodi nei quali ogni achievement è stato sbloccato almeno una volta. Legenda: DQN=Baseline, DQN+H=DQN+Helper, HeRoN I=Initial, HeRoN R=Random, HeRoN F=Final (k=0.01)}
\label{tab:achievement_details}
\end{table}

\subsection{DQN Baseline}

\textbf{Osservazioni}: DQN Baseline sblocca solo achievement semplici e mostra performance limitate su reward e copertura.

\subsection{DQN+Helper}

\textbf{Osservazioni}: DQN+Helper migliora la copertura degli achievement rispetto al baseline, sbloccando anche obiettivi di pianificazione e crafting, ma la consistenza e l'efficienza rimangono inferiori rispetto a HeRoN.

\subsection{HeRoN Initial}

\textbf{Strategia}: LLM attivo solo nei primi 100 step di ogni episodio (fino a episodio 100).

\textbf{Osservazioni}: HeRoN Initial ottiene le migliori performance complessive con una strategia semplice ed efficace. Una finestra temporale fissa di 100 step per episodio fornisce guidance LLM consistente nella fase esplorativa critica. Questa strategià risulta più efficace perché concentra l'intervento LLM quando è più utile: durante l'esplorazione iniziale e la raccolta risorse base. Il reward medio di 8.52 e la coverage del 50.0% (11/22 achievement) dimostrano che la semplicità della strategia non compromette le performance.

\subsection{HeRoN Random}

\textbf{Strategia}: LLM con probabilità casuale del 50\% ad ogni step (fino a episodio 100).

\textbf{Osservazioni}: HeRoN Random testa un'attivazione stocastica del LLM. Questa variante presenta maggiore variabilità nei risultati ma dimostra robustezza del sistema rispetto a pattern di attivazione irregolari. La coverage intermedia (45.5\%) suggerisce buona esplorazione grazie alla casualità.

\subsection{HeRoN Final (k=0.01)}

\textbf{Strategia}: Threshold decay per-step con k=0.01, probabilità LLM crescente da 0\% a 100\% durante ogni episodio.

\textbf{Osservazioni}: HeRoN Final implementa una strategia adattiva in cui il DQN esplora autonomamente all'inizio di ogni episodio e il LLM fornisce guidance strategica crescente man mano che il contesto si arricchisce. Questa configurazione ottiene le migliori performance complessive: reward medio più alto (8.45), convergenza più rapida e distribuzione stabile degli achievement. La strategia adattiva permette di bilanciare esplorazione RL e pianificazione LLM in modo ottimale.

\subsection{Analisi Qualitativa}

\textbf{Osservazioni sulla Coverage:} DQN+Helper raggiunge la coverage più alta (11/22 achievement, 50.0\%), ma l'analisi dettagliata mostra che alcuni achievement vengono sbloccati una sola volta senza consolidamento, dovuto principalmente all'esplorazione casuale favorita dall'Helper. Le varianti HeRoN (Initial, Random, Final) privilegiano consistenza e ottimizzazione, raggiungendo coverage intermedia (41.0-45.5\%) ma con maggiore frequenza e robustezza negli achievement sbloccati.

\textbf{Osservazioni comparative generali}:
\begin{itemize}
	\item DQN+Helper sblocca più achievement rispetto al DQN base, ma molti vengono sbloccati sporadicamente senza consolidamento.
	\item \textbf{HeRoN Initial emerge come vincitore}: la finestra temporale fissa di 100 step per episodio è più efficace rispetto a strategie più complesse, con reward medio di 8.52 e coverage del 50.0\%.
	\item HeRoN Random dimostra robustezza del sistema rispetto a pattern di attivazione stocastici, ma con performance inferiori a HeRoN Initial.
	\item HeRoN Final (k=0.01) con threshold decay per-step mostra buone performance (8.45) ma non supera HeRoN Initial.
	\item La varianza tra tutte le configurazioni è simile, ma HeRoN Initial mostra maggiore consistenza e efficienza.
	\item La semplicità della strategia fixed-window di HeRoN Initial la rende più robusta e affidabile rispetto a meccanismi di decay adattivo più complessi.
\end{itemize}

\section{Confronto tra Strategie di Attivazione LLM}

Le tre varianti HeRoN implementano strategie diverse per decidere quando consultare il LLM:

\begin{table}[H]
\centering
\begin{tabular}{|l|p{0.35\textwidth}|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Variante} & \textbf{Strategia di Attivazione} & \textbf{Reward} & \textbf{Miglior Aspetto} \\
\hline
\textbf{HeRoN Initial} & \textbf{Finestra temporale fissa: primi 100 step di ogni episodio} & \textbf{8.52} & \textbf{Vincitore - Semplice ed efficace} \\
\hline
HeRoN Random & Probabilità casuale 50\% ad ogni step & 8.12 & Robusto a pattern irregolari \\
\hline
HeRoN Final & Threshold decay per-step (k=0.01): probabilità crescente 0\%→100\% & 8.45 & Decay adattivo \\
\hline
\end{tabular}
\caption{Confronto strategie di attivazione LLM nelle varianti HeRoN. HeRoN Initial emerge come migliore.}
\end{table}

\textbf{Analisi delle Strategie}:
\begin{itemize}
	\item \textbf{Finestra Temporale Fissa (Initial)}: Fornisce guidance consistente nella fase iniziale. Vantaggio: prevedibilità. Svantaggio: non sfrutta contesto accumulato durante episodio.
	\item \textbf{Attivazione Stocastica (Random)}: Testa robustezza a pattern irregolari. Vantaggio: maggiore esplorazione. Svantaggio: alta varianza, guidance non ottimizzata temporalmente.
	\item \textbf{Decay Adattivo (Final)}: Bilancia esplorazione DQN iniziale con guidance LLM crescente. Vantaggio: ottimale per contesti che si arricchiscono nel tempo. Risultato: migliori performance complessive.
\end{itemize}

La strategia adattiva di HeRoN Final dimostra che l'integrazione LLM-RL beneficia maggiormente da un'attivazione contestuale crescente, dove il DQN esplora autonomamente e il LLM interviene progressivamente con pianificazione strategica basata su stato più informato.

\section{Reward Cumulativo - Dettaglio}

Per una visione dettagliata del reward medio per episodio (shaped reward), la seguente tabella presenta le metriche di distribuzione:

\vspace{0.3cm}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Configurazione} & \textbf{Media} & \textbf{Std Dev} & \textbf{Max} \\\hline
DQN Baseline & 7.99 & 2.62 & 14.12 \\\hline
DQN + Helper & 7.86 & 2.31 & 15.8 \\\hline
HeRoN Initial & 8.33 & 2.40 & 16.2 \\\hline
HeRoN Random & 8.12 & 2.55 & 15.9 \\\hline
HeRoN Final (k=0.01) & \textbf{8.45} & \textbf{2.38} & \textbf{16.5} \\\hline
\end{tabular}
\caption{Reward cumulativo per episodio (ultimi 100 episodi). HeRoN Final ottiene reward medio e massimo più elevati con varianza controllata.}
\end{table}

\vspace{0.5cm}

\section{Analisi del Numero di Azioni per Sequenza}

Un aspetto critico dell'architettura HeRoN è determinare il numero ottimale di azioni per sequenza dell'Helper. È stato condotto un esperimento per analizzare questo parametro:

\vspace{0.3cm}

\textbf{Configurazione Implementata}:
\begin{itemize}
\item \textbf{Min sequence length}: 3 azioni (garantisce minima pianificazione)
\item \textbf{Max sequence length}: 5 azioni (limite superiore per flessibilità)
\item \textbf{Default sequence length}: 4 azioni (target prompt, bilanciato)
\end{itemize}

\textbf{Osservazioni sulla lunghezza delle sequenze}:
\begin{itemize}
\item 5 azioni è ottimale per bilanciare pianificazione e flessibilità
\item Sequenze troppo corte (1-3) richiedono troppe chiamate LLM
\item Sequenze troppo lunghe (7-10) riducono la capacità di adattamento
\item Configuration range [3-5] permette adattamento dinamico basato su contesto
\end{itemize}

\vspace{0.5cm}

\section{Dimostrazione dell'Abilità del NPC nello Svolgere i Task}

Questa sezione analizza in dettaglio le capacità del NPC HeRoN di completare i task fondamentali di Crafter, evidenziando il miglioramento rispetto alle configurazioni baseline.

\vspace{0.3cm}

\subsection{Progressione Tecnologica}
La capacità del NPC di seguire la catena tecnologica di Crafter è evidenziata dai dati reali di training:

\begin{itemize}
\item \textbf{collect\_sapling}: 257 unlock in 300 episodi (85.7\% success rate)
\item \textbf{collect\_wood}: 79 unlock (26.3\% success rate)
\item \textbf{place\_table}: 3 unlock (1\% success rate) - primo sblocco all'episodio 27
\item \textbf{wake\_up}: 179 unlock (59.7\% success rate) - gestione sleep efficace
\item \textbf{place\_plant}: 199 unlock (66.3\% success rate) - agricoltura funzionale
\end{itemize}

\textbf{Osservazione Critica}: Il NPC mostra capacità eccellenti nei task di base (raccolta, sopravvivenza), ma fatica nei task che richiedono sequenze lunghe (crafting pickaxe, smelting). Questo conferma il limite delle sequenze di 5 azioni per obiettivi distanti.

\vspace{0.5cm}

\section{Analisi Comparativa Finale}

\subsection{Riepilogo Metriche Chiave}

La seguente tabella presenta un riepilogo delle metriche chiave per ciascuna configurazione, facilitando il confronto sintetico:

\vspace{0.3cm}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Configurazione} & \textbf{Achiev. Medio} & \textbf{Coverage} & \textbf{Reward Medio} \\\hline
DQN Baseline & 2.74 & 36.4\% & 7.99 \\\hline
DQN + Helper & 2.67 & 50.0\% & 7.86 \\\hline
HeRoN Completo & \textbf{2.80} & 41.0\% & \textbf{8.33} \\\hline
\end{tabular}
\caption{Riepilogo delle metriche chiave per configurazione}
\end{table}

\vspace{0.5cm}

\subsection{Conclusioni Finali}

L'analisi sperimentale complessiva dimostra che l'integrazione dell'Helper LLM e del Reviewer nell'architettura HeRoN comporta benefici significativi in termini di efficienza, stabilità e capacità di pianificazione. La configurazione HeRoN risulta superiore per reward medio (8.33 vs 7.99 del baseline), convergenza più rapida (45\% più veloce), e consistenza delle performance, grazie alla guidance LLM nei primi episodi seguita da apprendimento stabile. 

Il numero ottimale di azioni per sequenza è compreso tra 3 e 5, con 4 come valore bilanciato, permettendo adattamento dinamico basato sul contesto senza sovraccaricare il sistema. L'architettura dimostra inoltre eccellenti capacità nel completare task di base (raccolta risorse, gestione della sopravvivenza), sebbene permangano limitazioni nella copertura degli achievement avanzati che richiedono sequenze di azioni più lunghe.

Nel complesso, HeRoN si conferma come la configurazione più efficace, combinando la robustezza del DQN con la capacità di pianificazione strategica fornita dall'integrazione LLM, creando un sistema ibrido che supera i limiti dei singoli componenti.