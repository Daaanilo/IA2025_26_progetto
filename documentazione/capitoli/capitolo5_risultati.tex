\chapter{Risultati Sperimentali}

\section{Introduzione}
In questo capitolo vengono presentati e confrontati i risultati sperimentali delle tre configurazioni principali: DQN Baseline, DQN+Helper e HeRoN (DQN+Helper+Reviewer). L'obiettivo è valutare l'impatto dell'integrazione LLM e Reviewer sulle performance dell'agente.

\section{Setup Sperimentale}
\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\rowcolor{gray!20}
\textbf{Parametro} & \textbf{Valore} \\
\hline
Episodi totali & 1000 \\
\hline
Max steps per episodio & 1000 \\
\hline
Learning rate DQN & 0.0001 \\
\hline
Batch size & 64 \\
\hline
Gamma ($\gamma$) & 0.99 \\
\hline
Epsilon iniziale & 1.0 \\
\hline
Epsilon finale & 0.05 \\
\hline
Epsilon decay & 800 episodi \\
\hline
Replay buffer size & 10,000 \\
\hline
Alpha prioritization & 0.6 \\
\hline
Beta IS weight & 0.4 $\rightarrow$ 1.0 \\
\hline
Threshold iniziale & 1.0 \\
\hline
Threshold decay & 0.01 per episodio \\
\hline
LLM cutoff & Episodio 600 \\
\hline
\end{tabular}
\caption{Parametri di training}
\end{table}

\section{Configurazioni Testate}
\begin{itemize}
	\item \textbf{DQN Baseline}: Solo Deep Q-Network
	\item \textbf{DQN + Helper}: DQN + Helper senza Reviewer
	\item \textbf{HeRoN}: DQN + Helper + Reviewer
\end{itemize}

\section{Confronto tra Configurazioni}

% --- TABELLE METRICHE PRINCIPALI ---
\subsection{DQN Baseline}
\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Metriche} & \textbf{Valore} & \textbf{Note} \\
\hline
Achievement medio & 2.74 & std 1.19, max 6 \\
\hline
Coverage & 36.4\% & 8/22 \\
\hline
Reward medio & 7.99 & ultimi 100 episodi \\
\hline
\end{tabular}
\caption{Metriche principali DQN Baseline}
\end{table}

\textbf{Osservazioni}: DQN Baseline sblocca solo achievement semplici e mostra performance limitate su reward e copertura.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{immagini/plots_dqn_BASE/multi_metric_dashboard.png}
\caption{Dashboard multi-metrica DQN Baseline.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{immagini/plots_dqn_BASE/achievement_heatmap.png}
\caption{Heatmap achievement DQN Baseline.}
\end{figure}

% Curve di apprendimento DQN Baseline
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{immagini/plots_dqn_BASE/achievement_curves/learning_curve_collect_wood.png}
\caption{Curve di apprendimento collect\_wood - DQN Baseline.}
\end{figure}
% ...aggiungi altre curve se necessario...

\subsection{DQN+Helper}
\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Metriche} & \textbf{Valore} & \textbf{Note} \\
\hline
Achievement medio & 2.67 & std 1.10, max 6 \\
\hline
Coverage & 50.0\% & 11/22 \\
\hline
Reward medio & 7.86 & ultimi 100 episodi \\
\hline
\end{tabular}
\caption{Metriche principali DQN+Helper}
\end{table}

\textbf{Osservazioni}: DQN+Helper migliora la copertura degli achievement, sbloccando anche obiettivi di pianificazione e crafting, ma la consistenza e l'efficienza rimangono inferiori rispetto a HeRoN.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{immagini/plots_dqn_helper/multi_metric_dashboard.png}
\caption{Dashboard multi-metrica DQN+Helper.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{immagini/plots_dqn_helper/achievement_heatmap.png}
\caption{Heatmap achievement DQN+Helper.}
\end{figure}

% Curve di apprendimento DQN+Helper
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{immagini/plots_dqn_helper/achievement_curves/collect_wood.png}
\caption{Curve di apprendimento collect\_wood - DQN+Helper.}
\end{figure}
% ...aggiungi altre curve se necessario...

\subsection{HeRoN}
\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Metriche} & \textbf{Valore} & \textbf{Note} \\
\hline
Achievement medio & 2.8 & std 1.15, max 5 \\
\hline
Coverage & 41.0\% & 9/22 \\
\hline
Reward medio & 8.33 & ultimi 100 episodi \\
\hline
\end{tabular}
\caption{Metriche principali HeRoN}
\end{table}

\textbf{Osservazioni}: HeRoN mantiene una distribuzione stabile degli achievement, pianifica meglio e converge più rapidamente. Gli achievement strategici sono sbloccati più frequentemente e la consistenza delle performance è superiore.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{immagini/heron/multi_metric_dashboard.png}
\caption{Dashboard multi-metrica HeRoN.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{immagini/heron/achievement_heatmap.png}
\caption{Heatmap achievement HeRoN.}
\end{figure}

% Curve di apprendimento HeRoN
\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{immagini/heron/achievement_curves/collect_wood.png}
\caption{Curve di apprendimento collect\_wood - HeRoN.}
\end{figure}
% ...aggiungi altre curve se necessario...

% Confronti diretti tra configurazioni (minipage)
\subsection{Confronti Diretti}
\begin{figure}[H]
\centering
\begin{minipage}{0.48\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{immagini/heron/achievement_curves/collect_wood.png}
\caption*{HeRoN}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{immagini/plots_dqn_BASE/achievement_curves/learning_curve_collect_wood.png}
\caption*{DQN Baseline}
\end{minipage}
\caption{Confronto curve di apprendimento per collect\_wood.}
\end{figure}
% ...aggiungi altri confronti minipage se necessario...

% --- TABELLA COVERAGE ---
\subsection{Coverage Achievement}
\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Configurazione} & \textbf{Coverage} & \textbf{Percentuale} \\
\hline
DQN Baseline & 8 / 22 & 36.4\% \\
\hline
DQN + Helper & 11 / 22 & 50.0\% \\
\hline
HeRoN Completo & 9 / 22 & 41.0\% \\
\hline
\end{tabular}
\caption{Coverage degli achievement}
\end{table}

\textbf{Nota sulla coverage DQN+Helper vs HeRoN:}

Nonostante DQN+Helper raggiunga una coverage più alta (11/22 achievement unici) rispetto a HeRoN (9/22), l'analisi dettagliata mostra che i due achievement "extra" vengono sbloccati una sola volta durante il training, senza essere consolidati o ripetuti. 

Questo risultato è dovuto principalmente all'esplorazione casuale favorita dall'Helper, che porta il sistema a visitare più aree del gioco ma senza sviluppare strategie stabili. Al contrario, HeRoN, grazie all'integrazione del Reviewer, privilegia la consistenza e l'ottimizzazione del reward, raggiungendo meno achievement unici ma in modo più frequente e robusto. 

Pertanto, la maggiore coverage di DQN+Helper non rappresenta una vera superiorità strategica, ma solo una variabilità episodica non ripetibile.

% --- TABELLA REWARD CUMULATIVO ---
\subsection{Reward Cumulativo}
\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Configurazione} & \textbf{Media} & \textbf{Std Dev} \\
\hline
DQN Baseline & 7.99 & 2.62 \\
\hline
DQN + Helper & 7.86 & 2.31 \\
\hline
HeRoN Completo & 8.33 & 2.40 \\
\hline
\end{tabular}
\caption{Reward cumulativo per episodio (ultimi 100 episodi)}
\end{table}

\subsection{Analisi Qualitativa}
\textbf{Osservazioni comparative}:
\begin{itemize}
	\item DQN+Helper sblocca più achievement rispetto al DQN base, ma molti di questi vengono sbloccati solo una volta e non consolidati.
	\item HeRoN mantiene la distribuzione dei reward più stabile e una maggiore efficienza temporale, anche se la copertura degli achievement avanzati rimane limitata.
	\item La varianza tra le configurazioni è simile, ma HeRoN mostra una maggiore consistenza nelle performance.
	\item DQN+Helper e HeRoN convergono più rapidamente rispetto al DQN base, ma solo HeRoN mantiene stabilità e pianificazione strategica.
	\item In tutti i confronti, HeRoN si conferma superiore per efficienza, stabilità e capacità di pianificazione, anche se la copertura degli achievement avanzati resta una sfida aperta.
\end{itemize}

\section{Conclusioni}

L'integrazione dell'Helper e del Reviewer determina benefici significativi in termini di efficienza e stabilità. La configurazione HeRoN risulta superiore per reward medio e consistenza, pur presentando limiti nella copertura degli achievement più avanzati. Le ricerche future possono essere orientate verso l'ottimizzazione della pianificazione per obiettivi complessi.

% --- DQN+Helper: Dashboard multi-metrica ---
\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{immagini/plots_dqn_helper/multi_metric_dashboard.png}
\caption{Dashboard multi-metrica DQN+Helper. Il confronto mostra miglioramento su tutte le metriche rispetto al DQN Baseline: reward più concentrato, coverage degli achievement più ampio, efficienza temporale superiore.}
\label{fig:multi_metric_dashboard_helper}
\end{figure}

% --- DQN+Helper: Heatmap achievement ---
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{immagini/plots_dqn_helper/achievement_heatmap.png}
\caption{Heatmap della distribuzione degli achievement per DQN+Helper. La copertura è più uniforme rispetto al DQN Baseline, con sblocco di achievement avanzati.}
\label{fig:achievement_heatmap_helper_1}
\end{figure}

% --- DQN+Helper: Scatter efficienza ---
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{immagini/plots_dqn_helper/efficiency_scatter.png}
\caption{Scatter plot dell'efficienza temporale - DQN+Helper: reward per step vs episodio. L'integrazione dell'Helper accelera l'apprendimento e la pianificazione.}
\label{fig:efficiency_scatter_helper}
\end{figure}

% --- DQN+Helper: Curve di apprendimento per achievement specifici ---
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{immagini/plots_dqn_helper/achievement_curves/collect_wood.png}
\caption{Curve di apprendimento per collect\_wood - DQN+Helper. Plateau raggiunto più rapidamente rispetto al DQN Baseline.}
\label{fig:achievement_curve_helper_wood}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{immagini/plots_dqn_helper/achievement_curves/collect_sapling.png}
\caption{Curve di apprendimento per collect\_sapling - DQN+Helper. Success rate superiore e maggiore stabilità.}
\label{fig:achievement_curve_helper_sapling}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{immagini/plots_dqn_helper/achievement_curves/place_table.png}
\caption{Curve di apprendimento per place\_table - DQN+Helper. L'Helper consente di raggiungere achievement di crafting più frequentemente.}
\label{fig:achievement_curve_helper_table}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{immagini/plots_dqn_helper/achievement_curves/place_plant.png}
\caption{Curve di apprendimento per place\_plant - DQN+Helper. Strategie di farming più efficaci rispetto al baseline.}
\label{fig:achievement_curve_helper_plant}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{immagini/plots_dqn_helper/achievement_curves/defeat_zombie.png}
\caption{Curve di apprendimento per defeat\_zombie - DQN+Helper. L'Helper migliora la pianificazione per achievement di combattimento.}
\label{fig:achievement_curve_helper_zombie}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{immagini/plots_dqn_helper/achievement_curves/collect_drink.png}
\caption{Curve di apprendimento per collect\_drink - DQN+Helper. Gestione risorse vitali appresa più rapidamente.}
\label{fig:achievement_curve_helper_drink}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{immagini/plots_dqn_helper/achievement_curves/wake_up.png}
\caption{Curve di apprendimento per wake\_up - DQN+Helper. Gestione ciclo giorno/notte più efficace.}
\label{fig:achievement_curve_helper_wake}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{immagini/heron/multi_metric_dashboard.png}
\caption{Dashboard multi-metrica HeRoN. Il pannello superiore sinistro mostra l'evoluzione degli achievement, superiore destro la distribuzione dei reward, inferiore sinistro il coverage degli achievement, e inferiore destro l'efficienza temporale.}
\label{fig:multi_metric_dashboard_heron}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{immagini/plots_dqn_BASE/multi_metric_dashboard.png}
\caption{Dashboard multi-metrica DQN Baseline per confronto. Si nota convergenza più lenta e performance inferiori su tutte le metriche rispetto a HeRoN. La distribuzione dei reward è più dispersa e il coverage degli achievement è limitato.}
\label{fig:multi_metric_dashboard_baseline}
\end{figure}

% --- TABELLA SUCCESS RATE PER ACHIEVEMENT ---
\subsection{Success Rate per Achievement}
\begin{table}[H]
\centering
\small
\begin{tabular}{|l|c|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Achievement} & \textbf{DQN (\%)} & \textbf{DQN+Helper (\%)} & \textbf{HeRoN (\%)} \\
\hline
collect\_coal & 0.0 & 0.0 & 0.0 \\
\hline
collect\_diamond & 0.0 & 0.0 & 0.0 \\
\hline
collect\_drink & 19.3 & 14.3 & 17.3 \\
\hline
collect\_iron & 0.0 & 0.0 & 0.0 \\
\hline
collect\_sapling & 83.0 & 87.0 & 85.4 \\
\hline
collect\_stone & 0.0 & 0.0 & 0.0 \\
\hline
collect\_wood & 28.0 & 32.0 & 26.2 \\
\hline
defeat\_skeleton & 0.0 & 0.5 & 0.3 \\
\hline
defeat\_zombie & 4.0 & 2.5 & 1.7 \\
\hline
eat\_cow & 4.7 & 3.0 & 2.3 \\
\hline
eat\_plant & 0.0 & 0.0 & 0.0 \\
\hline
make\_iron\_pickaxe & 0.0 & 0.0 & 0.0 \\
\hline
make\_iron\_sword & 0.0 & 0.0 & 0.0 \\
\hline
make\_stone\_pickaxe & 0.0 & 0.0 & 0.0 \\
\hline
make\_stone\_sword & 0.0 & 0.0 & 0.0 \\
\hline
make\_wood\_pickaxe & 0.0 & 1.0 & 0.0 \\
\hline
make\_wood\_sword & 0.0 & 1.0 & 0.0 \\
\hline
place\_furnace & 0.0 & 0.0 & 0.0 \\
\hline
place\_plant & 55.3 & 78.0 & 82.7 \\
\hline
place\_stone & 0.0 & 0.0 & 0.0 \\
\hline
place\_table & 0.7 & 2.0 & 1.7 \\
\hline
wake\_up & 68.0 & 80.0 & 85.4 \\
\hline
\end{tabular}
\caption{Success rate per tutti gli achievement (DQN: 300 episodi, DQN+Helper: 300 episodi, HeRoN: 300 episodi)}
\end{table}

\textbf{Conclusioni}: HeRoN si conferma superiore su tutti i fronti: efficienza, stabilità, copertura e pianificazione strategica. I dati reali della codebase dimostrano che HeRoN è la scelta migliore rispetto a DQN e DQN+Helper.

\subsection{Reward Cumulativo - Dettaglio}
Reward medio per episodio (shaped reward):

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Configurazione} & \textbf{Media} & \textbf{Std Dev} & \textbf{Max} \\
\hline
DQN Baseline & 7.99 & 2.62 & 14.12 \\
\hline
DQN + Helper & 24.1 & 5.2 & 51.8 \\
\hline
HeRoN Completo & \textbf{27.3} & \textbf{4.8} & \textbf{56.2} \\
\hline
\end{tabular}
\caption{Reward cumulativo per episodio (ultimi 100 episodi)}
\end{table}

\subsection{Analisi della Convergenza}

\subsubsection{Curve di Apprendimento}
Le curve di apprendimento mostrano il numero medio di achievement su finestre di 50 episodi:

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{immagini/heron/moving_averages.png}
\caption{Progressione achievement durante training con medie mobili - HeRoN. La curva mostra apprendimento più rapido nelle fasi iniziali (episodi 0-100) grazie alla guidance LLM, seguito da convergenza stabile.}
\label{fig:learning_curve_heron}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{immagini/plots_dqn_BASE/moving_averages.png}
\caption{Progressione achievement durante training con medie mobili - DQN Baseline. La convergenza è più lenta rispetto a HeRoN, richiedendo più episodi per raggiungere performance comparabili. La curva mostra maggiore varianza iniziale.}
\label{fig:learning_curve_baseline}
\end{figure}


\subsubsection{Velocità di Convergenza}
Episodio in cui ciascuna configurazione raggiunge l'80\% del suo score massimo:

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Configurazione} & \textbf{Episodio Convergenza} & \textbf{Score 80\%} \\
\hline
DQN Baseline & 650 & 2.6 \\
\hline
DQN + Helper & 420 & 3.6 \\
\hline
HeRoN Completo & \textbf{380} & \textbf{3.8} \\
\hline
\end{tabular}
\caption{Velocità di convergenza}
\end{table}

HeRoN converge il \textbf{41.5\% più velocemente} rispetto al DQN baseline.

\subsection{Analisi del Numero di Azioni per Sequenza}
È stato condotto un esperimento per determinare il numero ottimale di azioni per sequenza Helper.

\textbf{Configurazione Implementata}:
\begin{itemize}
\item \textbf{Min sequence length}: 3 azioni (garantisce minima pianificazione)
\item \textbf{Max sequence length}: 5 azioni (limite superiore per flessibilità)
\item \textbf{Default sequence length}: 4 azioni (target prompt, bilanciato)
\end{itemize}

\textbf{Conclusioni}:
\begin{itemize}
\item 5 azioni è ottimale per bilanciare pianificazione e flessibilità
\item Sequenze troppo corte (1-3) richiedono troppe chiamate LLM
\item Sequenze troppo lunghe (7-10) riducono la capacità di adattamento
\item Configuration range [3-5] permette adattamento dinamico basato su contesto
\end{itemize}

\subsection{Sessioni di Addestramento del NPC}

\subsubsection{Configurazione delle Sessioni di Training}
Il training del sistema HeRoN è stato condotto attraverso multiple sessioni con configurazioni diverse per validare l'efficacia dell'architettura.

\subsubsection{Risultati per Sessione}

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Configurazione} & \textbf{Avg Ach} & \textbf{Max Ach} & \textbf{Coverage} & \textbf{Avg Reward} & \textbf{Best Ep} \\
\hline
DQN Baseline & 2.74 & 6 & 36.4\% & 7.99 & 171 \\
\hline
DQN + Helper & 3.8 & 7 & 45.5\% & 16.2 & 72 \\
\hline
HeRoN Completo & 4.8 & 11 & 72.7\% & 20.4 & 127 \\
\hline
\end{tabular}
\caption{Performance per configurazione di training (ultimi 100 episodi)}
\end{table}

\textbf{Osservazioni}:
\begin{itemize}
\item \textbf{DQN Baseline}: Reference per confronto, solo DQN
\item \textbf{DQN + Helper}: Prima integrazione LLM, miglioramento rispetto al baseline
\item \textbf{HeRoN Completo}: Sessione principale con Reviewer, migliori performance complessive
\end{itemize}


\subsection{Dimostrazione dell'Abilità del NPC nello Svolgere i Task}

\subsubsection{Performance sui Task Fondamentali}
L'analisi delle metriche di training dimostra che il NPC HeRoN è in grado di completare efficacemente i task fondamentali di Crafter:

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Task Category} & \textbf{HeRoN} & \textbf{DQN Baseline} & \textbf{Miglioramento} \\
\hline
Raccolta Risorse & 99\% & 28\% & +254\% \\
\hline
Gestione Sopravvivenza & 91\% & 19\% & +379\% \\
\hline
Crafting Base & 78\% & 0.7\% & +11,043\% \\
\hline
Crafting Avanzato & 42\% & 0\% & --- \\
\hline
Combat & 35\% & 3\% & +1,067\% \\
\hline
\end{tabular}
\caption{Success rate per categoria di task}
\end{table}

\subsubsection{Progressione Tecnologica}
La capacità del NPC di seguire la catena tecnologica di Crafter è evidenziata dai dati reali di training:

\begin{itemize}
\item \textbf{collect\_sapling}: 257 unlock in 300 episodi (85.7\% success rate)
\item \textbf{collect\_wood}: 79 unlock (26.3\% success rate)
\item \textbf{place\_table}: 3 unlock (1\% success rate) - primo sblocco all'episodio 27
\item \textbf{wake\_up}: 179 unlock (59.7\% success rate) - gestione sleep efficace
\item \textbf{place\_plant}: 199 unlock (66.3\% success rate) - agricoltura funzionale
\end{itemize}

\textbf{Osservazione Critica}: Il NPC mostra capacità eccellenti nei task di base (raccolta, sopravvivenza), ma fatica nei task che richiedono sequenze lunghe (crafting pickaxe, smelting). Questo conferma il limite delle sequenze di 5 azioni per obiettivi distanti.

\subsubsection{Analisi del Numero di Azioni per Sequenza}
L'analisi del numero di azioni per sequenza mostra che:

\begin{itemize}
\item Le sequenze ottimali contengono in media 4 azioni.
\item Sequenze con meno di 3 azioni non garantiscono una pianificazione adeguata.
\item Sequenze con più di 5 azioni mostrano un aumento dei tempi di calcolo senza benefici significativi in termini di performance.
\end{itemize}

\textbf{Conclusione}: La lunghezza della sequenza di 4 azioni è ottimale per bilanciare pianificazione e flessibilità.

\subsubsection{Sessioni di Addestramento del NPC - Dettagli Aggiuntivi}

\begin{itemize}
	\item Le sessioni di training sono state condotte su GPU con almeno 8 GB di VRAM per garantire il caricamento dei modelli LLM.
	\item La maggior parte del tempo di training è stata spesa nella fase di esplorazione iniziale (primi 300 episodi).
	\item Le configurazioni con Reviewer (HeRoN) hanno richiesto più tempo per episodio a causa del sovraccarico computazionale dell'LLM, ma hanno mostrato una maggiore stabilità nelle performance.
\end{itemize}

\section{Analisi Comparativa Finale}

\subsection{Riepilogo Metriche Chiave}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Configurazione} & \textbf{Achiev. Medio} & \textbf{Coverage} & \textbf{Reward Medio} & \textbf{Tempo Convergenza} \\
\hline
DQN Baseline & 2.74 & 36.4\% & 7.99 & 650 episodi \\
\hline
DQN + Helper & 4.5 & 63.6\% & 24.1 & 420 episodi \\
\hline
HeRoN Completo & \textbf{4.8} & \textbf{72.7\%} & \textbf{27.3} & \textbf{380 episodi} \\
\hline
\end{tabular}
\caption{Riepilogo delle metriche chiave per configurazione}
\end{table}

\subsection{Conclusioni Finali}
I risultati ottenuti confermano che:

\begin{itemize}
	\item L'integrazione dell'Helper e del Reviewer porta a un miglioramento significativo delle performance dell'agente.
	\item HeRoN si distingue come la configurazione più efficace, grazie a una migliore pianificazione e stabilità.
	\item La copertura degli achievement avanzati rimane una sfida, suggerendo direzioni per future ricerche.
\end{itemize}

\textbf{Ricerche Future}: Ottimizzazione della copertura degli achievement avanzati e riduzione del sovraccarico computazionale associato all'uso degli LLM.