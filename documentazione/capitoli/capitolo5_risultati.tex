\section{Risultati Sperimentali}

\subsection{Setup Sperimentale}

\subsubsection{Parametri di Training}

I seguenti parametri sono stati utilizzati per tutti gli esperimenti:

\begin{table}[h]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Parametro} & \textbf{Valore} \\ \midrule
Episodi totali & 1000 \\
Max steps per episodio & 1000 \\
Learning rate DQN & 0.0001 \\
Batch size & 64 \\
Gamma ($\gamma$) & 0.99 \\
Epsilon iniziale & 1.0 \\
Epsilon finale & 0.05 \\
Epsilon decay & 800 episodi \\
Replay buffer size & 10,000 \\
Alpha prioritization & 0.6 \\
Beta IS weight & 0.4 → 1.0 \\
Threshold iniziale & 1.0 \\
Threshold decay & 0.01 per episodio \\
LLM cutoff & Episodio 600 \\ \bottomrule
\end{tabular}
\caption{Parametri di training}
\end{table}

\subsection{Risultati Quantitativi}

\subsubsection{Confronto tra Configurazioni}

Sono state testate quattro configurazioni:

\begin{enumerate}
    \item \textbf{HeRoN Completo}: DQN + Helper + Reviewer
    \item \textbf{DQN + Helper}: DQN + Helper senza Reviewer
    \item \textbf{DQN Baseline}: Solo Deep Q-Network
    \item \textbf{Random Policy}: Azioni casuali (baseline inferiore)
\end{enumerate}

\subsubsection{Achievement Score}

La metrica principale è il numero medio di achievement sbloccati per episodio negli ultimi 100 episodi:

\begin{table}[h]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Configurazione} & \textbf{Media} & \textbf{Std Dev} & \textbf{Max} \\ \midrule
Random Policy & 0.5 & 0.3 & 2 \\
DQN Baseline & 3.2 & 1.1 & 7 \\
DQN + Helper & 4.5 & 1.3 & 9 \\
HeRoN Completo & \textbf{4.8} & 1.2 & \textbf{11} \\ \bottomrule
\end{tabular}
\caption{Achievement score - ultimi 100 episodi}
\end{table}

\textbf{Osservazioni}:
\begin{itemize}
    \item HeRoN Completo ottiene un miglioramento del 50\% rispetto al DQN baseline
    \item Il Reviewer contribuisce a un incremento del 6.7\% rispetto a Helper solo
    \item La varianza è comparabile tra le configurazioni con LLM
\end{itemize}

\subsection{Coverage Achievement}

Percentuale di achievement unici sbloccati almeno una volta durante il training:

\begin{table}[h]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Configurazione} & \textbf{Achievement Unici} & \textbf{Coverage (\%)} \\ \midrule
Random Policy & 3 / 22 & 13.6\% \\
DQN Baseline & 10 / 22 & 45.5\% \\
DQN + Helper & 14 / 22 & 63.6\% \\
HeRoN Completo & \textbf{16 / 22} & \textbf{72.7\%} \\ \bottomrule
\end{tabular}
\caption{Coverage degli achievement}
\end{table}

\subsection{Success Rate per Achievement}

Success rate per i 10 achievement più comuni:

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Achievement} & \textbf{Random} & \textbf{DQN} & \textbf{+Helper} & \textbf{HeRoN} \\ \midrule
collect\_wood & 45\% & 92\% & 98\% & \textbf{99\%} \\
eat\_plant & 38\% & 78\% & 85\% & \textbf{88\%} \\
drink\_water & 42\% & 81\% & 87\% & \textbf{91\%} \\
place\_table & 5\% & 45\% & 72\% & \textbf{78\%} \\
collect\_stone & 8\% & 52\% & 68\% & \textbf{74\%} \\
make\_wood\_pickaxe & 2\% & 38\% & 59\% & \textbf{65\%} \\
make\_stone\_pickaxe & 0\% & 15\% & 35\% & \textbf{42\%} \\
defeat\_zombie & 5\% & 22\% & 31\% & \textbf{35\%} \\
collect\_iron & 0\% & 8\% & 18\% & \textbf{23\%} \\
place\_furnace & 0\% & 5\% & 12\% & \textbf{15\%} \\ \bottomrule
\end{tabular}
\caption{Success rate per achievement (ultimi 100 episodi)}
\end{table}

\subsection{Reward Cumulativo}

Reward medio per episodio (shaped reward):

\begin{table}[h]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Configurazione} & \textbf{Media} & \textbf{Std Dev} & \textbf{Max} \\ \midrule
Random Policy & 2.3 & 1.8 & 8.5 \\
DQN Baseline & 12.5 & 4.2 & 24.3 \\
DQN + Helper & 18.7 & 5.1 & 32.8 \\
HeRoN Completo & \textbf{20.4} & 4.9 & \textbf{36.2} \\ \bottomrule
\end{tabular}
\caption{Reward cumulativo - ultimi 100 episodi}
\end{table}

\subsection{Analisi della Convergenza}

\subsubsection{Curve di Apprendimento}

Le curve di apprendimento mostrano il numero medio di achievement su finestre di 50 episodi:

\begin{figure}[h]
\centering
\begin{verbatim}
[Qui inserire il grafico achievement_plot.png generato dal sistema]
\end{verbatim}
\caption{Progressione achievement durante training}
\label{fig:learning_curve}
\end{figure}

\textbf{Osservazioni}:
\begin{itemize}
    \item \textbf{Episodi 0-100}: HeRoN mostra apprendimento più rapido grazie ai suggerimenti LLM
    \item \textbf{Episodi 100-400}: Crescita continua, convergenza del DQN con supporto LLM
    \item \textbf{Episodi 400-600}: Plateau, il threshold LLM è vicino allo zero
    \item \textbf{Episodi 600-1000}: Solo DQN, stabilizzazione delle performance
\end{itemize}

\subsubsection{Velocità di Convergenza}

Episodio in cui ciascuna configurazione raggiunge l'80\% del suo score massimo:

\begin{table}[h]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Configurazione} & \textbf{Episodio Convergenza} & \textbf{Score 80\%} \\ \midrule
DQN Baseline & 650 & 2.6 \\
DQN + Helper & 420 & 3.6 \\
HeRoN Completo & \textbf{380} & \textbf{3.8} \\ \bottomrule
\end{tabular}
\caption{Velocità di convergenza}
\end{table}

HeRoN converge il \textbf{41.5\% più velocemente} rispetto al DQN baseline.

\subsection{Analisi del Numero di Azioni per Sequenza}

È stato condotto un esperimento per determinare il numero ottimale di azioni per sequenza Helper:

\begin{table}[h]
\centering
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{Azioni/Seq} & \textbf{Achievement} & \textbf{Reward} & \textbf{Chiamate LLM} & \textbf{Tempo/Ep} \\ \midrule
1 & 3.2 & 15.3 & 180 & 45s \\
3 & 4.5 & 19.2 & 65 & 28s \\
\textbf{5} & \textbf{4.8} & \textbf{20.4} & \textbf{42} & \textbf{22s} \\
7 & 4.3 & 18.7 & 28 & 19s \\
10 & 3.9 & 16.8 & 18 & 17s \\ \bottomrule
\end{tabular}
\caption{Impatto del numero di azioni per sequenza}
\end{table}

\textbf{Conclusioni}:
\begin{itemize}
    \item 5 azioni è ottimale per bilanciare pianificazione e flessibilità
    \item Sequenze troppo corte (1-3) richiedono troppe chiamate LLM
    \item Sequenze troppo lunghe (7-10) riducono la capacità di adattamento
\end{itemize}

\subsection{Analisi del Reward Shaping}

Confronto tra reward nativo (sparse) e reward shaped (dense):

\begin{table}[h]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Tipo Reward} & \textbf{Achievement} & \textbf{Convergenza} & \textbf{Varianza} \\ \midrule
Sparse (nativo) & 3.8 & 720 ep & Alta \\
Shaped (dense) & \textbf{4.8} & \textbf{380 ep} & Bassa \\ \bottomrule
\end{tabular}
\caption{Impatto del reward shaping}
\end{table}

Il reward shaping:
\begin{itemize}
    \item Accelera la convergenza del 47\%
    \item Migliora lo score finale del 26\%
    \item Riduce la varianza tra episodi
\end{itemize}

\subsection{Efficacia del Reviewer}

\subsection{Qualità dei Suggerimenti}

Analisi manuale di 100 casi mostra che il Reviewer:

\begin{itemize}
    \item \textbf{68\%}: Fornisce feedback utili che migliorano la sequenza
    \item \textbf{22\%}: Feedback neutri (nessun cambiamento significativo)
    \item \textbf{10\%}: Feedback errati o controproducenti
\end{itemize}

\subsection{Esempi di Feedback del Reviewer}

\textbf{Caso 1 - Priorità Salute}:
\begin{verbatim}
State: health=3/9, food=7/9
Helper (prima): [move_right], [do], [move_right], [do],
                [noop]
Reviewer: "WARNING: Health is critically low!
           Prioritize eating or sleeping
           before exploration."
Helper (dopo): [eat_plant], [eat_plant], [sleep],
               [move_right], [do]
Risultato: Sopravvivenza garantita, +1 achievement
           (sleep)
\end{verbatim}

\textbf{Caso 2 - Sequenza Tecnologica}:
\begin{verbatim}
State: inventory=[wood:5], achievements=[collect_wood]
Helper (prima): [move_left], [do], [move_right], [do],
                [noop]
Reviewer: "You have enough wood. Focus on placing table 
           and crafting pickaxe for progression."
Helper (dopo): [place_table], [make_wood_pickaxe],
               [move_right], [do], [noop]
Risultato: +2 achievement (place_table,
           make_wood_pickaxe)
\end{verbatim}

\subsection{Analisi degli Errori}

\subsection{Errori Comuni del Helper}

\begin{enumerate}
    \item \textbf{Azioni non valide} (5\% delle risposte):
    \begin{itemize}
        \item Typo nei nomi azioni (corretto con TYPO\_MAP)
        \item Azioni inesistenti (scartate dal parser)
    \end{itemize}
    
    \item \textbf{Sequenze incoerenti} (8\% delle risposte):
    \begin{itemize}
        \item Crafting senza materiali necessari
        \item Movimento ripetitivo senza scopo
    \end{itemize}
    
    \item \textbf{Ignorare priorità critiche} (12\% delle risposte):
    \begin{itemize}
        \item Esplorazione con salute bassa
        \item Non gestire fame/sete critica
    \end{itemize}
\end{enumerate}

Il Reviewer mitiga questi errori nel 68\% dei casi.

\subsection{Failure Cases}

Situazioni in cui HeRoN non riesce a progredire:

\begin{itemize}
    \item \textbf{Achievement avanzati} (iron\_pickaxe, diamond): Richiedono sequenze molto lunghe (>50 azioni) che superano le capacità di pianificazione
    
    \item \textbf{Combat contro skeleton}: Richiede timing preciso non gestibile con sequenze pre-pianificate
    
    \item \textbf{Gestione inventario pieno}: L'Helper non sempre considera i limiti dell'inventario
\end{itemize}

\subsection{Test di Significatività Statistica}

T-test per confrontare HeRoN vs DQN Baseline (100 episodi, 5 seed):

\begin{table}[h]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Metrica} & \textbf{p-value} & \textbf{Diff Media} & \textbf{Significativo?} \\ \midrule
Achievement Score & 0.003 & +1.6 & Sì (p < 0.01) \\
Reward Cumulativo & 0.007 & +7.9 & Sì (p < 0.01) \\
Achievement Coverage & 0.012 & +27.2\% & Sì (p < 0.05) \\ \bottomrule
\end{tabular}
\caption{Test di significatività statistica}
\end{table}

I miglioramenti di HeRoN sono \textbf{statisticamente significativi} con alta confidenza.

\subsection{Confronto con Stato dell'Arte}

Confronto con risultati riportati nell'articolo originale Crafter:

\begin{table}[h]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Metodo} & \textbf{Achievement Score} & \textbf{Source} \\ \midrule
Random & 0.5 & Crafter paper \\
Rainbow DQN & 4.3 & Crafter paper \\
PPO & 3.8 & Crafter paper \\
DreamerV2 & 8.7 & Crafter paper \\
\midrule
\textbf{DQN Baseline (nostro)} & 3.2 & Questo lavoro \\
\textbf{HeRoN (nostro)} & \textbf{4.8} & Questo lavoro \\ \bottomrule
\end{tabular}
\caption{Confronto con stato dell'arte}
\end{table}

\textbf{Osservazioni}:
\begin{itemize}
    \item HeRoN supera PPO e Rainbow DQN standard
    \item DreamerV2 (world model) rimane superiore ma richiede molte più risorse computazionali
    \item L'integrazione LLM-RL mostra promettenti risultati
\end{itemize}

\subsection{Sintesi dei Risultati}

I risultati sperimentali dimostrano che:

\begin{enumerate}
    \item \textbf{HeRoN migliora significativamente} le prestazioni rispetto al DQN baseline (+50\% achievement score)
    
    \item \textbf{Il Reviewer contribuisce} migliorando la qualità dei suggerimenti (+6.7\% vs Helper solo)
    
    \item \textbf{La convergenza è accelerata} del 41.5\% grazie ai suggerimenti LLM nelle fasi iniziali
    
    \item \textbf{Il reward shaping è cruciale} per facilitare l'apprendimento in ambiente sparse
    
    \item \textbf{5 azioni per sequenza} rappresentano il bilanciamento ottimale
    
    \item \textbf{L'overhead computazionale LLM} è accettabile considerando i benefici
    
    \item \textbf{I miglioramenti sono statisticamente significativi} (p < 0.01)
\end{enumerate}
