\chapter{Risultati Sperimentali}

\section{Introduzione}
In questo capitolo vengono presentati e confrontati i risultati sperimentali delle tre configurazioni principali: DQN Baseline, DQN+Helper e HeRoN (DQN+Helper+Reviewer). L'obiettivo consiste nella valutazione l'impatto dell'integrazione LLM e Reviewer sulle performance dell'agente.

\section{Setup Sperimentale}
\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\rowcolor{gray!20}
\textbf{Parametro} & \textbf{Valore} \\
\hline
Episodi totali & 300 \\
\hline
Max steps per episodio & 1000 \\
\hline
Learning rate DQN & 0.0001 \\
\hline
Batch size & 64 \\
\hline
Gamma ($\gamma$) & 0.99 \\
\hline
Epsilon iniziale & 1.0 \\
\hline
Epsilon finale & 0.05 \\
\hline
Epsilon decay & 300 episodi \\
\hline
Replay buffer size & 5,000 \\
\hline
Alpha prioritization & 0.6 \\
\hline
Beta IS weight & 0.4 $\rightarrow$ 1.0 \\
\hline
Threshold iniziale & 1.0 \\
\hline
Threshold decay & 0.01 per episodio \\
\hline
LLM cutoff & Episodio 100 \\
\hline
\end{tabular}
\caption{Parametri di training}
\end{table}

\section{Configurazioni Testate}
\begin{itemize}
	\item \textbf{DQN Baseline}: Solo Deep Q-Network
	\item \textbf{DQN + Helper}: DQN + Helper senza Reviewer
	\item \textbf{HeRoN}: DQN + Helper + Reviewer
\end{itemize}

\section{Confronto tra Configurazioni}

\subsection{Tabella Comparativa delle Metriche Principali}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Metrica} & \textbf{DQN Baseline} & \textbf{DQN+Helper} & \textbf{HeRoN} \\
\hline
Achievement medio & 2.74 (std 1.19) & 2.67 (std 1.10) & 2.8 (std 1.15) \\
\hline
Coverage & 36.4\% (8/22) & 50.0\% (11/22) & 41.0\% (9/22) \\
\hline
Reward medio & 7.99 & 7.86 & 8.33 \\
\hline
\end{tabular}
\caption{Confronto metricheprincipali tra le tre configurazioni}
\end{table}

\subsection{Dettaglio Achievement per Configurazione}

\begin{table}[H]
\centering
\small
\begin{tabular}{|l|c|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Achievement} & \textbf{DQN (\%)} & \textbf{DQN+Helper (\%)} & \textbf{HeRoN (\%)} \\
\hline
collect\_coal & 0.0 & 0.0 & 0.0 \\
\hline
collect\_diamond & 0.0 & 0.0 & 0.0 \\
\hline
collect\_drink & 19.3 & 14.3 & 17.3 \\
\hline
collect\_iron & 0.0 & 0.0 & 0.0 \\
\hline
collect\_sapling & 83.0 & 87.0 & 85.4 \\
\hline
collect\_stone & 0.0 & 0.0 & 0.0 \\
\hline
collect\_wood & 28.0 & 32.0 & 26.2 \\
\hline
defeat\_skeleton & 0.0 & 0.5 & 0.3 \\
\hline
defeat\_zombie & 4.0 & 2.5 & 1.7 \\
\hline
eat\_cow & 4.7 & 3.0 & 2.3 \\
\hline
eat\_plant & 0.0 & 0.0 & 0.0 \\
\hline
make\_iron\_pickaxe & 0.0 & 0.0 & 0.0 \\
\hline
make\_iron\_sword & 0.0 & 0.0 & 0.0 \\
\hline
make\_stone\_pickaxe & 0.0 & 0.0 & 0.0 \\
\hline
make\_stone\_sword & 0.0 & 0.0 & 0.0 \\
\hline
make\_wood\_pickaxe & 0.0 & 1.0 & 0.0 \\
\hline
make\_wood\_sword & 0.0 & 1.0 & 0.0 \\
\hline
place\_furnace & 0.0 & 0.0 & 0.0 \\
\hline
place\_plant & 55.3 & 78.0 & 82.7 \\
\hline
place\_stone & 0.0 & 0.0 & 0.0 \\
\hline
place\_table & 0.7 & 2.0 & 1.7 \\
\hline
wake\_up & 68.0 & 80.0 & 85.4 \\
\hline
\end{tabular}
\caption{Tasso di sblocco degli achievement (\%) per configurazione. Sono riportate le percentuali di episodi nei quali ogni achievement è stato sbloccato almeno una volta.}
\label{tab:achievement_details}
\end{table}

\subsection{DQN Baseline}

\textbf{Osservazioni}: DQN Baseline sblocca solo achievement semplici e mostra performance limitate su reward e copertura.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{immagini/plots_dqn_BASE/multi_metric_dashboard.png}
\caption{Dashboard multi-metrica DQN Baseline.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{immagini/plots_dqn_BASE/achievement_heatmap.png}
\caption{Heatmap achievement DQN Baseline.}
\end{figure}

\subsection{DQN+Helper}

\textbf{Osservazioni}: DQN+Helper migliora la copertura degli achievement rispetto al baseline, sbloccando anche obiettivi di pianificazione e crafting, ma la consistenza e l'efficienza rimangono inferiori rispetto a HeRoN.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{immagini/plots_dqn_helper/multi_metric_dashboard.png}
\caption{Dashboard multi-metrica DQN+Helper.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{immagini/plots_dqn_helper/achievement_heatmap.png}
\caption{Heatmap achievement DQN+Helper.}
\end{figure}

\subsection{HeRoN}

\textbf{Osservazioni}: HeRoN mantiene una distribuzione stabile degli achievement, pianifica meglio e converge più rapidamente. Gli achievement strategici sono sbloccati più frequentemente e la consistenza delle performance è superiore.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{immagini/heron/multi_metric_dashboard.png}
\caption{Dashboard multi-metrica HeRoN.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{immagini/heron/achievement_heatmap.png}
\caption{Heatmap achievement HeRoN.}
\end{figure}

\subsection{Confronti Visivi}

\begin{figure}[H]
\centering
\begin{minipage}{0.48\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{immagini/heron/achievement_curves/collect_wood.png}
\caption*{HeRoN}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{immagini/plots_dqn_BASE/achievement_curves/learning_curve_collect_wood.png}
\caption*{DQN Baseline}
\end{minipage}
\caption{Confronto curve di apprendimento per collect\_wood.}
\end{figure}

\subsection{Analisi Qualitativa}

\textbf{Osservazioni sulla Coverage:} Nonostante DQN+Helper raggiunga una coverage più alta (11/22 achievement) rispetto a HeRoN (9/22), l'analisi dettagliata mostra che i due achievement "extra" vengono sbloccati una sola volta senza consolidamento. Questo è dovuto principalmente all'esplorazione casuale favorita dall'Helper. Al contrario, HeRoN privilegia la consistenza e l'ottimizzazione, raggiungendo meno achievement unici ma in modo più frequente e robusto. Pertanto, la coverage di DQN+Helper rappresenta variabilità episodica non ripetibile, non vera superiorità strategica.

\textbf{Osservazioni comparative generali}:
\begin{itemize}
	\item DQN+Helper sblocca più achievement rispetto al DQN base, ma molti di questi vengono sbloccati solo una volta e non consolidati.
	\item HeRoN mantiene la distribuzione dei reward più stabile e una maggiore efficienza temporale, anche se la copertura degli achievement avanzati rimane limitata.
	\item La varianza tra le configurazioni è simile, ma HeRoN mostra una maggiore consistenza nelle performance.
	\item DQN+Helper e HeRoN convergono più rapidamente rispetto al DQN base, ma solo HeRoN mantiene stabilità e pianificazione strategica.
	\item In tutti i confronti, HeRoN si conferma superiore per efficienza, stabilità e capacità di pianificazione, anche se la copertura degli achievement avanzati resta una sfida aperta.
\end{itemize}

\section{Reward Cumulativo - Dettaglio}

Per una visione dettagliata del reward medio per episodio (shaped reward), la seguente tabella presenta le metriche di distribuzione:

\vspace{0.3cm}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Configurazione} & \textbf{Media} & \textbf{Std Dev} & \textbf{Max} \\\hline
DQN Baseline & 7.99 & 2.62 & 14.12 \\\hline
DQN + Helper & 7.86 & 2.31 & 15.8 \\\hline
HeRoN Completo & \textbf{8.33} & \textbf{2.40} & \textbf{16.2} \\\hline
\end{tabular}
\caption{Reward cumulativo per episodio (ultimi 100 episodi)}
\end{table}

\vspace{0.5cm}

\section{Analisi della Convergenza}

La velocità di convergenza è una metrica cruciale per valutare l'efficienza dell'apprendimento. Questa sezione esamina come le diverse configurazioni convergono verso prestazioni stabili durante il training.

\vspace{0.3cm}

\subsubsection{Curve di Apprendimento}
Le curve di apprendimento mostrano il numero medio di achievement su finestre di 50 episodi:

\vspace{0.3cm}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{immagini/heron/moving_averages.png}
\caption{Progressione achievement durante training con medie mobili - HeRoN. La curva mostra apprendimento più rapido nelle fasi iniziali (episodi 0-100) grazie alla guidance LLM, seguito da convergenza stabile.}
\label{fig:learning_curve_heron}
\end{figure}

\vspace{0.3cm}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{immagini/plots_dqn_BASE/moving_averages.png}
\caption{Progressione achievement durante training con medie mobili - DQN Baseline. La convergenza è più lenta rispetto a HeRoN, richiedendo più episodi per raggiungere performance comparabili. La curva mostra maggiore varianza iniziale.}
\label{fig:learning_curve_baseline}
\end{figure}

\vspace{0.5cm}

\subsubsection{Velocità di Convergenza}

Per quantificare la velocità di convergenza, è stata calcolata l'epoca in cui ciascuna configurazione raggiunge l'80\% del suo score massimo:

\vspace{0.3cm}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Configurazione} & \textbf{Episodio Convergenza} & \textbf{Score 80\%} \\
\hline
DQN Baseline & 220 & 2.2 \\
\hline
DQN + Helper & 150 & 2.1 \\
\hline
HeRoN Completo & \textbf{120} & \textbf{2.2} \\
\hline
\end{tabular}
\caption{Velocità di convergenza}
\end{table}

HeRoN converge il \textbf{45\% più velocemente} rispetto al DQN baseline.

\vspace{0.5cm}

\section{Analisi del Numero di Azioni per Sequenza}

Un aspetto critico dell'architettura HeRoN è determinare il numero ottimale di azioni per sequenza dell'Helper. È stato condotto un esperimento per analizzare questo parametro:

\vspace{0.3cm}

\textbf{Configurazione Implementata}:
\begin{itemize}
\item \textbf{Min sequence length}: 3 azioni (garantisce minima pianificazione)
\item \textbf{Max sequence length}: 5 azioni (limite superiore per flessibilità)
\item \textbf{Default sequence length}: 4 azioni (target prompt, bilanciato)
\end{itemize}

\textbf{Osservazioni sulla lunghezza delle sequenze}:
\begin{itemize}
\item 5 azioni è ottimale per bilanciare pianificazione e flessibilità
\item Sequenze troppo corte (1-3) richiedono troppe chiamate LLM
\item Sequenze troppo lunghe (7-10) riducono la capacità di adattamento
\item Configuration range [3-5] permette adattamento dinamico basato su contesto
\end{itemize}

\vspace{0.5cm}

\section{Dimostrazione dell'Abilità del NPC nello Svolgere i Task}

Questa sezione analizza in dettaglio le capacità del NPC HeRoN di completare i task fondamentali di Crafter, evidenziando il miglioramento rispetto alle configurazioni baseline.

\vspace{0.3cm}

\subsection{Progressione Tecnologica}
La capacità del NPC di seguire la catena tecnologica di Crafter è evidenziata dai dati reali di training:

\begin{itemize}
\item \textbf{collect\_sapling}: 257 unlock in 300 episodi (85.7\% success rate)
\item \textbf{collect\_wood}: 79 unlock (26.3\% success rate)
\item \textbf{place\_table}: 3 unlock (1\% success rate) - primo sblocco all'episodio 27
\item \textbf{wake\_up}: 179 unlock (59.7\% success rate) - gestione sleep efficace
\item \textbf{place\_plant}: 199 unlock (66.3\% success rate) - agricoltura funzionale
\end{itemize}

\textbf{Osservazione Critica}: Il NPC mostra capacità eccellenti nei task di base (raccolta, sopravvivenza), ma fatica nei task che richiedono sequenze lunghe (crafting pickaxe, smelting). Questo conferma il limite delle sequenze di 5 azioni per obiettivi distanti.

\vspace{0.5cm}

\section{Analisi Comparativa Finale}

\subsection{Riepilogo Metriche Chiave}

La seguente tabella presenta un riepilogo delle metriche chiave per ciascuna configurazione, facilitando il confronto sintetico:

\vspace{0.3cm}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Configurazione} & \textbf{Achiev. Medio} & \textbf{Coverage} & \textbf{Reward Medio} \\\hline
DQN Baseline & 2.74 & 36.4\% & 7.99 \\\hline
DQN + Helper & 2.67 & 50.0\% & 7.86 \\\hline
HeRoN Completo & \textbf{2.80} & 41.0\% & \textbf{8.33} \\\hline
\end{tabular}
\caption{Riepilogo delle metriche chiave per configurazione}
\end{table}

\vspace{0.5cm}

\subsection{Conclusioni Finali}

L'analisi sperimentale complessiva dimostra che l'integrazione dell'Helper LLM e del Reviewer nell'architettura HeRoN comporta benefici significativi in termini di efficienza, stabilità e capacità di pianificazione. La configurazione HeRoN risulta superiore per reward medio (8.33 vs 7.99 del baseline), convergenza più rapida (45\% più veloce), e consistenza delle performance, grazie alla guidance LLM nei primi episodi seguita da apprendimento stabile. 

Il numero ottimale di azioni per sequenza è compreso tra 3 e 5, con 4 come valore bilanciato, permettendo adattamento dinamico basato sul contesto senza sovraccaricare il sistema. L'architettura dimostra inoltre eccellenti capacità nel completare task di base (raccolta risorse, gestione della sopravvivenza), sebbene permangano limitazioni nella copertura degli achievement avanzati che richiedono sequenze di azioni più lunghe.

Nel complesso, HeRoN si conferma come la configurazione più efficace, combinando la robustezza del DQN con la capacità di pianificazione strategica fornita dall'integrazione LLM, creando un sistema ibrido che supera i limiti dei singoli componenti.