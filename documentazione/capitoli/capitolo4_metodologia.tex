\chapter{Metodologia di Implementazione}

\section{Overview del Processo}

L'implementazione del progetto HeRoN per Crafter è stata suddivisa in fasi sequenziali, ciascuna volta a sviluppare e validare un componente specifico dell'architettura. La metodologia adottata segue un approccio iterativo che permette di validare progressivamente le componenti del sistema.

\section{Fase 1: Setup dell'Environment}

\subsection{Installazione e Configurazione}

Il primo passo ha riguardato la configurazione dell'environment di sviluppo:

\begin{enumerate}
    \item \textbf{Creazione ambiente Conda}:
    \begin{verbatim}
    conda create -n HeRoN python=3.9
    conda activate HeRoN
    \end{verbatim}
    
    \item \textbf{Installazione dipendenze}:
    \begin{itemize}
        \item PyTorch (con supporto CUDA per GPU)
        \item Crafter environment
        \item Transformers (per il Reviewer T5)
        \item LM Studio (per l'Helper LLM locale)
    \end{itemize}
    
    \item \textbf{Verifica installazione}:
    \begin{verbatim}
    python test_crafter_env.py
    python test_lmstudio_connection.py
    \end{verbatim}
\end{enumerate}

\subsection{Wrapper dell'Environment}

È stato sviluppato un wrapper custom per Crafter (\texttt{crafter\_environment.py}) che:

\begin{itemize}
    \item Estrae lo stato strutturato a 43 dimensioni dalle osservazioni RGB
    \item Gestisce l'estrazione di informazioni dall'oggetto \texttt{info}
    \item Normalizza i valori di stato per facilitare l'apprendimento
    \item Traccia gli achievement sbloccati
    \item Calcola il reward shaped combinando reward nativo e bonus
\end{itemize}

\subsection{Testing Iniziale}

Prima di procedere con l'implementazione degli agenti, sono stati eseguiti test per verificare:
\begin{itemize}
    \item Correttezza dell'estrazione dello stato
    \item Funzionamento delle azioni
    \item Consistenza del sistema di reward
    \item Performance computazionali
\end{itemize}

\section{Fase 2: Sviluppo del NPC (DQN Agent)}

\subsection{Architettura della Rete Neurale}

L'agente DQN è stato implementato con una rete neurale feedforward:

\begin{verbatim}
Input Layer: 43 neuroni (dimensione stato)
    ↓
Hidden Layer 1: 256 neuroni + ReLU + Dropout(0.1)
    ↓
Hidden Layer 2: 256 neuroni + ReLU + Dropout(0.1)
    ↓
Hidden Layer 3: 128 neuroni + ReLU
    ↓
Output Layer: 17 neuroni (Q-values per azioni)
\end{verbatim}

\subsection{Implementazione Double DQN}

Sono state implementate due reti neurali:
\begin{itemize}
    \item \textbf{Policy Network}: Usata per selezionare azioni e aggiornata ad ogni step
    \item \textbf{Target Network}: Usata per calcolare i target Q-values, aggiornata periodicamente
\end{itemize}

L'aggiornamento della target network avviene ogni $C = 1000$ step tramite soft update:
$$\theta_{target} \leftarrow \tau \theta_{policy} + (1-\tau) \theta_{target}$$
con $\tau = 0.001$.

\subsection{Prioritized Experience Replay}

Il replay buffer implementa prioritized sampling:

\begin{enumerate}
    \item \textbf{Calcolo priorità}: Per ogni transizione, la priorità è basata sul TD-error:
    $$p_i = |\delta_i|^\alpha + \epsilon$$
    dove $\delta_i = r + \gamma \max_{a'} Q_{target}(s', a') - Q(s, a)$
    
    \item \textbf{Sampling}: La probabilità di campionare la transizione $i$ è:
    $$P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha}$$
    
    \item \textbf{Importance Sampling}: Per correggere il bias, i pesi sono:
    $$w_i = \left(\frac{1}{N \cdot P(i)}\right)^\beta$$
\end{enumerate}

\subsection{Training Baseline DQN}

Prima di integrare i componenti LLM, è stato addestrato un agente DQN baseline:

\begin{itemize}
    \item \textbf{Episodi}: 1000
    \item \textbf{Max steps per episodio}: 1000
    \item \textbf{Epsilon decay}: $\epsilon = 1.0 \rightarrow 0.05$ in 800 episodi
    \item \textbf{Learning rate}: $\alpha = 0.0001$
    \item \textbf{Batch size}: 64
\end{itemize}

Il baseline serve come riferimento per valutare l'efficacia dell'integrazione con gli LLM.

\section{Fase 3: Sviluppo del Helper}

\subsection{Setup LM Studio}

LM Studio è stato configurato per servire il modello Llama-3.2-3B-Instruct:
\begin{itemize}
    \item Server locale su \texttt{http://127.0.0.1:1234}
    \item API compatibile con OpenAI
    \item Temperatura: 0.7 (bilanciamento tra creatività e coerenza)
    \item Max tokens: 150 (sufficiente per sequenze di 3-5 azioni)
\end{itemize}

\subsection{Progettazione del Prompt}

Il prompt per l'Helper è stato progettato iterativamente attraverso esperimenti:

\begin{verbatim}
You are an expert Crafter player. Given the current game state,
suggest a sequence of 3-5 actions to achieve progress.

Current State:
- Health: {health}/9
- Food: {food}/9  
- Water: {water}/9
- Position: ({x}, {y})
- Inventory: {inventory_items}
- Achievements unlocked: {achievements}

Available actions:
[move_left, move_right, move_up, move_down, do, sleep,
 place_stone, place_table, place_furnace, place_plant,
 make_wood_pickaxe, make_stone_pickaxe, make_iron_pickaxe,
 make_wood_sword, make_stone_sword, make_iron_sword, noop]

Output format: [action1], [action2], [action3], [action4], [action5]

Suggest actions:
\end{verbatim}

\subsection{Parsing delle Risposte}

È stato implementato un parser robusto per gestire:
\begin{itemize}
    \item Estrazione delle azioni tramite regex: \texttt{r'\textbackslash[(.*?)\textbackslash]'}
    \item Rimozione di tag di ragionamento: \texttt{<think>...</think>}
    \item Correzione typo comuni (13 mappings predefiniti)
    \item Validazione azioni (controllo che siano nell'action space)
    \item Limitazione a 5 azioni massimo
\end{itemize}

\subsection{Meccanismi di Re-planning}

Sono state implementate logiche per interrompere e ri-pianificare:

\begin{algorithm}
\caption{Re-planning durante esecuzione}
\begin{algorithmic}
\WHILE{esecuzione sequenza}
    \STATE $next\_state, reward, done, info \gets env.step(action)$
    \IF{achievement sbloccato}
        \STATE Genera nuova sequenza con contesto aggiornato
        \STATE BREAK
    \ENDIF
    \IF{$health \leq 5$}
        \STATE Fallback a DQN per sopravvivenza immediata
        \STATE BREAK
    \ENDIF
    \IF{$health < 0.3 \times max\_health$}
        \STATE Re-query con priorità gestione salute
        \STATE BREAK
    \ENDIF
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\section{Fase 4: Generazione Dataset per Reviewer}

\subsection{Processo di Raccolta Dati}

Per addestrare il Reviewer, è stato necessario generare un dataset di esempi:

\begin{enumerate}
    \item \textbf{Esecuzione episodi}: 50 episodi di gioco con Helper zero-shot
    \item \textbf{Registrazione}: Per ogni chiamata Helper, salvare:
    \begin{itemize}
        \item Stato dell'environment
        \item Sequenza di azioni suggerite
        \item Risultato dell'esecuzione (reward, achievement)
    \end{itemize}
    \item \textbf{Annotazione}: Generazione di feedback basati su:
    \begin{itemize}
        \item Successo/fallimento della sequenza
        \item Efficienza (step sprecati)
        \item Priorità rispetto allo stato (es. salute bassa ignorata)
    \end{itemize}
\end{enumerate}

\subsection{Struttura del Dataset}

Il dataset (\texttt{game\_scenarios\_dataset\_crafter.jsonl}) contiene esempi nel formato:

\begin{verbatim}
{
    "input": "State: health=5, food=3, inventory=[wood:2]
              Suggestion: [move_right], [do], [move_left]",
    "output": "CRITICAL: Health is low! Prioritize finding
               food or water. Consider sleeping if it's
               night. Suggested: [eat_plant], [drink_water]"
}
\end{verbatim}

Tipicamente, 50 episodi producono circa 2,500 esempi di training.

\section{Fase 5: Fine-tuning del Reviewer}

\subsection{Scelta del Modello Base}

È stato scelto T5-small come modello base per il Reviewer:
\begin{itemize}
    \item Dimensioni gestibili (60M parametri)
    \item Buone capacità di text-to-text generation
    \item Veloce per inference durante il training
\end{itemize}

\subsection{Configurazione Training}

Il fine-tuning è stato eseguito con i seguenti parametri:

\begin{verbatim}
Optimizer: AdamW
Learning rate: 5e-5
Batch size: 8
Epochs: 5
Max input length: 512 tokens
Max output length: 128 tokens
Gradient accumulation steps: 2
\end{verbatim}

\subsection{Validazione}

Il dataset è stato diviso in:
\begin{itemize}
    \item Training set: 80\% (circa 2,000 esempi)
    \item Validation set: 20\% (circa 500 esempi)
\end{itemize}

Metriche monitorate durante il training:
\begin{itemize}
    \item Training loss
    \item Validation loss
    \item BLEU score (similarità con feedback attesi)
\end{itemize}

\section{Fase 6: Training Integrato HeRoN}

\subsection{Protocollo di Training}

Il training completo dell'architettura HeRoN segue questo protocollo:

\begin{algorithm}
\caption{Training Loop HeRoN}
\begin{algorithmic}
\STATE $threshold \gets 1.0$
\STATE $threshold\_decay \gets 0.01$
\STATE $threshold\_episodes \gets 100$
\FOR{$episode = 1$ to $max\_episodes$}
    \STATE $state \gets env.reset()$
    \STATE $action\_sequence \gets []$
    \STATE $sequence\_index \gets 0$
    \FOR{$step = 1$ to $max\_steps$}
        \IF{$len(action\_sequence) == 0$ OR $sequence\_index \geq len(action\_sequence)$}
            \IF{$random() > threshold$ AND $episode < 600$}
                \STATE $action\_sequence \gets$ Helper-Reviewer workflow
                \STATE $sequence\_index \gets 0$
            \ELSE
                \STATE $action \gets$ DQN selection
            \ENDIF
        \ELSE
            \STATE $action \gets action\_sequence[sequence\_index]$
            \STATE $sequence\_index \gets sequence\_index + 1$
        \ENDIF
        \STATE Esegui azione e aggiorna DQN
    \ENDFOR
    \IF{$episode < threshold\_episodes$}
        \STATE $threshold \gets \max(0, threshold - threshold\_decay)$
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Parametri di Training}

\begin{itemize}
    \item \textbf{Episodi totali}: 1000
    \item \textbf{Max steps per episodio}: 1000
    \item \textbf{Threshold decay}: 1.0 → 0.0 in 100 episodi
    \item \textbf{LLM cutoff}: Episodio 600 (dopo, solo DQN)
    \item \textbf{Checkpoint}: Salvataggio ogni 50 episodi + best model
\end{itemize}

\subsection{Analisi del Numero Ottimale di Azioni}

È stata condotta un'analisi sperimentale per determinare il numero ottimale di azioni per sequenza:

\begin{table}[h]
\centering
\begin{tabular}{@{}ccc@{}}
\toprule
Azioni per sequenza & Achievement medi & Chiamate Helper/episodio \\ \midrule
1 & 3.2 & 150-200 \\
3 & 4.5 & 50-80 \\
5 & 4.8 & 30-50 \\
7 & 4.3 & 20-35 \\
10 & 3.9 & 15-25 \\ \bottomrule
\end{tabular}
\caption{Impatto del numero di azioni per sequenza}
\end{table}

Il valore ottimale è risultato essere 5 azioni, che bilancia:
\begin{itemize}
    \item Pianificazione strategica (non troppo breve)
    \item Flessibilità di re-planning (non troppo lungo)
    \item Overhead computazionale LLM
\end{itemize}

\section{Fase 7: Valutazione delle Prestazioni}

\subsection{Metriche di Valutazione}

Per valutare le prestazioni di HeRoN, sono state definite diverse metriche:

\begin{enumerate}
    \item \textbf{Achievement Score}: Numero medio di achievement sbloccati per episodio
    $$\text{Score} = \frac{1}{N} \sum_{i=1}^N \text{achievements}_i$$
    
    \item \textbf{Coverage}: Percentuale di achievement unici sbloccati almeno una volta
    $$\text{Coverage} = \frac{|\text{achievement unici}|}{22} \times 100\%$$
    
    \item \textbf{Success Rate per Achievement}: Percentuale di episodi in cui ciascun achievement è stato sbloccato
    
    \item \textbf{Reward Cumulativo}: Somma dei reward durante l'episodio (shaped e nativo)
    
    \item \textbf{Convergenza}: Episodio in cui lo score medio si stabilizza
\end{enumerate}

\subsection{Baseline di Confronto}

HeRoN è stato confrontato con:
\begin{itemize}
    \item \textbf{DQN puro}: Stesso agente senza componenti LLM
    \item \textbf{Random policy}: Azioni casuali (sanity check)
    \item \textbf{Helper solo}: DQN + Helper senza Reviewer
\end{itemize}

\subsection{Protocollo di Test}

Per garantire validità statistica:
\begin{itemize}
    \item Ogni configurazione testata per 100 episodi
    \item 5 seed casuali diversi
    \item Media e deviazione standard riportate
    \item Test statistici (t-test) per significatività
\end{itemize}

\section{Fase 8: Analisi e Ottimizzazione}

\subsection{Profiling delle Performance}

Sono state analizzate le performance computazionali:
\begin{itemize}
    \item Tempo per step DQN: ~2ms
    \item Tempo per chiamata Helper: ~500ms
    \item Tempo per chiamata Reviewer: ~300ms
    \item Overhead totale LLM: significativo ma accettabile
\end{itemize}

\subsection{Ottimizzazioni Implementate}

\begin{enumerate}
    \item \textbf{Caching delle sequenze}: Evitare ri-query multiple per stati simili
    \item \textbf{Batch processing}: Aggiornamenti DQN in batch per efficienza GPU
    \item \textbf{Early stopping LLM}: Dopo episodio 600, solo DQN per accelerare
    \item \textbf{Checkpoint intelligenti}: Salvare solo modelli con miglioramenti
\end{enumerate}

\subsection{Tuning degli Iperparametri}

Grid search limitata su:
\begin{itemize}
    \item Learning rate DQN: [1e-4, 5e-4, 1e-3]
    \item Threshold decay rate: [0.005, 0.01, 0.02]
    \item Peso reward shaping: [0.5, 1.0, 2.0]
\end{itemize}

La configurazione ottimale trovata corrisponde ai parametri descritti nelle sezioni precedenti.
