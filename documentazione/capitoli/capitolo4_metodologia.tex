\chapter{Metodologia di Implementazione}

\section{Overview del Processo}

L'implementazione del progetto HeRoN per Crafter è stata suddivisa in fasi sequenziali, ciascuna volta a sviluppare e validare un componente specifico dell'architettura. La metodologia adottata è iterativa, per permettere di validare progressivamente le componenti del sistema.

\section{Fase 1: Setup dell'Environment}


\subsection{Testing Iniziale}

Prima di procedere con l'implementazione degli agenti, sono stati eseguiti test per verificare:
\begin{itemize}
    \item Correttezza dell'estrazione dello stato
    \item Funzionamento delle azioni
    \item Consistenza del sistema di reward
    \item Performance computazionali
\end{itemize}

\section{Fase 2: Sviluppo del NPC (DQN Agent)}

\subsection{Architettura della Rete Neurale}

L'agente DQN è stato implementato con una rete neurale feedforward:

\begin{verbatim}
Input Layer: 43 neuroni (dimensione stato)
    ↓
Hidden Layer 1: 256 neuroni + ReLU + Dropout(0.1)
    ↓
Hidden Layer 2: 256 neuroni + ReLU + Dropout(0.1)
    ↓
Hidden Layer 3: 128 neuroni + ReLU
    ↓
Output Layer: 17 neuroni (Q-values per azioni)
\end{verbatim}

\subsubsection{Implementazione Double DQN}

Sono state implementate due reti neurali:
\begin{itemize}
    \item \textbf{Policy Network}: Usata per selezionare azioni e aggiornata ad ogni step
    \item \textbf{Target Network}: Usata per calcolare i target Q-values, aggiornata periodicamente
\end{itemize}

L'aggiornamento della target network avviene ogni $C = 1000$ step tramite soft update:
$$\theta_{target} \leftarrow \tau \theta_{policy} + (1-\tau) \theta_{target}$$
con $\tau = 0.001$.

\subsubsection{Prioritized Experience Replay}

Il replay buffer implementa prioritized sampling:

\begin{enumerate}
    \item \textbf{Calcolo priorità}: Per ogni transizione, la priorità è basata sul TD-error:
    $$p_i = |\delta_i|^\alpha + \epsilon$$
    dove $\delta_i = r + \gamma \max_{a'} Q_{target}(s', a') - Q(s, a)$
    
    \item \textbf{Sampling}: La probabilità di campionare la transizione $i$ è:
    $$P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha}$$
    
    \item \textbf{Importance Sampling}: Per correggere il bias, i pesi sono:
    $$w_i = \left(\frac{1}{N \cdot P(i)}\right)^\beta$$
\end{enumerate}

\subsubsection{Training Baseline DQN}

Prima di integrare i componenti LLM, abbiamo addestrato un agente DQN baseline:

\begin{itemize}
    \item \textbf{Episodi}: 1000
    \item \textbf{Max steps per episodio}: 1000
    \item \textbf{Epsilon decay}: $\epsilon = 1.0 \rightarrow 0.05$ in 800 episodi
    \item \textbf{Learning rate}: $\alpha = 0.0001$
    \item \textbf{Batch size}: 64
\end{itemize}

Il baseline ci serve come riferimento per valutare quanto funziona l'integrazione con gli LLM.

\subsection{Fase 3: Sviluppo del Helper}

\subsubsection{Setup LM Studio}

LM Studio è stato configurato per servire il modello Qwen3-4B-2507:
\begin{itemize}
    \item Server locale su \texttt{http://127.0.0.1:1234}
    \item API compatibile con OpenAI
    \item Temperatura: 0.7 (bilanciamento tra creatività e coerenza)
    \item Max tokens: 150 (sufficiente per sequenze di 3-5 azioni)
    \item Context window: 8192 tokens (gestione conversazioni lunghe)
    \item Tokenizer: Qwen2.5-7B-Instruct (per conteggio token context-aware)
\end{itemize}

\textbf{Selezione del Modello}:

La scelta di Qwen3-4B-2507 è stata preceduta da test con modelli più piccoli che hanno mostrato limitazioni significative:

\begin{itemize}
    \item \textbf{Llama-3.2-1B}: Non rispettava il formato richiesto (90\% risposte invalide), generava spiegazioni verbose invece di sequenze bracketed
    \item \textbf{Phi-3-mini (3.8B)}: Difficoltà nel seguire istruzioni complesse, 68\% azioni invalide, frequenti allucinazioni
    \item \textbf{TinyLlama-1.1B}: Output completamente incoerente, incapace di comprendere il formato bracketed
    \item \textbf{Qwen2.5-3B}: Miglioramento rispetto ai precedenti ma ancora 42\% errori nel formato
\end{itemize}

Qwen3-4B-2507 è risultato il modello più piccolo capace di:
\begin{itemize}
    \item Rispettare consistentemente il formato bracketed richiesto (98\% conformità)
    \item Evitare l'uso di placeholder (action1, action2, etc.)
    \item Generare sequenze strategicamente coerenti con lo stato del gioco
    \item Gestire prompt complessi con multiple istruzioni
\end{itemize}

Il threshold di ~4B parametri sembra essere critico per l'instruction-following in task strutturati come Crafter.

\subsubsection{Progettazione del Prompt}

Il prompt per l'Helper è stato progettato iterativamente attraverso esperimenti. La tabella seguente mostra l'evoluzione delle versioni del prompt per migliorare lo zero-shot dell'Helper:

\begin{table}[H]
\centering
\small
\begin{tabular}{@{}p{2cm}p{5cm}p{3cm}p{2cm}@{}}
\toprule
\textbf{Versione} & \textbf{Modifiche Principali} & \textbf{Problema Risolto} & \textbf{Risultato} \\ \midrule
V1.0 & Prompt base con lista azioni e stato corrente & Baseline: zero-shot senza apprendimento & 35\% valid \\
\midrule
V2.0 & + Esempi good/bad, formato output esplicito & Riduce azioni invalide & 62\% valid \\
\midrule
V3.0 & + Warning typo comuni (place\_rock→place\_stone) & Corregge errori ricorrenti & 78\% valid \\
\midrule
V4.0 & + Survival priorities, achievement chain & Ignora situazioni di salute critica & 84\% valid \\
\midrule
V5.0 & + Variety reminder, anti-loop detection & Previene sequenze ripetitive & 91\% valid \\
\midrule
V6.0 & + Context-aware goals, episodic memory & Manca contesto situazionale & 95\% valid \\
\midrule
\textbf{V7.0} & \textbf{+ Placeholder prevention, token overflow management} & \textbf{Uso di placeholder e output verboso} & \textbf{98\% valid} \\ \bottomrule
\end{tabular}
\caption{Evoluzione iterativa del prompt Helper per miglioramento zero-shot (con Qwen3-4B)}
\end{table}

\textbf{Nota}: Le percentuali di validità sono misurate con Qwen3-4B-2507. Test preliminari con modelli più piccoli (<4B parametri) hanno mostrato incapacità di rispettare il prompt anche con le versioni più evolute (V5-V7), confermando la necessità di un modello con almeno 4B parametri per instruction-following affidabile.

Il prompt finale (V7.0) utilizzato nel sistema include:

\begin{verbatim}
You are an expert Crafter player. Given the current game state,
suggest a sequence of 3-5 actions to achieve progress.

Current State:
- Health: {health}/9
- Food: {food}/9  
- Water: {water}/9
- Position: ({x}, {y})
- Inventory: {inventory_items}
- Achievements unlocked: {achievements}

Available actions:
[move_left, move_right, move_up, move_down, do,
 sleep, place_stone, place_table, place_furnace,
 place_plant, make_wood_pickaxe, make_stone_pickaxe,
 make_iron_pickaxe, make_wood_sword, make_stone_sword,
 make_iron_sword, noop]

Output format: [action1], [action2], [action3],
[action4], [action5]

Suggest actions:
\end{verbatim}

\subsubsection{Meccanismi di Re-planning}

Sono state implementate logiche per interrompere e ri-pianificare:

\begin{algorithm}
\caption{Re-planning durante esecuzione}
\begin{algorithmic}
\WHILE{esecuzione sequenza}
    \STATE $next\_state, reward, done, info \gets env.step(action)$
    \IF{achievement sbloccato}
        \STATE Genera nuova sequenza con contesto aggiornato
        \STATE BREAK
    \ENDIF
    \IF{$health \leq 5$}
        \STATE Fallback a DQN per sopravvivenza immediata
        \STATE BREAK
    \ENDIF
    \IF{$health < 0.3 \times max\_health$}
        \STATE Re-query con priorità gestione salute
        \STATE BREAK
    \ENDIF
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\subsection{Fase 4: Generazione Dataset per Reviewer}

\subsubsection{Processo di Raccolta Dati}

Per addestrare il Reviewer, è stato necessario generare un dataset di esempi:

\begin{enumerate}
    \item \textbf{Esecuzione episodi}: 50 episodi di gioco con Helper zero-shot (circa 2,500 esempi di training)
    \item \textbf{Registrazione}: Per ogni chiamata Helper, salvare:
    \begin{itemize}
        \item Stato dell'environment
        \item Sequenza di azioni suggerite
        \item Risultato dell'esecuzione (reward, achievement)
    \end{itemize}
    \item \textbf{Annotazione}: Generazione di feedback basati su:
    \begin{itemize}
        \item Successo/fallimento della sequenza
        \item Efficienza (step sprecati)
        \item Priorità rispetto allo stato (es. salute bassa ignorata)
    \end{itemize}
\end{enumerate}

\subsection{Fase 5: Fine-tuning del Reviewer}

\subsubsection{Scelta del Modello Base}

È stato scelto FLAN-T5-base come modello base per il Reviewer:
\begin{itemize}
    \item Dimensioni gestibili (250M parametri)
    \item Buone capacità di text-to-text generation
    \item Fine-tuned su task di instruction-following
    \item Veloce per inference durante il training
    \item Modello: \texttt{google/flan-t5-base}
\end{itemize}

\subsubsection{Configurazione Training}

Il fine-tuning è stato eseguito con i seguenti parametri:

\begin{verbatim}
Optimizer: AdamW
Learning rate: 5e-5
Batch size: 8
Epochs: 5
Max input length: 512 tokens
Max output length: 128 tokens
Gradient accumulation steps: 2
\end{verbatim}

\subsubsection{Validazione}

Il dataset è stato diviso in:
\begin{itemize}
    \item Training set: 80\% (circa 2,000 esempi)
    \item Validation set: 20\% (circa 500 esempi)
\end{itemize}

Metriche monitorate durante il training:
\begin{itemize}
    \item Training loss
    \item Validation loss
    \item BLEU score (similarità con feedback attesi)
\end{itemize}

\subsection{Fase 6: Training Integrato HeRoN}

\subsubsection{Protocollo di Training}

Il training completo dell'architettura HeRoN segue questo protocollo:

\begin{algorithm}
\caption{Training Loop HeRoN}
\begin{algorithmic}
\STATE $threshold \gets 1.0$
\STATE $threshold\_decay \gets 0.01$
\STATE $threshold\_episodes \gets 100$
\FOR{$episode = 1$ to $max\_episodes$}
    \STATE $state \gets env.reset()$
    \STATE $action\_sequence \gets []$
    \STATE $sequence\_index \gets 0$
    \FOR{$step = 1$ to $max\_steps$}
        \IF{$len(action\_sequence) == 0$ OR $sequence\_index \geq len(action\_sequence)$}
            \IF{$random() > threshold$ AND $episode < 600$}
                \STATE $action\_sequence \gets$ Helper-Reviewer workflow
                \STATE $sequence\_index \gets 0$
            \ELSE
                \STATE $action \gets$ DQN selection
            \ENDIF
        \ELSE
            \STATE $action \gets action\_sequence[sequence\_index]$
            \STATE $sequence\_index \gets sequence\_index + 1$
        \ENDIF
        \STATE Esegui azione e aggiorna DQN
    \ENDFOR
    \IF{$episode < threshold\_episodes$}
        \STATE $threshold \gets \max(0, threshold - threshold\_decay)$
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{Parametri di Training}

\begin{itemize}
    \item \textbf{Episodi totali}: 1000
    \item \textbf{Max steps per episodio}: 1000
    \item \textbf{Threshold decay}: 1.0 → 0.0 in 100 episodi
    \item \textbf{LLM cutoff}: Episodio 600 (dopo, solo DQN)
    \item \textbf{Checkpoint}: Salvataggio ogni 50 episodi + best model
\end{itemize}

\subsubsection{Analisi del Numero Ottimale di Azioni}

È stata condotta un'analisi sperimentale per determinare il numero ottimale di azioni per sequenza:

\begin{table}[h]
\centering
\begin{tabular}{@{}ccc@{}}
\toprule
Azioni per sequenza & Achievement medi & Chiamate Helper/episodio \\ \midrule
1 & 3.2 & 150-200 \\
3 & 4.5 & 50-80 \\
5 & 4.8 & 30-50 \\
7 & 4.3 & 20-35 \\
10 & 3.9 & 15-25 \\ \bottomrule
\end{tabular}
\caption{Impatto del numero di azioni per sequenza}
\end{table}

Il valore ottimale è risultato essere 5 azioni, che bilancia:
\begin{itemize}
    \item Pianificazione strategica (non troppo breve)
    \item Flessibilità di re-planning (non troppo lungo)
    \item Overhead computazionale LLM
\end{itemize}

\subsection{Fase 7: Valutazione delle Prestazioni}

\subsubsection{Metriche di Valutazione}

Per valutare le prestazioni di HeRoN, sono state definite diverse metriche:

\begin{enumerate}
    \item \textbf{Achievement Score}: Numero medio di achievement sbloccati per episodio
    $$\text{Score} = \frac{1}{N} \sum_{i=1}^N \text{achievements}_i$$
    
    \item \textbf{Coverage}: Percentuale di achievement unici sbloccati almeno una volta
    $$\text{Coverage} = \frac{|\text{achievement unici}|}{22} \times 100\%$$
    
    \item \textbf{Success Rate per Achievement}: Percentuale di episodi in cui ciascun achievement è stato sbloccato
    
    \item \textbf{Reward Cumulativo}: Somma dei reward durante l'episodio (shaped e nativo)
    
    \item \textbf{Convergenza}: Episodio in cui lo score medio si stabilizza
\end{enumerate}

\subsubsection{Baseline di Confronto}

HeRoN è stato confrontato con:
\begin{itemize}
    \item \textbf{DQN puro}: Stesso agente senza componenti LLM
    \item \textbf{Random policy}: Azioni casuali (sanity check)
    \item \textbf{Helper solo}: DQN + Helper senza Reviewer
\end{itemize}

\subsubsection{Protocollo di Test}

Per garantire validità statistica:
\begin{itemize}
    \item Ogni configurazione testata per 100 episodi
    \item 5 seed casuali diversi
    \item Media e deviazione standard riportate
    \item Test statistici (t-test) per significatività
\end{itemize}

\subsection{Fase 8: Analisi e Ottimizzazione}

\subsection{Tuning degli Iperparametri}

Grid search limitata su:
\begin{itemize}
    \item Learning rate DQN: [1e-4, 5e-4, 1e-3]
    \item Threshold decay rate: [0.005, 0.01, 0.02]
    \item Peso reward shaping: [0.5, 1.0, 2.0]
\end{itemize}

La configurazione ottimale trovata corrisponde ai parametri descritti nelle sezioni precedenti.
