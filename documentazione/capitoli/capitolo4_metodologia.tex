La formula utilizzata per il calcolo della percentuale di azioni valide è la seguente:
dove $N_{valid}$ è il numero di azioni conformi al set ufficiale e al formato richiesto, mentre $N_{total}$ è il numero totale di azioni generate dal modello per il campione analizzato.
\begin{equation}
\text{Valid Actions \%} = \frac{N_{valid}}{N_{total}} \times 100
\end{equation}
dove $N_{valid}$ è il numero di azioni conformi al set ufficiale e al formato richiesto, mentre $N_{total}$ è il numero totale di azioni generate dal modello per il campione analizzato.
\chapter{Metodologia di Implementazione}

\section{Introduzione}

Questo capitolo descrive la metodologia utilizzata per sviluppare e valutare l'architettura HeRoN nel dominio Crafter. Viene fornita una panoramica delle fasi principali (implementazione dell'NPC, integrazione LLM, generazione del dataset per il Reviewer, fine-tuning e training integrato), seguita da dettagli tecnici e protocolli sperimentali.

\section{Panoramica del Processo}

Di seguito vengono sintetizzate le fasi principali del flusso di lavoro. Le sezioni successive sviluppano ciascun punto in dettaglio.
\begin{enumerate}
    \item Implementazione e addestramento del NPC (DQN)
    \item Progettazione e integrazione dell'Helper (LLM)
    \item Generazione del dataset per il Reviewer
    \item Fine-tuning supervised del Reviewer (T5)
    \item Estensione: fine-tuning tramite Reinforcement Learning (PPO)
    \item Training integrato dell'architettura HeRoN e valutazione
\end{enumerate}

\section{NPC (DQN Agent)}

\subsection{Architettura della Rete Neurale}

L'agente DQN è stato implementato con una rete neurale feedforward:

\begin{verbatim}
Input Layer: 43 neuroni (dimensione stato)
    ↓
Hidden Layer 1: 256 neuroni + ReLU + Dropout(0.1)
    ↓
Hidden Layer 2: 256 neuroni + ReLU + Dropout(0.1)
    ↓
Hidden Layer 3: 128 neuroni + ReLU
    ↓
Output Layer: 17 neuroni (Q-values per azioni)
\end{verbatim}

\subsubsection{Implementazione Double DQN}

Sono state implementate due reti neurali:
\begin{itemize}
    \item \textbf{Policy Network}: Usata per selezionare azioni e aggiornata ad ogni step
    \item \textbf{Target Network}: Usata per calcolare i target Q-values, aggiornata periodicamente
\end{itemize}

L'aggiornamento della target network avviene ogni $C = 1000$ step tramite soft update:
$$\theta_{target} \leftarrow \tau \theta_{policy} + (1-\tau) \theta_{target}$$
con $\tau = 0.001$.

\subsubsection{Prioritized Experience Replay}

Il replay buffer implementa prioritized sampling:

\begin{enumerate}
    \item \textbf{Calcolo priorità}: Per ogni transizione, la priorità è basata sul TD-error:
    $$p_i = |\delta_i|^\alpha + \epsilon$$
    dove $\delta_i = r + \gamma \max_{a'} Q_{target}(s', a') - Q(s, a)$
    
    \item \textbf{Sampling}: La probabilità di campionare la transizione $i$ è:
    $$P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha}$$
    
    \item \textbf{Importance Sampling}: Per correggere il bias, i pesi sono:
    $$w_i = \left(\frac{1}{N \cdot P(i)}\right)^\beta$$
\end{enumerate}

\subsubsection{Training Baseline DQN}

Prima di integrare i componenti LLM, abbiamo addestrato un agente DQN baseline:

\begin{itemize}
    \item \textbf{Episodi}: 1000
    \item \textbf{Max steps per episodio}: 1000
    \item \textbf{Epsilon decay}: $\epsilon = 1.0 \rightarrow 0.05$ in 800 episodi
    \item \textbf{Learning rate}: $\alpha = 0.0001$
    \item \textbf{Batch size}: 64
\end{itemize}

Il baseline ci serve come riferimento per valutare quanto funziona l'integrazione con gli LLM.

\section{Helper (LLM) e Prompt Design}


\subsubsection{Setup LM Studio}
LM Studio è stato configurato per servire il modello Qwen3-4B-2507:
\begin{itemize}
    \item Server locale su \texttt{http://127.0.0.1:1234}
    \item API compatibile con OpenAI
    \item Temperatura: 0.7 (bilanciamento tra creatività e coerenza)
    \item Max tokens: 150 (sufficiente per sequenze di 3-5 azioni)
    \item Context window: 8192 tokens (gestione conversazioni lunghe)
    \item Tokenizer: Qwen2.5-7B-Instruct (per conteggio token context-aware)
\end{itemize}

\textbf{Selezione del Modello}:
Sono stati testati diversi modelli, ma nella tabella sono riportati solo quelli rilevanti per la selezione finale. Qwen3-4B-2507 è stato scelto per la sua elevata conformità al formato richiesto e coerenza strategica.

% --- TABELLA CONFRONTO MODELLI LLM ---
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|p{0.35\textwidth}|}
\hline
\rowcolor{gray!20}
\textbf{Modello} & \textbf{Parametri} & \textbf{Conformità (\%)} & \textbf{Coerenza} & \textbf{Note} \\
\hline
Llama-3.2-1B & 1.1B & 23\% & Bassa & Genera spiegazioni verbose invece di sequenze \\
\hline
Phi-3-mini & 3.8B & 72\% & Media & Difficoltà istruzioni, allucinazioni frequenti \\
\hline
\textbf{Qwen3-4B-2507} & 4B & \textbf{98\%} & \textbf{Molto alta} & \textbf{Selezionato: rispetta formato, sequenze coerenti. Finetunato per tool use e reasoning, oltre a seguire istruzioni specifiche.} \\
\hline
\end{tabular}%
}
\caption{Confronto tra modelli LLM testati per l'Helper. Qwen3-4B-2507 è il migliore per conformità e coerenza.}
\end{table}

\subsubsection{Progettazione del Prompt}

Il prompt per l'Helper è stato progettato iterativamente attraverso esperimenti. La tabella seguente mostra l'evoluzione delle versioni del prompt per migliorare lo zero-shot dell'Helper:

% --- TABELLA 4.1 CON VERSIONI ESPLICITE ---
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|p{0.22\textwidth}|p{0.22\textwidth}|p{0.22\textwidth}|p{0.22\textwidth}|}
\hline
\rowcolor{gray!20}
\textbf{Versione Prompt} & \textbf{Descrizione} & \textbf{Obiettivo} & \textbf{\% valid actions} \\
\hline
v1 (Base) & Azioni semplici, nessun contesto & Sequenza generica & 87\% \\
\hline
v2 (Intermedio) & Lista azioni valide, goal survival & Sequenza contestualizzata & 94\% \\
\hline
v3 (Rifinito) & Goal multipli, errori da evitare, stato attuale & Sequenza ottimizzata & 98\% \\
\hline
\end{tabular}%
}
\caption{Versioni dei prompt Helper LLM e percentuale di azioni valide generate}
\end{table}

\textbf{Nota metodologica sulle percentuali di azioni valide:} 

Le percentuali di valid actions riportate in tabella sono state calcolate eseguendo ciascuna versione del prompt su un campione di 100 sequenze generate dall'Helper LLM. 

Ogni sequenza è stata analizzata automaticamente tramite uno script Python che verifica la conformità delle azioni al set ufficiale di azioni ammesse (17 azioni Crafter). Sono considerate valide solo le azioni che rispettano il formato bracketed e corrispondono esattamente alle azioni implementate nell'ambiente. 

Gli errori di formato, azioni non riconosciute o placeholder (es. [action1]) sono stati conteggiati come non validi. Il valore percentuale rappresenta la proporzione di azioni corrette sul totale delle azioni generate per ciascun prompt.

% --- NUOVA TABELLA: ESEMPI DI PROMPT ---
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|p{0.95\textwidth}|}
\hline
\rowcolor{gray!20}
\textbf{Prompt base} \\
\hline
Generate a sequence of actions for Crafter. \\
Use actions like move, do, place. \\
Format: [action1], [action2] \\
\hline
\rowcolor{gray!20}
\textbf{Prompt intermedio} \\
\hline
You are a Crafter AI. GOALS: Survive and unlock achievements. \\
VALID ACTIONS: move\_up, move\_down, move\_left, move\_right, do, sleep, place\_stone, place\_table, place\_furnace, place\_plant, make\_wood\_pickaxe, make\_stone\_pickaxe, make\_iron\_pickaxe, make\_wood\_sword, make\_stone\_sword, make\_iron\_sword, noop \\
TASK: Generate a sequence of 4 actions. \\
FORMAT: [action1], [action2], [action3], [action4] \\
EXAMPLES: [move\_right], [do], [place\_table] \\
\hline
\rowcolor{gray!20}
\textbf{Prompt raffinato} \\
\hline
You are a Crafter AI. GOALS: 1) Survive 2) Unlock achievements 3) Be efficient. \\
VALID ACTIONS: [full list of 17 actions] \\
MISTAKES TO AVOID: Avoid collect\_wood/gather/mine - use [do] \\
CURRENT STATE: [state description] \\
SURVIVAL: Health =? Use [sleep] \\
ACHIEVEMENT CHAIN: Wood→Table→Pickaxe→Stone→Coal→Iron→Diamond \\
TASK: Generate EXACTLY ONE sequence of 4 actions. \\
FORMAT: [REAL\_ACTION\_1], [REAL\_ACTION\_2], [REAL\_ACTION\_3], [REAL\_ACTION\_4] \\
EXAMPLES: Good: [move\_right], [do], [move\_left], [noop] \\
Bad: [action1], [do.something] \\
YOUR TURN: [final instructions] \\
\hline
\end{tabular}%
}
\caption{Esempi espliciti dei prompt Helper LLM utilizzati}
\end{table}

\subsubsection{Meccanismi di Re-planning}

Sono state implementate logiche per interrompere e ri-pianificare:

\begin{algorithm}
\caption{Re-planning durante esecuzione}
\begin{algorithmic}
\WHILE{esecuzione sequenza}
    \STATE $next\_state, reward, done, info \gets env.step(action)$
    \IF{achievement sbloccato}
        \STATE Genera nuova sequenza con contesto aggiornato
        \STATE BREAK
    \ENDIF
    \IF{$health \leq 5$}
        \STATE Fallback a DQN per sopravvivenza immediata
        \STATE BREAK
    \ENDIF
    \IF{$health < 0.3 \times max\_health$}
        \STATE Re-query con priorità gestione salute
        \STATE BREAK
    \ENDIF
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\section{Generazione del Dataset per il Reviewer}

\subsubsection{Processo di Raccolta Dati}

Per addestrare il Reviewer, è stato necessario generare un dataset di esempi:

\begin{enumerate}
    \item \textbf{Esecuzione episodi}: 50 episodi di gioco con Helper zero-shot (circa 2,500 esempi di training)
    \item \textbf{Registrazione}: Per ogni chiamata Helper, salvare:
    \begin{itemize}
        \item Stato dell'environment
        \item Sequenza di azioni suggerite
        \item Risultato dell'esecuzione (reward, achievement)
    \end{itemize}
    \item \textbf{Annotazione}: Generazione di feedback basati su:
    \begin{itemize}
        \item Successo/fallimento della sequenza
        \item Efficienza (step sprecati)
        \item Priorità rispetto allo stato (es. salute bassa ignorata)
    \end{itemize}
\end{enumerate}

\subsection{Fase 5: Fine-tuning del Reviewer}

\subsubsection{Scelta del Modello Base}

È stato scelto FLAN-T5-base come modello base per il Reviewer:
\begin{itemize}
    \item Dimensioni gestibili (250M parametri)
    \item Buone capacità di text-to-text generation
    \item Fine-tuned su task di instruction-following
    \item Veloce per inference durante il training
    \item Modello: \texttt{google/flan-t5-base}
\end{itemize}

\subsubsection{Configurazione Training}

Il fine-tuning è stato eseguito con i seguenti parametri:

\begin{verbatim}
Optimizer: AdamW
Learning rate: 5e-5
Batch size: 8
Epochs: 5
Max input length: 512 tokens
Max output length: 128 tokens
Gradient accumulation steps: 2
\end{verbatim}

\subsubsection{Validazione}

Il dataset è stato diviso in:
\begin{itemize}
    \item Training set: 80\% (circa 2,000 esempi)
    \item Validation set: 20\% (circa 500 esempi)
\end{itemize}

Metriche monitorate durante il training:
\begin{itemize}
    \item Training loss
    \item Validation loss
    \item BLEU score (similarità con feedback attesi)
\end{itemize}

\subsection{Fase 6: Training Integrato HeRoN}

\subsubsection{Protocollo di Training}

Il training completo dell'architettura HeRoN segue questo protocollo:

\begin{algorithm}
\caption{Training Loop HeRoN}
\begin{algorithmic}
\STATE $threshold \gets 1.0$
\STATE $threshold\_decay \gets 0.01$
\STATE $threshold\_episodes \gets 100$
\FOR{$episode = 1$ to $max\_episodes$}
    \STATE $state \gets env.reset()$
    \STATE $action\_sequence \gets []$
    \STATE $sequence\_index \gets 0$
    \FOR{$step = 1$ to $max\_steps$}
        \IF{$len(action\_sequence) == 0$ OR $sequence\_index \geq len(action\_sequence)$}
            \IF{$random() > threshold$ AND $episode < 600$}
                \STATE $action\_sequence \gets$ Helper-Reviewer workflow
                \STATE $sequence\_index \gets 0$
            \ELSE
                \STATE $action \gets$ DQN selection
            \ENDIF
        \ELSE
            \STATE $action \gets action\_sequence[sequence\_index]$
            \STATE $sequence\_index \gets sequence\_index + 1$
        \ENDIF
        \STATE Esegui azione e aggiorna DQN
    \ENDFOR
    \IF{$episode < threshold\_episodes$}
        \STATE $threshold \gets \max(0, threshold - threshold\_decay)$
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{Parametri di Training}

\begin{itemize}
    \item \textbf{Episodi totali}: 1000
    \item \textbf{Max steps per episodio}: 1000
    \item \textbf{Threshold decay}: 1.0 → 0.0 in 100 episodi
    \item \textbf{LLM cutoff}: Episodio 600 (dopo, solo DQN)
    \item \textbf{Checkpoint}: Salvataggio ogni 50 episodi + best model
\end{itemize}

\subsubsection{Analisi del Numero Ottimale di Azioni}

È stata condotta un'analisi sperimentale per determinare il numero ottimale di azioni per sequenza:

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Azioni per sequenza} & \textbf{Achievement medi} & \textbf{Chiamate Helper/episodio} \\
\hline
1 & 3.2 & 150-200 \\
\hline
3 & 4.5 & 50-80 \\
\hline
5 & 4.8 & 30-50 \\
\hline
7 & 4.3 & 20-35 \\
\hline
10 & 3.9 & 15-25 \\
\hline
\end{tabular}%
\caption{Impatto del numero di azioni per sequenza}
\end{table}

Il valore ottimale è risultato essere 5 azioni, che bilancia:
\begin{itemize}
    \item Pianificazione strategica (non troppo breve)
    \item Flessibilità di re-planning (non troppo lungo)
    \item Overhead computazionale LLM
\end{itemize}

\subsection{Fase 7: Valutazione delle Prestazioni}

\subsubsection{Metriche di Valutazione}

Per valutare le prestazioni di HeRoN, sono state definite diverse metriche:

\begin{enumerate}
    \item \textbf{Achievement Score}: Numero medio di achievement sbloccati per episodio
    $$\text{Score} = \frac{1}{N} \sum_{i=1}^N \text{achievements}_i$$
    
    \item \textbf{Coverage}: Percentuale di achievement unici sbloccati almeno una volta
    $$\text{Coverage} = \frac{|\text{achievement unici}|}{22} \times 100\%$$
    
    \item \textbf{Success Rate per Achievement}: Percentuale di episodi in cui ciascun achievement è stato sbloccato
    
    \item \textbf{Reward Cumulativo}: Somma dei reward durante l'episodio (shaped e nativo)
    
    \item \textbf{Convergenza}: Episodio in cui lo score medio si stabilizza
\end{enumerate}

\subsubsection{Baseline di Confronto}

HeRoN è stato confrontato con:
\begin{itemize}
    \item \textbf{DQN puro}: Stesso agente senza componenti LLM
    \item \textbf{Helper solo}: DQN + Helper senza Reviewer
\end{itemize}

\subsubsection{Protocollo di Test}

Per garantire validità statistica:
\begin{itemize}
    \item Ogni configurazione testata per 100 episodi
    \item 5 seed casuali diversi
    \item Media e deviazione standard riportate
    \item Test statistici (t-test) per significatività
\end{itemize}

\subsection{Fase 8: Analisi e Ottimizzazione}

\subsection{Tuning degli Iperparametri}

Grid search limitata su:
\begin{itemize}
    \item Learning rate DQN: [1e-4, 5e-4, 1e-3]
    \item Threshold decay rate: [0.005, 0.01, 0.02]
    \item Peso reward shaping: [0.5, 1.0, 2.0]
\end{itemize}

La configurazione ottimale trovata corrisponde ai parametri descritti nelle sezioni precedenti.

\subsection{Tuning e Configurazioni del Reviewer (T5)}

Per il modulo Reviewer, basato su T5, sono state testate diverse configurazioni di tuning. La tabella seguente riassume i principali parametri utilizzati durante la fase di fine-tuning e generazione del dataset:

\begin{table}[H]
\centering
\caption{Configurazioni testate per il tuning del Reviewer (T5)}
\resizebox{0.8\textwidth}{!}{%
\begin{tabular}{|l|l|}
\hline
\rowcolor{gray!20} 
\textbf{Parametro} & \textbf{Valore/Testato} \\
\hline
Modello & google/flan-t5-base \\
\hline
Dimensione dataset & 2000--5000 samples \\
\hline
Episodi generazione dataset & 50--100 \\
\hline
Lunghezza episodio & 500 steps \\
\hline
Batch size (train/eval) & 8 \\
\hline
Epoche & 5 \\
\hline
Learning rate & 5e-5 \\
\hline
Weight decay & 0.01 \\
\hline
Logging steps & 10 \\
\hline
Salvataggio modello & Ogni epoca (save\_strategy='epoch') \\
\hline
Metriche best model & eval\_loss \\
\hline
Limite salvataggi & 3 \\
\hline
Helper calls per episodio & ogni 5 step \\
\hline
\end{tabular}%
}
\end{table}

Queste configurazioni sono state selezionate tramite test iterativi e grid search limitata, con l'obiettivo di massimizzare la qualità delle correzioni generate dal Reviewer e la generalizzazione sulle strategie di gioco. Il dataset è stato generato simulando tra 50 e 100 episodi, con chiamate al modulo Helper ogni 5 step, per ottenere una varietà di situazioni e feedback strategici.

\subsection{Fine-Tuning del Reviewer tramite Reinforcement Learning}

Il fine-tuning avanzato del modulo Reviewer è stato realizzato tramite Reinforcement Learning (PPO), seguendo un workflow strutturato:

\begin{itemize}
    \item \textbf{Generazione del dataset:} Per ogni episodio vengono raccolti:
    \begin{itemize}
        \item Stato dell'environment (descrizione dettagliata)
        \item Sequenza di azioni suggerite dall'Helper
        \item Feedback correttivo (strategic feedback) generato da regole o Reviewer simulato
        \item Reward associato alla qualità del feedback
    \end{itemize}
    \item \textbf{Struttura del sample:} Ogni esempio contiene: stato, azioni, feedback, reward, outcome, refined sequence.
    \item \textbf{Addestramento RL:} Il Reviewer viene addestrato tramite PPO (Proximal Policy Optimization), ottimizzando la policy per generare feedback strategici e correttivi, massimizzando il reward rispetto a un target ideale.
    \item \textbf{Obiettivo:} Migliorare la capacità del Reviewer di fornire feedback utili e strategici, ottimizzando la collaborazione con Helper e NPC.
\end{itemize}

\begin{table}[H]
\centering
\caption{Workflow Fine-Tuning Reviewer RL}
\resizebox{0.7\textwidth}{!}{%
\begin{tabular}{|l|l|}
\hline
\rowcolor{gray!20} 
\textbf{Fase} & \textbf{Descrizione} \\
\hline
Generazione dataset & Stati, azioni Helper, feedback, reward \\
\hline
Feedback & Regole/Reviewer simulato, strategico \\
\hline
Algoritmo RL & PPO (Proximal Policy Optimization) \\
\hline
Reward & Qualità del feedback rispetto al target \\
\hline
Obiettivo & Policy ottimizzata per feedback strategici \\
\hline
\end{tabular}%
}
\end{table}

Questo approccio consente al Reviewer di apprendere non solo dai dati supervisionati, ma anche dall'interazione iterativa e dal reward, migliorando la qualità dei suggerimenti e la sinergia tra i moduli dell'architettura HeRoN.