\chapter{Metodologia di Implementazione}

\section{Introduzione}

Questo capitolo descrive la metodologia utilizzata per sviluppare e valutare l'architettura HeRoN nel dominio Crafter. Viene fornita una panoramica delle fasi principali (implementazione dell'NPC, integrazione LLM, generazione del dataset per il Reviewer, fine-tuning e training integrato), seguita da dettagli tecnici e protocolli sperimentali.

Le sezioni sono organizzate al fine di orientare la lettura dalla teoria all'implementazione pratica, passando per la progettazione dei moduli, la raccolta dati e la valutazione sperimentale. Ogni tabella e algoritmo è presentato subito dopo la relativa spiegazione, per garantire coerenza e leggibilità.

\section{Panoramica del Processo}

Le fasi principali dello sviluppo sono:
\begin{enumerate}
    \item Implementazione e addestramento del NPC (DQN)
    \item Progettazione e integrazione dell'Helper (LLM)
    \item Generazione del dataset per il Reviewer
    \item Fine-tuning supervised del Reviewer (T5)
    \item Training integrato dell'architettura HeRoN e valutazione
\end{enumerate}

Per una descrizione dettagliata dei componenti architetturali (NPC, Helper, Reviewer) e del meccanismo di threshold decay, si rimanda al Capitolo~2. Le sezioni seguenti sviluppano i dettagli implementativi di ciascuna fase.

\section{NPC (DQN Agent)}


\subsection{Architettura della Rete Neurale}
L'agente DQN è stato progettato con una rete neurale feedforward composta da:
\begin{itemize}
    \item \textbf{Input Layer}: 43 neuroni (corrispondenti alla dimensione dello stato)
    \item \textbf{Hidden Layer 1}: 128 neuroni, attivazione ReLU
    \item \textbf{Hidden Layer 2}: 128 neuroni, attivazione ReLU
    \item \textbf{Hidden Layer 3}: 64 neuroni, attivazione ReLU
    \item \textbf{Output Layer}: 17 neuroni (Q-values, uno per ciascuna azione)
\end{itemize}

\subsubsection{Algoritmi RL: Double DQN, PER, TD-error, Backpropagation}

L'addestramento dell'agente DQN si basa su alcuni concetti fondamentali dell'apprendimento automatico, che spieghiamo qui in modo semplice:

\begin{itemize}
    \item \textbf{Funzione di perdita Q (Errore di previsione)}:
    
    La funzione di perdita misura la differenza tra la previsione del valore di un'azione in uno stato e il valore ideale atteso. La formula è:
    $$L = \mathbb{E}[(Q(s,a) - y)^2]$$
    Dove:
    \begin{itemize}
        \item $Q(s,a)$ rappresenta la stima del valore dell'azione $a$ nello stato $s$.
        \item $y$ è il valore ideale, calcolato come:
        $$y = r + \gamma \max_{a'} Q(s',a')$$
        \item $r$ è il premio (reward) ottenuto dopo l'esecuzione dell'azione.
        \item $\gamma$ è un numero tra 0 e 1 che indica il peso attribuito ai premi futuri (più è vicino a 1, maggiore è l'importanza del lungo termine).
        \item $s'$ è lo stato successivo dopo l'azione.
        \item $\max_{a'} Q(s',a')$ indica la migliore stima tra tutte le azioni possibili nel nuovo stato.
    \end{itemize}

    \item \textbf{Double DQN}:
    
    Per evitare che l'agente sia troppo ottimista nelle sue previsioni, si usano due reti neurali diverse:
    \begin{itemize}
        \item La "policy network" sceglie le azioni.
        \item La "target network" serve solo per calcolare il valore ideale $y$.
    \end{itemize}
    La formula diventa:
    $$y = r + \gamma Q'(s', \arg\max_{a'} Q(s',a'))$$
    Dove $Q'$ è la rete target e $\arg\max$ indica l'azione migliore secondo la policy network.

    \item \textbf{Prioritized Experience Replay (PER)}:
    
    L'agente impara rivedendo le sue esperienze passate. Non tutte le esperienze sono uguali: quelle dove ha sbagliato di più sono più utili per imparare. Si assegna una "priorità" a ogni esperienza usando questa formula:
    $$p_i = |\delta_i| + \epsilon$$
    Dove:
    \begin{itemize}
        \item $\delta_i$ è l'errore di previsione (TD-error):
        $$\delta_i = r + \gamma \max_{a'} Q_{target}(s', a') - Q(s, a)$$
        Più è grande, più l'agente ha sbagliato e più deve imparare da quell'esperienza.
        \item $\epsilon$ è un piccolo numero che serve solo a evitare problemi matematici.
    \end{itemize}

    \item \textbf{Backpropagation (Aggiornamento dei pesi)}:
    
    L'agente aggiorna i "pesi" della rete neurale (cioè i suoi parametri interni) per ridurre l'errore. La regola di aggiornamento è:
    $$\theta \leftarrow \theta - \alpha \nabla_{\theta} L$$
    Dove:
    \begin{itemize}
        \item $\theta$ sono i pesi della rete.
        \item $\alpha$ è la velocità di apprendimento (learning rate).
        \item $\nabla_{\theta} L$ è il gradiente, cioè la direzione in cui bisogna cambiare i pesi per ridurre l'errore.
    \end{itemize}
\end{itemize}

In sintesi: l'agente prova azioni, riceve premi, aggiorna le sue previsioni e impara soprattutto dagli errori più grandi, migliorando così la sua capacità di prendere decisioni nel tempo.

L'aggiornamento della target network avviene ogni $C = 100$ step tramite hard copy:
$$\theta_{target} \leftarrow \theta_{policy}$$

\subsubsection{Prioritized Experience Replay}
Il replay buffer utilizza un campionamento prioritario per migliorare l'efficienza dell'apprendimento:
\begin{enumerate}
    \item \textbf{Calcolo priorità}: Per ogni transizione, la priorità si basa sul TD-error:
    $$p_i = |\delta_i|^\alpha + \epsilon$$
    dove $\delta_i = r + \gamma \max_{a'} Q_{target}(s', a') - Q(s, a)$
    \item \textbf{Sampling}: La probabilità di selezionare la transizione $i$ è:
    $$P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha}$$
    \item \textbf{Importance Sampling}: Per correggere il bias introdotto dal campionamento, i pesi sono:
    $$w_i = \left(\frac{1}{N \cdot P(i)}\right)^\beta$$
\end{enumerate}

\subsubsection{Training Baseline DQN}
Prima dell'integrazione dei moduli LLM, è stato addestrato un agente DQN baseline con i seguenti parametri:
\begin{itemize}
    \item \textbf{Episodi}: 300
    \item \textbf{Max steps per episodio}: 1000
    \item \textbf{Epsilon decay}: $\epsilon = 1.0 \rightarrow 0.05$ in 300 episodi
    \item \textbf{Learning rate}: $\alpha = 0.0001$
    \item \textbf{Batch size}: 64
\end{itemize}
Questo baseline costituisce il riferimento per valutare l'efficacia dell'integrazione con i moduli LLM.

\section{Helper (LLM) e Prompt Design}

La progettazione del modulo Helper e dei prompt è stata fondamentale per ottenere sequenze di azioni coerenti e strategiche. Le tabelle seguenti mostrano la selezione dei modelli e l'evoluzione dei prompt.


\subsubsection{Setup LM Studio}
LM Studio è stato configurato per servire il modello Qwen3-4B-2507:
\begin{itemize}
    \item Server host: http://127.0.0.1:1234
    \item Modello: qwen/qwen3-4b-2507
    \item Temperatura: 0.7 (bilanciamento tra creatività e coerenza)
    \item Max tokens: 150 (sufficiente per sequenze di 3-5 azioni)
    \item Context window: 8192 tokens (gestione conversazioni lunghe)
    \item Tokenizer: Qwen2.5-7B-Instruct (per conteggio token context-aware)
    \item Safe threshold: 6500 tokens (per evitare context overflow)
    \item Max message history: 12 messaggi (gestione memoria conversazionale)
    \item Timeout LLM: 60 secondi
\end{itemize}

\textbf{Parametri Sequenze Azioni} (classe \texttt{CrafterHelper}):
\begin{itemize}
    \item \texttt{min\_sequence\_length}: 3 azioni (garantisce pianificazione minima)
    \item \texttt{default\_sequence\_length}: 4 azioni (target prompt, bilanciato)
    \item \texttt{max\_sequence\_length}: 5 azioni (limite superiore per flessibilità)
\end{itemize}

\textbf{Selezione del Modello}:

Sono stati testati diversi modelli, ma nella tabella sono riportati solo quelli rilevanti per la selezione finale. Qwen3-4B-2507 è stato scelto per la sua elevata conformità al formato richiesto e coerenza strategica.

\medskip
	\textbf{Calcolo della percentuale di azioni valide}

Per valutare la conformità delle azioni generate dai modelli LLM rispetto al set ufficiale e al formato richiesto, è stata utilizzata la seguente formula:
\begin{equation}
	\text{Valid Actions \%} = \frac{N_{valid}}{N_{total}} \times 100
\end{equation}
dove $N_{valid}$ è il numero di azioni conformi e $N_{total}$ il numero totale di azioni generate per il campione analizzato.

% --- TABELLA CONFRONTO MODELLI LLM ---
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|p{0.35\textwidth}|}
\hline
\rowcolor{gray!20}
\textbf{Modello} & \textbf{Parametri} & \textbf{Conformità (\%)} & \textbf{Coerenza} & \textbf{Note} \\
\hline
Llama-3.2-1B & 1.1B & 23\% & Bassa & Genera spiegazioni verbose invece di sequenze \\
\hline
Phi-3-mini & 3.8B & 72\% & Media & Difficoltà istruzioni, allucinazioni frequenti \\
\hline
\textbf{Qwen3-4B-2507} & 4B & \textbf{98\%} & \textbf{Molto alta} & \textbf{Selezionato: rispetta formato, sequenze coerenti. Finetunato per tool use e reasoning, oltre a seguire istruzioni specifiche.} \\
\hline
\end{tabular}%
}
\caption{Confronto tra modelli LLM testati per l'Helper. Qwen3-4B-2507 è il migliore per conformità e coerenza.}
\end{table}

\subsubsection{Progettazione del Prompt}

Il prompt per l'Helper è stato sottoposto a progettazione iterativa mediante sperimentazione. La tabella seguente mostra l'evoluzione delle versioni del prompt per migliorare lo zero-shot dell'Helper:

% --- TABELLA 4.1 CON VERSIONI ESPLICITE ---
\begin{table}[H]
\centering
\begin{tabular}{|p{0.22\textwidth}|p{0.35\textwidth}|p{0.30\textwidth}|}
\hline
\rowcolor{gray!20}
\textbf{Versione Prompt} & \textbf{Descrizione} & \textbf{Obiettivo} \\
\hline
v1 (Base) & Azioni semplici, nessun contesto & Sequenza generica \\
\hline
v2 (Intermedio) & Lista azioni valide, goal survival & Sequenza contestualizzata \\
\hline
v3 (Rifinito) & Goal multipli, errori da evitare, stato attuale & Sequenza ottimizzata \\
\hline
\end{tabular}
\caption{Versioni dei prompt Helper LLM}
\end{table}


% --- NUOVA TABELLA: ESEMPI DI PROMPT ---
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|p{0.95\textwidth}|}
\hline
\rowcolor{gray!20}
\textbf{Prompt base} \\
\hline
Generate a sequence of actions for Crafter. \\
Use actions like move, do, place. \\
Format: [action1], [action2] \\
\hline
\rowcolor{gray!20}
\textbf{Prompt intermedio} \\
\hline
You are a Crafter AI. GOALS: Survive and unlock achievements. \\
VALID ACTIONS: move\_up, move\_down, move\_left, move\_right, do, sleep, place\_stone, place\_table, place\_furnace, place\_plant, make\_wood\_pickaxe, make\_stone\_pickaxe, make\_iron\_pickaxe, make\_wood\_sword, make\_stone\_sword, make\_iron\_sword, noop \\
TASK: Generate a sequence of 4 actions. \\
FORMAT: [action1], [action2], [action3], [action4] \\
EXAMPLES: [move\_right], [do], [place\_table] \\
\hline
\rowcolor{gray!20}
\textbf{Prompt raffinato} \\
\hline
You are a Crafter AI. GOALS: 1) Survive 2) Unlock achievements 3) Be efficient. \\
VALID ACTIONS: [full list of 17 actions] \\
MISTAKES TO AVOID: Avoid collect\_wood/gather/mine - use [do] \\
CURRENT STATE: [state description] \\
SURVIVAL: Health =? Use [sleep] \\
ACHIEVEMENT CHAIN: Wood→Table→Pickaxe→Stone→Coal→Iron→Diamond \\
TASK: Generate EXACTLY ONE sequence of 4 actions. \\
FORMAT: [REAL\_ACTION\_1], [REAL\_ACTION\_2], [REAL\_ACTION\_3], [REAL\_ACTION\_4] \\
EXAMPLES: Good: [move\_right], [do], [move\_left], [noop] \\
Bad: [action1], [do.something] \\
YOUR TURN: [final instructions] \\
\hline
\end{tabular}%
}
\caption{Esempi espliciti dei prompt Helper LLM utilizzati}
\end{table}

\subsubsection{Meccanismi di Re-planning}

\textbf{Cos'è il Re-planning?}

Il re-planning è il meccanismo di \textbf{interruzione adattiva della sequenza di azioni} per rispondere a cambiamenti critici dello stato. L'Helper genera una sequenza di 3-5 azioni, ma NON le esegue tutte passivamente. Invece, un sistema di monitoraggio verifica costantemente se accadono eventi importanti:

\begin{itemize}
    \item Se accade un evento critico, la sequenza viene \textbf{immediatamente interrotta}
    \item Una nuova query viene inviata all'Helper con il \textbf{contesto aggiornato}
    \item Una \textbf{nuova sequenza} viene generata e subito eseguita
\end{itemize}

\textbf{Esempio}: L'agente sta eseguendo [move\_right, do, move\_left, noop] quando sblocca il achievement collect\_wood. Immediatamente: interruzione della sequenza, nuova query all'Helper ("hai appena sbloccato wood collection, cosa fai?"), nuova sequenza generata e in esecuzione.

Sono state implementate logiche per interrompere e ri-pianificare durante l'esecuzione di sequenze LLM (costanti nella classe \texttt{CrafterHelper}):

\begin{itemize}
    \item \texttt{REPLAN\_THRESHOLD\_HP = 0.3}: Re-planning se health < 30\% del massimo
    \item \texttt{REPLAN\_THRESHOLD\_HP\_CRITICAL = 5}: Fallback immediato a DQN se health $\leq$ 5
    \item \texttt{REPLAN\_THRESHOLD\_ACHIEVEMENT = True}: Re-planning immediato quando achievement sbloccato
    \item \texttt{REPLAN\_THRESHOLD\_INVENTORY\_CHANGE = True}: Monitoring cambiamenti inventario
\end{itemize}

L'algoritmo seguente descrive il processo di re-planning durante l'esecuzione, garantendo che l'agente possa adattarsi dinamicamente a cambiamenti critici nello stato.

\begin{algorithm}
\caption{Re-planning durante esecuzione}
\begin{algorithmic}
\WHILE{esecuzione sequenza}
    \STATE $next\_state, reward, done, info \gets env.step(action)$
    \IF{achievement sbloccato}
        \STATE Genera nuova sequenza con contesto aggiornato
        \STATE BREAK
    \ENDIF
    \IF{$health \leq 5$}
        \STATE Fallback a DQN per sopravvivenza immediata
        \STATE BREAK
    \ENDIF
    \IF{$health < 0.3 \times max\_health$}
        \STATE Re-query con priorità gestione salute
        \STATE BREAK
    \ENDIF
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\section{Generazione del Dataset per il Reviewer}

\subsubsection{Processo di Raccolta Dati}

Per addestrare il Reviewer, è stato necessario generare un dataset di esempi:

\begin{enumerate}
    \item \textbf{Esecuzione episodi}: 50 episodi di gioco con Helper zero-shot (circa 2,500 esempi di training)
    \item \textbf{Registrazione}: Per ogni chiamata Helper, salvare:
    \begin{itemize}
        \item Stato dell'environment
        \item Sequenza di azioni suggerite
        \item Risultato dell'esecuzione (reward, achievement)
    \end{itemize}
    \item \textbf{Annotazione}: Generazione di feedback basati su:
    \begin{itemize}
        \item Successo/fallimento della sequenza
        \item Efficienza (step sprecati)
        \item Priorità rispetto allo stato (es. salute bassa ignorata)
    \end{itemize}
\end{enumerate}

\subsection{Fase 5: Fine-tuning del Reviewer}

\subsubsection{Scelta del Modello Base}

È stato scelto FLAN-T5-base come modello base per il Reviewer, successivamente fine-tuned con PPO:
\begin{itemize}
    \item Dimensioni gestibili (250M parametri)
    \item Buone capacità di text-to-text generation
    \item Fine-tuned su task di instruction-following
    \item Veloce per inference durante il training
    \item Modello base: \texttt{google/flan-t5-base}
    \item Modello fine-tuned utilizzato: \texttt{reviewer\_retrained\_ppo}
    \item Training: Supervised learning + PPO reinforcement learning
\end{itemize}

\subsubsection{Configurazione Training}

Il fine-tuning è stato eseguito con i seguenti parametri:

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\rowcolor{gray!20}
\textbf{Parametro} & \textbf{Valore} \\
\hline
Optimizer & AdamW \\
\hline
Learning rate & 5e-5 \\
\hline
Batch size & 8 \\
\hline
Epochs & 5 \\
\hline
Max input length & 512 token \\
\hline
Max output length & 128 token \\
\hline
Gradient accumulation steps & 2 \\
\hline
\end{tabular}
\caption{Parametri di fine-tuning del Reviewer}
\end{table}

\subsubsection{Validazione}

Il dataset è stato diviso in:
\begin{itemize}
    \item Training set: 80\% (circa 2,000 esempi)
    \item Validation set: 20\% (circa 500 esempi)
\end{itemize}

Metriche monitorate durante il training:
\begin{itemize}
    \item Training loss
    \item Validation loss
    \item BLEU score (similarità con feedback attesi)
\end{itemize}

\section{Training Integrato HeRoN}

\subsubsection{Protocollo di Training}

Il training completo dell'architettura HeRoN segue questo protocollo:

\subsubsection{Parametri di Training}

\begin{itemize}
    \item \textbf{Episodi totali}: 300
    \item \textbf{Max steps per episodio}: 1000
    \item \textbf{LLM cutoff}: Episodio 100 (dopo, solo DQN)
    \item \textbf{Checkpoint}: Salvataggio ogni 50 episodi + best model
\end{itemize}

\subsubsection{Analisi del Numero Ottimale di Azioni}

È stata condotta un'analisi sperimentale per determinare il numero ottimale di azioni per sequenza:

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Azioni per sequenza} & \textbf{Achievement medi} & \textbf{Chiamate Helper/episodio} \\
\hline
1 & 3.2 & 150-200 \\
\hline
3 & 4.5 & 50-80 \\
\hline
5 & 4.8 & 30-50 \\
\hline
7 & 4.3 & 20-35 \\
\hline
10 & 3.9 & 15-25 \\
\hline
\end{tabular}%
\caption{Impatto del numero di azioni per sequenza}
\end{table}

Il valore ottimale è risultato essere 5 azioni, che bilancia:
\begin{itemize}
    \item Pianificazione strategica (non troppo breve)
    \item Flessibilità di re-planning (non troppo lungo)
    \item Overhead computazionale LLM
\end{itemize}

\section{Configurazioni di Training}

Sono state implementate cinque configurazioni per valutare diverse strategie di integrazione LLM-RL:

\subsection{DQN Baseline}

\textbf{Strategia}: Pure Reinforcement Learning senza integrazione LLM. Usa sempre DQN con epsilon-greedy, senza consultazione LLM, con reward shaping applicato per facilitare apprendimento.

\subsection{DQN + Helper}

\textbf{Strategia}: LLM (solo Helper, senza Reviewer) attivo nei primi 100 step di ogni episodio.

\textbf{Parametri}:
\begin{itemize}
    \item \texttt{ASSISTED\_STEPS = 100}
    \item \texttt{threshold\_episodes = 100}
\end{itemize}

\subsection{HeRoN Initial}

\textbf{Strategia}: LLM (Helper + Reviewer) attivo nei primi 100 step di ogni episodio.

\textbf{Parametri}:
\begin{itemize}
    \item \texttt{ASSISTED\_STEPS = 100}
    \item \texttt{threshold\_episodes = 100}
    \item Reviewer: T5 PPO fine-tuned (\texttt{reviewer\_retrained\_ppo})
\end{itemize}

\subsection{HeRoN Random}

\textbf{Strategia}: LLM (Helper + Reviewer) attivo con probabilità casuale 50\% ad ogni step.

\textbf{Parametri}:
\begin{itemize}
    \item \texttt{LLM\_PROBABILITY = 0.5}
    \item \texttt{threshold\_episodes = 100}
    \item Reviewer: T5 PPO fine-tuned (\texttt{reviewer\_retrained\_ppo})
\end{itemize}

\subsection{HeRoN Final}

\textbf{Strategia}: LLM (Helper + Reviewer) con probabilità crescente durante ogni episodio (threshold decay per-step).

\textbf{Parametri}:
\begin{itemize}
    \item \texttt{KAPPA = 0.01} (decremento threshold per step, equivalente al parametro $k$ nella formula $\theta(t) = \max(0, 1.0 - k \times t)$)
    \item \texttt{threshold\_episodes = 100}
    \item Reviewer: T5 PPO fine-tuned (\texttt{reviewer\_retrained\_ppo})
\end{itemize}

\textbf{Significato di k (KAPPA)}:

Il parametro $k$ rappresenta il \textbf{tasso di decadimento del threshold} per-step. Specificamente:
\begin{itemize}
    \item $k = 0.01$ significa che il threshold si riduce di 0.01 ad ogni step dell'episodio
    \item Dopo 100 step, il threshold raggiunge 0 (transizione completa verso LLM al 100\%)
    \item Quindi, \textbf{il parametro k determina quanto rapidamente l'agente passa dal 100\% DQN (step 0) al 100\% LLM} 
    \item Valori di $k$ \textbf{più alti} (es. 0.05) comportano una \textbf{transizione più rapida} verso LLM
    \item Valori di $k$ \textbf{più bassi} (es. 0.005) mantengono \textbf{più a lungo l'esplorazione DQN autonoma}
\end{itemize}

\textbf{Evoluzione del threshold durante l'episodio}:
\begin{itemize}
    \item Step 0: $\theta = 1.0$ (0\% LLM, 100\% DQN)
    \item Step 50: $\theta = 0.5$ (~50\% LLM, ~50\% DQN)
    \item Step 100+: $\theta = 0.0$ (100\% LLM, 0\% DQN)
    \item Il threshold si resetta a 1.0 all'inizio di ogni nuovo episodio
\end{itemize}

\section{Parametri di Training Comuni}

Tutte le configurazioni condividono i seguenti parametri di base:

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\rowcolor{gray!20}
\textbf{Parametro} & \textbf{Valore} \\
\hline
Episodi totali & 300 \\
\hline
Max steps per episodio & 1000 \\
\hline
Batch size DQN & 64 \\
\hline
Replay buffer size & 5,000 transizioni \\
\hline
Learning rate & 0.0001 (Adam) \\
\hline
Gamma ($\gamma$) & 0.99 \\
\hline
Epsilon decay & Lineare 1.0 $\rightarrow$ 0.05 in 300 episodi \\
\hline
Target network update & Ogni 100 step (hard copy) \\
\hline
Prioritized Replay $\alpha$ & 0.6 \\
\hline
Prioritized Replay $\beta$ & 0.4 $\rightarrow$ 1.0 (incremento +0.001/step) \\
\hline
LLM cutoff episodi & 100 (DQN autonomo dopo ep. 100) \\
\hline
LLM model & qwen/qwen3-4b-2507 (via LM Studio) \\
\hline
Reviewer model & T5 fine-tuned con PPO (reviewer\_retrained\_ppo) \\
\hline
\end{tabular}
\caption{Parametri di training comuni a tutte le configurazioni}
\end{table}

\section{Metriche di Valutazione}

Per valutare le prestazioni di HeRoN e delle sue varianti, sono state definite diverse metriche:

\begin{enumerate}
    \item \textbf{Achievement Score}: Numero medio di achievement sbloccati per episodio
    $$\text{Score} = \frac{1}{N} \sum_{i=1}^N \text{achievements}_i$$
    
    \item \textbf{Coverage}: Percentuale di achievement unici sbloccati almeno una volta
    $$\text{Coverage} = \frac{|\text{achievement unici}|}{22} \times 100\%$$
    
    \item \textbf{Success Rate per Achievement}: Percentuale di episodi in cui ciascun achievement è stato sbloccato
    
    \item \textbf{Reward Cumulativo}: Somma dei reward durante l'episodio (shaped e nativo)
\end{enumerate}

\subsubsection{Baseline di Confronto}

HeRoN è stato confrontato con:
\begin{itemize}
    \item \textbf{DQN puro}: Stesso agente senza componenti LLM
    \item \textbf{Helper solo}: DQN + Helper senza Reviewer
\end{itemize}

\subsubsection{Protocollo di Test}

Per garantire validità statistica:
\begin{itemize}
    \item Ogni configurazione testata per 100 episodi
    \item 5 seed casuali diversi
    \item Media e deviazione standard riportate
    \item Test statistici (t-test) per significatività
\end{itemize}

\subsection{Fase 8: Analisi e Ottimizzazione}

\subsection{Tuning degli Iperparametri}

Grid search limitata su:
% La 'grid search limitata' consiste nel testare sistematicamente solo un sottoinsieme selezionato di combinazioni di iperparametri, invece di esplorare tutte le possibili opzioni. Questo approccio permette di individuare rapidamente i valori più promettenti, riducendo il tempo e le risorse computazionali necessarie rispetto a una grid search completa.
\begin{itemize}
    \item Learning rate DQN: [1e-4, 5e-4, 1e-3]
    \item Threshold decay rate: [0.005, 0.01, 0.02]
    \item Peso reward shaping: [0.5, 1.0, 2.0]
\end{itemize}

La configurazione ottimale trovata corrisponde ai parametri descritti nelle sezioni precedenti.

\subsection{Tuning e Configurazioni del Reviewer (T5)}

Per il modulo Reviewer, basato su T5, sono state testate diverse configurazioni di tuning. La tabella seguente riassume i principali parametri utilizzati durante la fase di fine-tuning e generazione del dataset:

\begin{table}[H]
\centering
\caption{Configurazioni testate per il tuning del Reviewer (T5)}
\resizebox{0.8\textwidth}{!}{%
\begin{tabular}{|l|l|}
\hline
\rowcolor{gray!20} 
\textbf{Parametro} & \textbf{Valore/Testato} \\
\hline
Modello & google/flan-t5-base \\
\hline
Dimensione dataset & 2000--5000 samples \\
\hline
Episodi generazione dataset & 50--100 \\
\hline
Lunghezza episodio & 500 steps \\
\hline
Batch size (train/eval) & 8 \\
\hline
Epoche & 5 \\
\hline
Learning rate & 5e-5 \\
\hline
Weight decay & 0.01 \\
\hline
Logging steps & 10 \\
\hline
Metriche best model & eval\_loss \\
\hline
Limite salvataggi & 3 \\
\hline
Helper calls per episodio & ogni 5 step \\
\hline
\end{tabular}%
}
\end{table}

Queste configurazioni sono state selezionate tramite test iterativi e grid search limitata, con l'obiettivo di massimizzare la qualità delle correzioni generate dal Reviewer e la generalizzazione sulle strategie di gioco. Il dataset è stato generato simulando tra 50 e 100 episodi, con chiamate al modulo Helper ogni 5 step, per ottenere una varietà di situazioni e feedback strategici.

\section{Estensione: Fine-tuning del Reviewer tramite RL}

Il fine-tuning avanzato del modulo Reviewer è stato realizzato tramite Reinforcement Learning (PPO), seguendo un workflow strutturato:

\begin{itemize}
    \item \textbf{Generazione del dataset:} Per ogni episodio vengono raccolti:
    \begin{itemize}
        \item Stato dell'environment (descrizione dettagliata)
        \item Sequenza di azioni suggerite dall'Helper
        \item Feedback correttivo (strategic feedback) generato da regole o Reviewer simulato
        \item Reward associato alla qualità del feedback
    \end{itemize}
    \item \textbf{Struttura del sample:} Ogni esempio contiene: stato, azioni, feedback, reward, outcome, refined sequence.
    \item \textbf{Addestramento RL:} Il Reviewer viene addestrato tramite PPO (Proximal Policy Optimization), ottimizzando la policy per generare feedback strategici e correttivi, massimizzando il reward rispetto a un target ideale.
    \item \textbf{Obiettivo:} Migliorare la capacità del Reviewer di fornire feedback utili e strategici, ottimizzando la collaborazione con Helper e NPC.
\end{itemize}

\begin{table}[H]
\centering
\caption{Workflow Fine-Tuning Reviewer RL}
\resizebox{0.7\textwidth}{!}{%
\begin{tabular}{|l|l|}
\hline
\rowcolor{gray!20} 
\textbf{Fase} & \textbf{Descrizione} \\
\hline
Generazione dataset & Stati, azioni Helper, feedback, reward \\
\hline
Feedback & Regole/Reviewer simulato, strategico \\
\hline
Algoritmo RL & PPO (Proximal Policy Optimization) \\
\hline
Reward & Qualità del feedback rispetto al target \\
\hline
Obiettivo & Policy ottimizzata per feedback strategici \\
\hline
\end{tabular}%
}
\end{table}

Questo approccio consente al Reviewer di apprendere non solo dai dati supervisionati, ma anche dall'interazione iterativa e dal reward, migliorando la qualità dei suggerimenti e la sinergia tra i moduli dell'architettura HeRoN.

\subsection{Reward Function per PPO}

La reward function utilizzata per l'addestramento PPO del Reviewer è stata progettata per valutare la qualità dei feedback strategici generati. La funzione è multi-componente e combina diversi criteri:

\begin{equation}
r = r_{\text{length}} + r_{\text{terms}} + r_{\text{actions}} + r_{\text{quality}} + r_{\text{penalty}}
\end{equation}

I componenti sono definiti come segue:

\begin{itemize}
    \item \textbf{Penalità lunghezza} ($r_{\text{length}}$): Feedback troppo corti (meno di 10 caratteri) o vuoti ricevono una penalità di $-5.0$, poiché non forniscono informazioni utili.
    
    \item \textbf{Bonus termini strategici} ($r_{\text{terms}}$): 
    \begin{equation}
    r_{\text{terms}} = 0.5 \times \sum_{t \in T} \mathbb{1}[t \in \text{feedback}]
    \end{equation}
    dove $T$ è l'insieme dei termini strategici specifici di Crafter: \textit{achievement, resource, collect, craft, health, wood, stone, iron, pickaxe, sword, table, prioritize, efficiency, progression, tier}.
    
    \item \textbf{Overlap azioni} ($r_{\text{actions}}$): Misura la corrispondenza tra le azioni suggerite nel feedback e quelle ideali nel dataset:
    \begin{equation}
    r_{\text{actions}} = 3.0 \times \frac{|A_{\text{ideal}} \cap A_{\text{suggested}}|}{\max(|A_{\text{ideal}}|, 1)}
    \end{equation}
    dove $A_{\text{ideal}}$ e $A_{\text{suggested}}$ sono rispettivamente gli insiemi di azioni nel feedback target e in quello generato, estratte tramite pattern matching sui tag \texttt{[action]}.
    
    \item \textbf{Indicatori di qualità} ($r_{\text{quality}}$): Un bonus di $+2.0$ viene assegnato se il feedback contiene indicatori strutturati come \texttt{EXCELLENT}, \texttt{GOOD}, \texttt{CRITICAL}, \texttt{WARNING} o \texttt{SUGGESTION}.
    
    \item \textbf{Penalità verbosità} ($r_{\text{penalty}}$): Feedback eccessivamente lunghi (oltre 500 caratteri) ricevono una penalità di $-1.0$ per scoraggiare output prolissi e poco concisi.
\end{itemize}

\subsubsection{Pipeline di Addestramento PPO}

Il processo di addestramento segue questi passi:

\begin{enumerate}
    \item \textbf{Input}: Il modello riceve la concatenazione di stato del gioco e risposta dell'Helper
    \item \textbf{Generazione}: Il Reviewer genera un feedback strategico
    \item \textbf{Calcolo reward}: La reward function valuta la qualità del feedback generato
    \item \textbf{Aggiornamento policy}: PPO aggiorna i pesi del modello per massimizzare il reward atteso
\end{enumerate}

\begin{table}[H]
\centering
\caption{Parametri PPO per Fine-Tuning Reviewer}
\begin{tabular}{|l|l|}
\hline
\rowcolor{gray!20}
\textbf{Parametro} & \textbf{Valore} \\
\hline
Learning rate & $5 \times 10^{-7}$ \\
\hline
PPO epochs & 1 \\
\hline
Mini batch size & 1 \\
\hline
Batch size & 1 \\
\hline
Temperature (generazione) & 0.4 \\
\hline
Top-k sampling & 50 \\
\hline
Top-p sampling & 0.8 \\
\hline
Max new tokens & 128 \\
\hline
\end{tabular}
\end{table}

Questa configurazione è stata scelta per garantire stabilità nell'addestramento e generazione di feedback concisi ma informativi.