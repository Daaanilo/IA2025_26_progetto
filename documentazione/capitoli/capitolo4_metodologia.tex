\chapter{Metodologia di Implementazione}

\section{Introduzione}

Questo capitolo descrive la metodologia utilizzata per sviluppare e valutare l'architettura HeRoN nel dominio Crafter. Il processo di sviluppo è articolato in cinque fasi principali:

\begin{enumerate}
    \item Implementazione e addestramento del NPC (DQN baseline)
    \item Progettazione e integrazione dell'Helper (LLM)
    \item Generazione del dataset per il Reviewer
    \item Fine-tuning del Reviewer (supervised + PPO)
    \item Training integrato dell'architettura HeRoN e valutazione
\end{enumerate}

\section{NPC (DQN Agent)}

L'agente DQN costituisce il modulo di base dell'architettura HeRoN, responsabile dell'apprendimento attraverso reinforcement learning e dell'esecuzione delle azioni nell'environment Crafter.

\subsection{Architettura della Rete Neurale}

La rete neurale feedforward è composta da:
\begin{itemize}
    \item \textbf{Input Layer}: 43 neuroni (dimensione dello stato Crafter)
    \item \textbf{Hidden Layers}: 128-128-64 neuroni con attivazione ReLU
    \item \textbf{Output Layer}: 17 neuroni (Q-values per ciascuna azione)
\end{itemize}

\subsection{Algoritmi di Reinforcement Learning}

L'addestramento dell'agente implementa diverse tecniche avanzate di reinforcement learning:

\begin{itemize}
    \item \textbf{Double DQN}: Utilizza due reti neurali separate per ridurre la sovrastima dei Q-values:
    \begin{itemize}
        \item Policy network per la selezione delle azioni
        \item Target network per la valutazione, aggiornata ogni 100 step: $\theta_{target} \leftarrow \theta_{policy}$
    \end{itemize}
    Il target viene calcolato come:
    $$y = r + \gamma Q'(s', \arg\max_{a'} Q(s',a'))$$

    \item \textbf{Prioritized Experience Replay}: Campionamento prioritario basato sul TD-error per ottimizzare l'apprendimento:
        \begin{align*}
        p_i &= |\delta_i|^\alpha + \epsilon && \text{(priorità)} \\
        P(i) &= \frac{p_i^\alpha}{\sum_k p_k^\alpha} && \text{(probabilità di sampling)} \\
        w_i &= \left(\frac{1}{N \cdot P(i)}\right)^\beta && \text{(importance sampling weights)}
        \end{align*}
   
    \item \textbf{Backpropagation}: Aggiornamento dei pesi tramite discesa del gradiente:
    $$\theta \leftarrow \theta - \alpha \nabla_{\theta} L$$
    dove $L = \mathbb{E}[(Q(s,a) - y)^2]$ è la funzione di perdita Q.
\end{itemize}

\subsection{Parametri di Training DQN}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\rowcolor{gray!20}
\textbf{Parametro} & \textbf{Valore} \\
\hline
Episodi totali & 300 \\
\hline
Max steps per episodio & 1000 \\
\hline
Batch size & 64 \\
\hline
Replay buffer size & 5,000 transizioni \\
\hline
Learning rate ($\alpha$) & 0.0001 (Adam) \\
\hline
Discount factor ($\gamma$) & 0.99 \\
\hline
Epsilon decay & Lineare 1.0 $\rightarrow$ 0.05 in 300 episodi \\
\hline
Target network update & Ogni 100 step (hard copy) \\
\hline
PER $\alpha$ & 0.6 \\
\hline
PER $\beta$ & 0.4 $\rightarrow$ 1.0 (+0.001/step) \\
\hline
\end{tabular}
\caption{Parametri di training dell'agente DQN (comuni a tutte le configurazioni)}
\end{table}

\section{Helper (LLM)}

Il modulo Helper fornisce suggerimenti strategici sotto forma di sequenze di azioni, guidando l'agente DQN verso obiettivi a lungo termine. La progettazione ha richiesto sia la selezione del modello LLM più adatto sia lo sviluppo iterativo di prompt efficaci.

\subsection{Selezione del Modello}

Sono stati testati diversi modelli LLM, valutando conformità al formato richiesto e coerenza strategica. La percentuale di azioni valide è stata calcolata come:
\begin{equation}
	\text{Valid Actions \%} = \frac{N_{valid}}{N_{total}} \times 100
\end{equation}

% --- TABELLA CONFRONTO MODELLI LLM ---
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|p{0.35\textwidth}|}
\hline
\rowcolor{gray!20}
\textbf{Modello} & \textbf{Parametri} & \textbf{Conformità (\%)} & \textbf{Coerenza} & \textbf{Note} \\
\hline
Llama-3.2-1B & 1.1B & 23\% & Bassa & Genera spiegazioni verbose invece di sequenze \\
\hline
Phi-3-mini & 3.8B & 72\% & Media & Difficoltà istruzioni, allucinazioni frequenti \\
\hline
\textbf{Qwen3-4B-2507} & 4B & \textbf{98\%} & \textbf{Molto alta} & \textbf{Selezionato: rispetta formato, sequenze coerenti. Finetunato per tool use e reasoning} \\
\hline
\end{tabular}%
}
\caption{Confronto tra modelli LLM testati per l'Helper}
\end{table}

\subsection{Progettazione del Prompt}

Il prompt è stato sviluppato attraverso sperimentazione iterativa per massimizzare le prestazioni zero-shot:

% --- TABELLA 4.1 CON VERSIONI ESPLICITE ---
\begin{table}[H]
\centering
\begin{tabular}{|p{0.22\textwidth}|p{0.35\textwidth}|p{0.30\textwidth}|}
\hline
\rowcolor{gray!20}
\textbf{Versione Prompt} & \textbf{Descrizione} & \textbf{Obiettivo} \\
\hline
v1 (Base) & Azioni semplici, nessun contesto & Sequenza generica \\
\hline
v2 (Intermedio) & Lista azioni valide, goal survival & Sequenza contestualizzata \\
\hline
v3 (Rifinito) & Goal multipli, errori da evitare, stato attuale & Sequenza ottimizzata \\
\hline
\end{tabular}
\caption{Evoluzione dei prompt Helper LLM}
\end{table}

\newpage

% --- NUOVA TABELLA: ESEMPI DI PROMPT ---
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|p{0.95\textwidth}|}
\hline
\rowcolor{gray!20}
\textbf{Prompt base (v1)} \\
\hline
Generate a sequence of actions for Crafter. \\
Use actions like move, do, place. \\
Format: [action1], [action2] \\
\hline
\rowcolor{gray!20}
\textbf{Prompt intermedio (v2)} \\
\hline
You are a Crafter AI. GOALS: Survive and unlock achievements. \\
VALID ACTIONS: move\_up, move\_down, move\_left, move\_right, do, sleep, place\_stone, place\_table, place\_furnace, place\_plant, make\_wood\_pickaxe, make\_stone\_pickaxe, make\_iron\_pickaxe, make\_wood\_sword, make\_stone\_sword, make\_iron\_sword, noop \\
TASK: Generate a sequence of 4 actions. \\
FORMAT: [action1], [action2], [action3], [action4] \\
EXAMPLES: [move\_right], [do], [place\_table] \\
\hline
\rowcolor{gray!20}
\textbf{Prompt raffinato (v3)} \\
\hline
You are a Crafter AI. GOALS: 1) Survive 2) Unlock achievements 3) Be efficient. \\
VALID ACTIONS: [full list of 17 actions] \\
MISTAKES TO AVOID: Avoid collect\_wood/gather/mine - use [do] \\
CURRENT STATE: [state description] \\
SURVIVAL: Health =? Use [sleep] \\
ACHIEVEMENT CHAIN: Wood→Table→Pickaxe→Stone→Coal→Iron→Diamond \\
TASK: Generate EXACTLY ONE sequence of 4 actions. \\
FORMAT: [REAL\_ACTION\_1], [REAL\_ACTION\_2], [REAL\_ACTION\_3], [REAL\_ACTION\_4] \\
EXAMPLES: Good: [move\_right], [do], [move\_left], [noop] \\
Bad: [action1], [do.something] \\
YOUR TURN: [final instructions] \\
\hline
\end{tabular}%
}
\caption{Esempi espliciti dei prompt Helper LLM utilizzati}
\end{table}

\subsection{Numero Ottimale di Azioni per Sequenza}

È stata condotta un'analisi sperimentale per determinare la lunghezza ottimale delle sequenze:

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Azioni/sequenza} & \textbf{Achievement medi} & \textbf{Chiamate Helper/ep.} \\
\hline
1 & 3.2 & 150-200 \\
\hline
3 & 4.5 & 50-80 \\
\hline
\textbf{5} & \textbf{4.8} & \textbf{30-50} \\
\hline
7 & 4.3 & 20-35 \\
\hline
10 & 3.9 & 15-25 \\
\hline
\end{tabular}
\caption{Impatto del numero di azioni per sequenza}
\end{table}

Il valore ottimale è risultato 5 azioni, che bilancia pianificazione strategica, flessibilità di re-planning e overhead computazionale.


\section{Reviewer (T5): Dataset e Fine-tuning}

Il modulo Reviewer analizza le sequenze dell'Helper e fornisce feedback correttivi. Il suo sviluppo ha richiesto la generazione di un dataset specifico e un processo di fine-tuning in due fasi: supervised learning e reinforcement learning (PPO).

\subsection{Generazione del Dataset}

Il dataset è stato creato attraverso l'esecuzione di 150 episodi con Helper zero-shot, raccogliendo circa 15.000 esempi. Per ogni chiamata Helper sono stati registrati:
\begin{itemize}
    \item Stato dell'environment (descrizione dettagliata)
    \item Sequenza di azioni suggerite dall'Helper
    \item Achievement sbloccati durante l'esecuzione
    \item Feedback strategico e istruzioni correttive
    \item Metadati (episodio, step, reward)
\end{itemize}

\subsection{Scelta del Modello Base}

È stato selezionato FLAN-T5-base (\texttt{google/flan-t5-base}) per le seguenti caratteristiche:
\begin{itemize}
    \item Dimensioni gestibili (250M parametri)
    \item Capacità di text-to-text generation
    \item Pre-training su task di instruction-following
    \item Efficienza nell'inference durante il training
\end{itemize}

\subsection{Fase 1: Fine-tuning Supervised}

Il primo stadio di addestramento utilizza supervised learning sui dati raccolti:

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\rowcolor{gray!20}
\textbf{Parametro} & \textbf{Valore} \\
\hline
Optimizer & AdamW \\
\hline
Learning rate & 5e-5 \\
\hline
Batch size & 8 \\
\hline
Epochs & 5 \\
\hline
Max input length & 512 token \\
\hline
Max output length & 150 token \\
\hline
Weight decay & 0.01 \\
\hline
\end{tabular}
\caption{Parametri di fine-tuning supervisionato del Reviewer}
\end{table}

\subsection{Fase 2: Fine-tuning con Reinforcement Learning (PPO)}

Dopo il supervised learning, il Reviewer viene ulteriormente ottimizzato tramite PPO per massimizzare la qualità dei feedback strategici.

\subsubsection{Reward Function}

La reward function multi-componente valuta la qualità dei feedback generati:

\begin{equation}
r = r_{\text{length}} + r_{\text{terms}} + r_{\text{actions}} + r_{\text{quality}} + r_{\text{penalty}}
\end{equation}

dove:
\begin{itemize}
    \item $r_{\text{length}} = -5.0$ se il feedback è vuoto o $<$ 10 caratteri
    \item $r_{\text{terms}} = 0.5 \times \sum_{t \in T} \mathbb{1}[t \in \text{feedback}]$, con $T$ = \{achievement, resource, collect, craft, health, wood, stone, iron, pickaxe, sword, table, prioritize, efficiency, progression, tier\}
    \item $r_{\text{actions}} = 3.0 \times \frac{|A_{\text{ideal}} \cap A_{\text{suggested}}|}{\max(|A_{\text{ideal}}|, 1)}$ (overlap azioni)
    \item $r_{\text{quality}} = +2.0$ se il feedback contiene indicatori strutturati (EXCELLENT, GOOD, CRITICAL, WARNING, SUGGESTION)
    \item $r_{\text{penalty}} = -1.0$ se il feedback supera i 500 caratteri
\end{itemize}

\subsubsection{Pipeline di Addestramento PPO}

\begin{enumerate}
    \item \textbf{Input}: Concatenazione di stato del gioco e risposta dell'Helper
    \item \textbf{Generazione}: Il Reviewer genera un feedback strategico
    \item \textbf{Calcolo reward}: La reward function valuta la qualità del feedback
    \item \textbf{Aggiornamento policy}: PPO aggiorna i pesi per massimizzare il reward atteso
\end{enumerate}

\begin{table}[H]
\centering
\caption{Parametri PPO per Fine-Tuning Reviewer}
\begin{tabular}{|l|l|}
\hline
\rowcolor{gray!20}
\textbf{Parametro} & \textbf{Valore} \\
\hline
Learning rate & $5 \times 10^{-7}$ \\
\hline
Training epochs (esterni) & 3 \\
\hline
PPO epochs (interni) & 1 \\
\hline
Mini batch size & 1 \\
\hline
Temperature & 0.4 \\
\hline
Top-k sampling & 50 \\
\hline
Top-p sampling & 0.8 \\
\hline
Max new tokens & 128 \\
\hline
\end{tabular}
\end{table}

\section{Training Integrato HeRoN}

Completata la preparazione dei singoli moduli, l'architettura HeRoN viene addestrata integrando NPC (DQN), Helper (LLM) e Reviewer (T5). Il training prevede l'uso dei moduli LLM nei primi 100 episodi (cutoff), con transizione al DQN autonomo.

\subsection{Configurazioni Sperimentali}

Sono state implementate cinque configurazioni per valutare diverse strategie di integrazione:

\subsubsection{1. DQN Baseline}
\textbf{Strategia}: RL puro senza LLM, solo DQN con epsilon-greedy e reward shaping.

\subsubsection{2. DQN + Helper}
\textbf{Strategia}: Helper attivo nei primi 100 step/episodio, senza Reviewer.
\begin{itemize}
    \item \texttt{ASSISTED\_STEPS = 100}
    \item \texttt{threshold\_episodes = 100}
\end{itemize}

\subsubsection{3. HeRoN Initial}
\textbf{Strategia}: Helper + Reviewer attivi nei primi 100 step/episodio.
\begin{itemize}
    \item \texttt{ASSISTED\_STEPS = 100}
    \item \texttt{threshold\_episodes = 100}
\end{itemize}

\subsubsection{4. HeRoN Random}
\textbf{Strategia}: Helper + Reviewer con probabilità 50\% ad ogni step.
\begin{itemize}
    \item \texttt{LLM\_PROBABILITY = 0.5}
    \item \texttt{threshold\_episodes = 100}
\end{itemize}

\subsubsection{5. HeRoN Final}
\textbf{Strategia}: Helper + Reviewer con probabilità crescente durante l'episodio.
\begin{itemize}
    \item \texttt{K = 0.01}
    \item \texttt{threshold\_episodes = 100}
\end{itemize}

\subsection{Parametri di Training}

Tutte le configurazioni condividono i parametri DQN descritti nella Sezione 4.2.3, con i seguenti parametri aggiuntivi:

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\rowcolor{gray!20}
\textbf{Parametro} & \textbf{Valore} \\
\hline
Episodi totali & 300 \\
\hline
Max steps per episodio & 1000 \\
\hline
LLM cutoff episodi & 100 \\
\hline
Azioni per sequenza Helper & 5 \\
\hline
Helper model & qwen/qwen3-4b-2507 \\
\hline
Reviewer model & FLAN-T5-base (fine-tuned) \\
\hline
\end{tabular}
\caption{Parametri specifici per training integrato HeRoN}
\end{table}


\section{Metriche di Valutazione e Protocollo Sperimentale}

\subsection{Metriche}

Per valutare le prestazioni delle diverse configurazioni sono state definite le seguenti metriche:

\begin{enumerate}
    \item \textbf{Achievement Score}: Numero medio di achievement sbloccati per episodio
    $$\text{Score} = \frac{1}{N} \sum_{i=1}^N \text{achievements}_i$$
    
    \item \textbf{Coverage}: Percentuale di achievement unici sbloccati almeno una volta
    $$\text{Coverage} = \frac{|\text{achievement unici}|}{22} \times 100\%$$
    
    \item \textbf{Success Rate per Achievement}: Percentuale di episodi in cui ciascun achievement è stato sbloccato
    
    \item \textbf{Reward Cumulativo}: Somma dei reward durante l'episodio (shaped e nativo)
\end{enumerate}

\subsection{Baseline di Confronto}

HeRoN è stato confrontato con:
\begin{itemize}
    \item \textbf{DQN baseline}: Agente senza componenti LLM
    \item \textbf{DQN + Helper}: DQN con solo Helper, senza Reviewer
\end{itemize}
