\chapter{Implementazione}

\section{Organizzazione del Codice}

Il progetto è organizzato nella seguente struttura di directory:

\begin{verbatim}
IA2025_26_progetto/
├── classes/
│   ├── agent.py                    # DQN Agent
│   ├── crafter_environment.py      # Environment wrapper
│   ├── crafter_helper.py           # Helper LLM
│   └── instructor_agent.py         # Reviewer T5
├── training/
│   ├── heron_training.py           # Training HeRoN completo
│   └── DQN_training.py             # Baseline DQN
├── dataset Reviewer/
│   └── crafter_dataset_generation.py
├── reviewer_fine_tuning/
│   ├── reviewer_fine_tuning.py
│   └── game_scenarios_dataset_crafter.jsonl
├── evaluation/
│   ├── evaluation_system.py
│   ├── evaluation_report_generator.py
│   └── evaluation_plots.py
├── models/                         # Modelli salvati
├── checkpoints/                    # Checkpoint durante training
└── requirements.txt
\end{verbatim}

\section{Implementazione del DQN Agent}

\subsection{Architettura della Rete (\texttt{agent.py})}

La classe \texttt{DQNAgent} implementa l'algoritmo Double DQN con Prioritized Experience Replay:

\begin{lstlisting}[language=Python]
class DQN(nn.Module):
    def __init__(self, state_size, action_size):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_size, 256)
        self.dropout1 = nn.Dropout(0.1)
        self.fc2 = nn.Linear(256, 256)
        self.dropout2 = nn.Dropout(0.1)
        self.fc3 = nn.Linear(256, 128)
        self.fc4 = nn.Linear(128, action_size)
    
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        x = self.dropout2(x)
        x = F.relu(self.fc3(x))
        return self.fc4(x)
\end{lstlisting}

\subsection{Prioritized Replay Buffer}

Il replay buffer implementa sampling basato su priorità:

\begin{lstlisting}[language=Python]
class PrioritizedReplayBuffer:
    def __init__(self, capacity, alpha=0.6, beta=0.4):
        self.capacity = capacity
        self.alpha = alpha
        self.beta = beta
        self.buffer = []
        self.priorities = []
        self.position = 0
    
    def add(self, state, action, reward, next_state, done):
        max_priority = max(self.priorities) if self.priorities else 1.0
        
        if len(self.buffer) < self.capacity:
            self.buffer.append((state, action, reward, next_state, done))
            self.priorities.append(max_priority)
        else:
            self.buffer[self.position] = (state, action, reward, next_state, done)
            self.priorities[self.position] = max_priority
        
        self.position = (self.position + 1) % self.capacity
    
    def sample(self, batch_size):
        # Calcola probabilità di sampling
        priorities = np.array(self.priorities[:len(self.buffer)])
        probs = priorities ** self.alpha
        probs /= probs.sum()
        
        # Campiona indici
        indices = np.random.choice(len(self.buffer), batch_size, p=probs)
        
        # Calcola importance sampling weights
        total = len(self.buffer)
        weights = (total * probs[indices]) ** (-self.beta)
        weights /= weights.max()
        
        return indices, weights, [self.buffer[idx] for idx in indices]
    
    def update_priorities(self, indices, td_errors):
        for idx, error in zip(indices, td_errors):
            self.priorities[idx] = abs(error) + 1e-5
\end{lstlisting}

\subsection{Metodo di Training}

L'aggiornamento dei pesi della rete segue l'algoritmo Double DQN:

\begin{lstlisting}[language=Python]
def train_step(self, batch_size):
    if len(self.memory) < batch_size:
        return
    
    # Campiona batch dal replay buffer
    indices, weights, batch = self.memory.sample(batch_size)
    states, actions, rewards, next_states, dones = zip(*batch)
    
    # Converti a tensori
    states = torch.FloatTensor(states).to(self.device)
    actions = torch.LongTensor(actions).to(self.device)
    rewards = torch.FloatTensor(rewards).to(self.device)
    next_states = torch.FloatTensor(next_states).to(self.device)
    dones = torch.FloatTensor(dones).to(self.device)
    weights = torch.FloatTensor(weights).to(self.device)
    
    # Current Q values
    current_q = self.model(states).gather(1, actions.unsqueeze(1))
    
    # Double DQN: seleziona azione con policy network
    next_actions = self.model(next_states).argmax(1).unsqueeze(1)
    # Valuta con target network
    next_q = self.target_model(next_states).gather(1, next_actions)
    
    # Calcola target Q values
    target_q = rewards.unsqueeze(1) + (1 - dones.unsqueeze(1)) * self.gamma * next_q
    
    # TD errors per prioritized replay
    td_errors = (current_q - target_q).detach().cpu().numpy()
    
    # Loss pesato da importance sampling weights
    loss = (weights.unsqueeze(1) * F.mse_loss(current_q, target_q, reduction='none')).mean()
    
    # Backpropagation
    self.optimizer.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
    self.optimizer.step()
    
    # Aggiorna priorità nel buffer
    self.memory.update_priorities(indices, td_errors.flatten())
    
    return loss.item()
\end{lstlisting}

\subsection{Selezione delle Azioni}

La selezione delle azioni implementa epsilon-greedy con decay:

\begin{lstlisting}[language=Python]
def act(self, state, env):
    if random.random() < self.epsilon:
        # Esplorazione: azione casuale
        return env.action_space.sample()
    
    # Exploitation: azione con massimo Q-value
    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
    with torch.no_grad():
        q_values = self.model(state_tensor)
    return q_values.argmax().item()

def update_epsilon(self):
    self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
\end{lstlisting}

\subsection{Persistenza del Modello}

Il sistema di salvataggio memorizza 4 file per ricostruire completamente lo stato dell'agente:

\begin{lstlisting}[language=Python]
def save(self, filepath):
    # Salva pesi della rete
    torch.save({
        'model_state_dict': self.model.state_dict(),
        'target_model_state_dict': self.target_model.state_dict(),
        'optimizer_state_dict': self.optimizer.state_dict(),
    }, filepath + '.pth')
    
    # Salva replay buffer
    with open(filepath + '_memory.pkl', 'wb') as f:
        pickle.dump(self.memory.buffer, f)
    
    # Salva priorità
    with open(filepath + '_priorities.pkl', 'wb') as f:
        pickle.dump(self.memory.priorities, f)
    
    # Salva epsilon
    with open(filepath + '_epsilon.txt', 'w') as f:
        f.write(str(self.epsilon))

def load(self, filepath):
    # Carica pesi
    checkpoint = torch.load(filepath + '.pth', map_location=self.device)
    self.model.load_state_dict(checkpoint['model_state_dict'])
    self.target_model.load_state_dict(checkpoint['target_model_state_dict'])
    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    # Carica replay buffer e priorità
    with open(filepath + '_memory.pkl', 'rb') as f:
        self.memory.buffer = pickle.load(f)
    
    with open(filepath + '_priorities.pkl', 'rb') as f:
        self.memory.priorities = pickle.load(f)
    
    # Carica epsilon
    with open(filepath + '_epsilon.txt', 'r') as f:
        self.epsilon = float(f.read())
\end{lstlisting}

\section{Implementazione del Crafter Environment}

\subsection{Wrapper Custom (\texttt{crafter\_environment.py})}

Il wrapper estrae lo stato strutturato da Crafter:

\begin{lstlisting}[language=Python]
class CrafterEnvironment:
    def __init__(self):
        self.env = crafter.Env()
        self.state_size = 43
        self.action_size = 17
        
    def extract_state(self, obs, info):
        # Estrai inventario (16 dimensioni)
        inventory = [
            info.get('inventory', {}).get('wood', 0),
            info.get('inventory', {}).get('stone', 0),
            info.get('inventory', {}).get('coal', 0),
            # ... altri 13 item
        ]
        
        # Estrai posizione (2 dimensioni, normalizzate)
        pos_x = info.get('player_pos', [0, 0])[0] / 64.0
        pos_y = info.get('player_pos', [0, 0])[1] / 64.0
        
        # Estrai statistiche vitali (3 dimensioni)
        health = info.get('health', 9) / 9.0
        food = info.get('food', 9) / 9.0
        water = info.get('water', 9) / 9.0
        
        # Estrai achievement (22 dimensioni binarie)
        achievements = [
            1 if info.get('achievements', {}).get('collect_wood', False) else 0,
            1 if info.get('achievements', {}).get('collect_stone', False) else 0,
            # ... altri 20 achievement
        ]
        
        # Combina in vettore a 43 dimensioni
        state = np.array(inventory + [pos_x, pos_y] + [health, food, water] + achievements)
        return state
    
    def step(self, action):
        obs, reward_native, done, info = self.env.step(action)
        state = self.extract_state(obs, info)
        
        # Calcola reward shaped
        reward_shaped = self.calculate_shaped_reward(reward_native, info)
        
        return state, reward_shaped, done, info
    
    def calculate_shaped_reward(self, reward_native, info):
        reward = reward_native
        
        # Bonus risorse
        if self.has_new_resource(info):
            reward += 0.1
        
        # Bonus salute
        health_delta = info.get('health', 9) - self.prev_health
        if health_delta > 0:
            reward += 0.05
        
        # Bonus tier tecnologico
        if self.has_tier_increase(info):
            reward += 0.05
        
        # Bonus nuovi strumenti
        if self.has_new_tool(info):
            reward += 0.02
        
        return reward
\end{lstlisting}

\section{Implementazione del Helper}

\subsection{Client LM Studio (\texttt{crafter\_helper.py})}

Il Helper comunica con LM Studio tramite API REST:

\begin{lstlisting}[language=Python]
class CrafterHelper:
    def __init__(self, base_url="http://127.0.0.1:1234/v1"):
        self.base_url = base_url
        self.model = "llama-3.2-3b-instruct"
        
        # Mapping azioni
        self.ACTION_ID_MAP = {
            "move_left": 0, "move_right": 1,
            "move_up": 2, "move_down": 3,
            "do": 4, "sleep": 5,
            # ... altre azioni
        }
        
        # Correzioni typo comuni
        self.TYPO_MAP = {
            "place_rock": "place_stone",
            "make_pickaxe": "make_wood_pickaxe",
            # ... altre correzioni
        }
    
    def generate_action_sequence(self, state, info, feedback=None):
        # Costruisci prompt
        prompt = self.build_prompt(state, info, feedback)
        
        # Chiamata API LM Studio
        response = requests.post(
            f"{self.base_url}/chat/completions",
            json={
                "model": self.model,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.7,
                "max_tokens": 150
            }
        )
        
        # Estrai testo risposta
        llm_response = response.json()['choices'][0]['message']['content']
        
        # Parsing
        actions = self.parse_actions(llm_response)
        
        return actions
    
    def build_prompt(self, state, info, feedback=None):
        inventory_str = self.format_inventory(info)
        achievements_str = self.format_achievements(info)
        
        prompt = f"""You are an expert Crafter player. Suggest 3-5 actions.

Current State:
- Health: {info.get('health', 9)}/9
- Food: {info.get('food', 9)}/9
- Water: {info.get('water', 9)}/9
- Inventory: {inventory_str}
- Achievements: {achievements_str}

Available actions: [move_left, move_right, move_up, move_down, do, 
sleep, place_stone, place_table, place_furnace, place_plant,
make_wood_pickaxe, make_stone_pickaxe, make_iron_pickaxe,
make_wood_sword, make_stone_sword, make_iron_sword, noop]
"""
        
        if feedback:
            prompt += f"\nReviewer feedback: {feedback}\n"
        
        prompt += "\nOutput format: [action1], [action2], [action3]\nSuggest actions:"
        return prompt
    
    def parse_actions(self, response):
        # Rimuovi tag di ragionamento
        response = re.sub(r"<think>.*?</think>", "", response, flags=re.DOTALL)
        
        # Estrai azioni con regex
        action_names = re.findall(r'\[(.*?)\]', response)
        
        # Limita a 5 azioni
        action_names = action_names[:5]
        
        # Converti a ID, gestendo typo
        action_ids = []
        for name in action_names:
            name = name.strip().lower()
            name = self.TYPO_MAP.get(name, name)
            action_id = self.ACTION_ID_MAP.get(name)
            if action_id is not None:
                action_ids.append(action_id)
        
        return action_ids if action_ids else [16]  # fallback: noop
\end{lstlisting}

\subsection{Meccanismi di Re-planning}

Il Helper implementa logiche per interrompere sequenze:

\begin{lstlisting}[language=Python]
def should_replan(self, prev_info, current_info):
    # Nuovo achievement sbloccato
    prev_achievements = set(k for k, v in prev_info.get('achievements', {}).items() if v)
    curr_achievements = set(k for k, v in current_info.get('achievements', {}).items() if v)
    if curr_achievements > prev_achievements:
        return True, "achievement_unlocked"
    
    # Salute critica
    if current_info.get('health', 9) <= 5:
        return True, "critical_health"
    
    # Salute bassa (< 30%)
    if current_info.get('health', 9) < 3:
        return True, "low_health"
    
    return False, None
\end{lstlisting}

\section{Implementazione del Reviewer}

\subsection{Fine-tuning T5 (\texttt{instructor\_agent.py})}

Il Reviewer è basato su T5-small fine-tuned:

\begin{lstlisting}[language=Python]
class InstructorAgent:
    def __init__(self, model_path="reviewer_retrained"):
        self.tokenizer = T5Tokenizer.from_pretrained(model_path)
        self.model = T5ForConditionalGeneration.from_pretrained(model_path)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        self.model.eval()
    
    def generate_suggestion(self, state_description, helper_suggestion):
        # Costruisci input
        input_text = f"State: {state_description}\nSuggestion: {helper_suggestion}"
        
        # Tokenize
        inputs = self.tokenizer(
            input_text,
            return_tensors="pt",
            max_length=512,
            truncation=True
        ).to(self.device)
        
        # Generate feedback
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=128,
                num_beams=4,
                early_stopping=True
            )
        
        # Decode
        feedback = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return feedback
\end{lstlisting}

\subsection{Training del Reviewer (\texttt{reviewer\_fine\_tuning.py})}

Il fine-tuning utilizza il dataset generato:

\begin{lstlisting}[language=Python]
def train_reviewer(dataset_path, output_dir, epochs=5):
    # Carica dataset
    with open(dataset_path, 'r') as f:
        data = [json.loads(line) for line in f]
    
    # Split train/validation
    train_data = data[:int(0.8 * len(data))]
    val_data = data[int(0.8 * len(data)):]
    
    # Inizializza modello
    tokenizer = T5Tokenizer.from_pretrained("t5-small")
    model = T5ForConditionalGeneration.from_pretrained("t5-small")
    
    # Training arguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=epochs,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        learning_rate=5e-5,
        warmup_steps=500,
        weight_decay=0.01,
        logging_steps=100,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
    )
    
    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
    )
    
    # Train
    trainer.train()
    
    # Save
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
\end{lstlisting}

\section{Loop di Training Integrato}

\subsection{Training HeRoN Completo (\texttt{heron\_training.py})}

Il training integrato coordina tutti i componenti:

\begin{lstlisting}[language=Python]
def train_heron(episodes=1000, max_steps=1000):
    # Inizializza componenti
    env = CrafterEnvironment()
    agent = DQNAgent(state_size=43, action_size=17)
    helper = CrafterHelper()
    
    # Carica Reviewer se disponibile
    try:
        reviewer = InstructorAgent("reviewer_retrained")
        reviewer_available = True
    except:
        reviewer_available = False
    
    # Parametri
    threshold = 1.0
    threshold_decay = 0.01
    
    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        action_sequence = []
        sequence_index = 0
        
        for step in range(max_steps):
            # Decide se consultare LLM
            if len(action_sequence) == 0 or sequence_index >= len(action_sequence):
                if random.random() > threshold and episode < 600:
                    # Workflow Helper-Reviewer
                    sequence = helper.generate_action_sequence(state, env.info)
                    
                    if reviewer_available:
                        state_desc = env.format_state_description()
                        feedback = reviewer.generate_suggestion(state_desc, str(sequence))
                        sequence = helper.generate_action_sequence(state, env.info, feedback)
                    
                    action_sequence = sequence
                    sequence_index = 0
                else:
                    # Solo DQN
                    action = agent.act(state, env)
            else:
                # Esegui prossima azione della sequenza
                action = action_sequence[sequence_index]
                sequence_index += 1
            
            # Esegui azione
            next_state, reward, done, info = env.step(action)
            
            # Memorizza transizione
            agent.remember(state, action, reward, next_state, done)
            
            # Train DQN
            if len(agent.memory) > agent.batch_size:
                agent.train_step(agent.batch_size)
            
            # Aggiorna target network
            if step % 1000 == 0:
                agent.update_target_network()
            
            # Check re-planning
            should_replan, reason = helper.should_replan(env.prev_info, info)
            if should_replan and sequence_index < len(action_sequence):
                action_sequence = []
                sequence_index = 0
            
            state = next_state
            total_reward += reward
            
            if done:
                break
        
        # Aggiorna epsilon e threshold
        agent.update_epsilon()
        if episode < 100:
            threshold = max(0, threshold - threshold_decay)
        
        # Salva checkpoint
        if (episode + 1) % 50 == 0:
            agent.save(f"checkpoints/heron_ep{episode+1}")
        
        print(f"Episode {episode+1}: Reward={total_reward:.2f}, "
              f"Achievements={len(env.achievements_unlocked)}")
    
    # Salva modello finale
    agent.save("models/crafter_heron_final")
\end{lstlisting}

\section{Sistema di Valutazione}

\subsection{Metrics Collection (\texttt{evaluation\_system.py})}

Il sistema raccoglie metriche durante il training e la valutazione:

\begin{lstlisting}[language=Python]
class EvaluationSystem:
    def __init__(self):
        self.metrics = {
            'episodes': [],
            'rewards': [],
            'achievements_count': [],
            'achievements_unlocked': [],
            'episode_length': []
        }
    
    def record_episode(self, episode, reward, achievements, length):
        self.metrics['episodes'].append(episode)
        self.metrics['rewards'].append(reward)
        self.metrics['achievements_count'].append(len(achievements))
        self.metrics['achievements_unlocked'].append(list(achievements))
        self.metrics['episode_length'].append(length)
    
    def save_metrics(self, filepath):
        df = pd.DataFrame(self.metrics)
        df.to_csv(filepath, index=False)
    
    def generate_plots(self, output_dir):
        # Plot reward nel tempo
        plt.figure(figsize=(10, 6))
        plt.plot(self.metrics['episodes'], self.metrics['rewards'])
        plt.xlabel('Episode')
        plt.ylabel('Total Reward')
        plt.title('Training Progress - Reward')
        plt.savefig(f"{output_dir}/reward_plot.png")
        
        # Plot achievement count
        plt.figure(figsize=(10, 6))
        plt.plot(self.metrics['episodes'], self.metrics['achievements_count'])
        plt.xlabel('Episode')
        plt.ylabel('Achievements Unlocked')
        plt.title('Training Progress - Achievements')
        plt.savefig(f"{output_dir}/achievements_plot.png")
\end{lstlisting}

Questa implementazione fornisce un sistema completo per il training, valutazione e analisi dell'architettura HeRoN applicata a Crafter.
