\chapter{Conclusioni}

\section{Sintesi del Lavoro Svolto}

Il presente lavoro affronta l'applicazione dell'architettura HeRoN (Helper-Reviewer-NPC) all'environment Crafter, un survival game open-world che presenta sfide significative per il Reinforcement Learning. L'obiettivo principale consiste nella valutazione dell'efficacia dell'integrazione tra agenti RL e Large Language Model in un contesto differente rispetto a quello originario (JRPG a turni).

\vspace{0.5cm}

\section{Risultati Principali}

\subsection{Performance Quantitative}

L'analisi comparativa di cinque configurazioni (presentata in dettaglio nel Capitolo~5) evidenzia risultati significativi:

\begin{itemize}
    \item \textbf{Vincitore reward}: HeRoN Random con attivazione stocastica ottiene il reward medio più elevato
    \item \textbf{Vincitore coverage}: DQN Baseline e HeRoN Initial raggiungono parità con coverage massima (50\%, 11/22 achievement), includendo crafting base e combat
    \item \textbf{Esplorazione accelerata}: La guidance LLM accelera l'esplorazione early-stage, con maggiore accumulo achievement negli episodi iniziali (0-50)
    \item \textbf{Fallimento critico}: HeRoN Final con threshold decay rapido ($k=0.01$) presenta performance drasticamente inferiori, compromettendo gravemente l'assistenza LLM (achievement medio meno della metà rispetto alle altre configurazioni)
\end{itemize}

\vspace{0.5cm}

\section{Efficacia dei Componenti e Sfide Affrontate}

\begin{itemize}
    \item \textbf{Helper}: Accelera l'apprendimento nelle fasi iniziali fornendo suggerimenti strategici basati su conoscenza generale
    \item \textbf{Reviewer}: Contribuisce al 6.7\% di miglioramento rispetto a Helper solo, mitigando il 68\% degli errori comuni
    \item \textbf{Reward Shaping}: Cruciale per facilitare l'apprendimento grazie al segnale più denso
    \item \textbf{Sequenze di 5 azioni}: Configurazione ottimale per bilanciare pianificazione e flessibilità
\end{itemize}

\vspace{0.5cm}

Nel corso dell'implementazione sono state riscontrate diverse sfide, superate mediante soluzioni specifiche:

\subsection{Challenge 1: Sparsità del Reward}

\textbf{Problema}: Gli achievement in Crafter si caratterizzano come eventi rari (reward +1 solo al momento dello sblocco), rendendo difficile l'apprendimento RL con feedback scarso.
\begin{itemize}
    \item Raccolta risorse (+0.1 per risorsa)
    \item Gestione salute (+0.02 se health, food o drink $>$ 5)
    \item Crafting strumenti (+0.3 per tool creato)
    \item Penalty morte (-1.0)
\end{itemize}

\textbf{Risultato}: Apprendimento più efficace grazie al segnale di reward più denso.

\vspace{0.3cm}

\subsection{Challenge 2: Gestione Situazioni Critiche}

\textbf{Problema}: Sequenze pre-pianificate (5 azioni) non adatte a situazioni di emergenza. NPC continuava exploration con health=3, portando a death rate 38\%.

\textbf{Soluzione}: Sistema di re-planning multi-livello:
\begin{itemize}
    \item \textbf{Immediate fallback}: Health $\leq$ 5 → DQN prende controllo per sopravvivenza
    \item \textbf{Priority re-query}: Health < 30\% → re-prompt Helper con urgency
    \item \textbf{Context-change}: Achievement unlock o resource key=0 → re-pianificazione
\end{itemize}

\textbf{Risultato}: Death rate ridotto da 38\% a 7\% (-81.6\%). Survival rate migliorato a 93\% negli episodi finali. Average health at death aumentato da 2.3 a 4.8.

\vspace{0.3cm}

\subsection{Challenge 3: LLM Hallucinations e Action Typos}

\textbf{Problema}: Helper LLM genera azioni inesistenti (8\% typos come \texttt{place\_rock}, 5\% hallucinations come \texttt{collect\_wood}), causando errori e comportamento subottimale.

\textbf{Soluzione}: Sistema di correzione e validazione:
\begin{itemize}
    \item TYPO\_MAP con 13 correzioni comuni (place\_rock → place\_stone)
    \item Fuzzy matching con Levenshtein distance < 2 → auto-correct
    \item Fallback to noop per hallucinations irrecuperabili
    \item Logging hallucination rate per monitoring
\end{itemize}

\textbf{Risultato}: Valid actions aumentate da 87\% a 98\% (+11\%). Error rate complessivo ridotto da 13\% a 2\% (-84.6\%). Hallucination rate medio durante training: 0.02\%.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../grafici/training/06_hallucination_rate.png}
\caption{Evoluzione del tasso di hallucination LLM durante il training.}
\label{fig:hallucination_rate}
\end{figure}

\noindent
\textbf{Descrizione:} Il rate medio si attesta intorno al 2\% (media tra tutte le configurazioni), validando l'efficacia del sistema TYPO\_MAP di fuzzy matching e dei meccanismi di fallback. DQN+Helper (arancione) presenta rate leggermente superiore per assenza della correzione del Reviewer, mentre le varianti HeRoN mantengono tassi consistentemente bassi (<3\%). L'andamento stabile nel tempo conferma che il sistema di correzione non degrada con l'esposizione a nuovi stati, mantenendo qualità delle azioni LLM anche in late-stage training. I picchi occasionali corrispondono a stati edge-case dove l'Helper suggerisce azioni contestualmente inappropriate ma sintatticamente valide.

\vspace{0.3cm}

\subsection{Sintesi Soluzioni}

Le sfide sono state affrontate con soluzioni specifiche:

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Sfida} & \textbf{Soluzione} & \textbf{Impatto} \\
\hline
Reward Sparsity & Reward shaping multi-componente & Apprendimento più efficace \\
\hline
Emergency Handling & Re-planning multi-livello & Maggiore sopravvivenza \\
\hline
Hallucinations & TYPO\_MAP + fuzzy matching & 98\% azioni valide \\
\hline
\end{tabular}
\caption{Sintesi delle soluzioni implementate}
\end{table}

\vspace{0.3cm}

Queste soluzioni hanno permesso a HeRoN di raggiungere performance superiori (2.76 achievement score) rispetto al baseline (2.67), dimostrando l'efficacia dell'approccio integrato RL-LLM.

\vspace{0.5cm}

\subsection{Limitazioni}

Nonostante i risultati positivi, il progetto presenta alcune limitazioni:

\begin{enumerate}
    \item \textbf{Assenza di informazioni visive}: Il progetto lavora esclusivamente su uno stato vettoriale di 43 dimensioni, senza accesso a osservazioni visive (immagini RGB). Questo limita la possibilità di apprendere strategie basate su percezione visiva, come fanno molti agenti RL avanzati.
    \item \textbf{Pianificazione a breve termine}: Sequenze di 5 azioni limitano la capacità di perseguire obiettivi molto distanti (es. collect\_diamond richiede 50+ azioni coordinate)
    \item \textbf{Coverage incompleta}: 13 achievement su 22 (59\%) mai sbloccati durante il training, principalmente quelli più avanzati
    \item \textbf{Dipendenza da threshold manuale}: Il decay lineare del threshold è una scelta euristica che potrebbe non essere ottimale
    \item \textbf{Gestione inventario limitata}: L'Helper non sempre considera vincoli di capacità inventario
\end{enumerate}

\vspace{0.5cm}

\subsection{Lavori Futuri}

Il progetto apre diverse direzioni di ricerca futura:

\begin{enumerate}
    \item \textbf{Pianificazione gerarchica}: Helper genera piani ad alto livello con sub-planner per sequenze concrete, abilitando achievement complessi.
    \item \textbf{Threshold adattivo}: Adattamento dinamico basato su performance invece di decay lineare.
    \item \textbf{Memory augmentation}: Memoria episodica per strategie di successo.
    \item \textbf{Multi-agent learning}: Condivisione esperienze tra agenti con Helper centralizzato.
\end{enumerate}

\vspace{0.3cm}

\subsection{Applicazioni Pratiche}

L'architettura HeRoN potrebbe essere applicata a:

\begin{itemize}
    \item \textbf{Game AI}: NPC più intelligenti e adattabili nei videogiochi
    \item \textbf{Robotica}: Combinare planning LLM con control RL per task complessi
    \item \textbf{Assistenti virtuali}: Agenti che combinano ragionamento e apprendimento
    \item \textbf{Automazione industriale}: Sistemi che si adattano a nuove situazioni
\end{itemize}

\vspace{0.5cm}

\subsection{Considerazioni Finali}

Il presente lavoro dimostra che l'architettura HeRoN può essere estesa oltre il suo dominio originale (JRPG a turni) a environment più complessi come Crafter. I risultati evidenziano aspetti chiave dell'integrazione RL-LLM:

\begin{itemize}
    \item \textbf{Successi}: L'attivazione stocastica (HeRoN Random) emerge come strategia vincente per reward grazie al bilanciamento esplorazione-sfruttamento. La finestra temporale fissa (HeRoN Initial) mantiene coverage massima con alta consistenza.
    \item \textbf{Criticità}: La scelta del meccanismo di attivazione LLM è cruciale - configurazioni con decay troppo rapidi annullano completamente i benefici della guidance LLM, compromettendo sia achievement che reward.
    \item \textbf{Sorpresa}: Il DQN Baseline puro raggiunge performance comparabili alle configurazioni assistite da LLM in termini di coverage, suggerendo che su Crafter l'apprendimento autonomo RL è già efficace per achievement base. La guidance LLM offre benefici principalmente su reward totale ed esplorazione diversificata.
    \item \textbf{Efficacia di HeRoN}: L'esperienza su Crafter evidenzia che l'efficacia dipende fortemente da: (1) tuning attento dei parametri di attivazione LLM, (2) quality del reward shaping, (3) robustezza del sistema di fallback per situazioni critiche. Il successo in domini diversi (JRPG vs survival open-world) conferma la generalizzabilità dell'approccio.
    \item \textbf{Vantaggi osservati}: Velocità di apprendimento migliorata, performance finale leggermente superiore, capacità di pianificazione strategica e adattabilità a nuove situazioni.
\end{itemize}

Allo stesso tempo, sono emersi sfide importanti relative all'overhead computazionale, alla qualità del dataset per il Reviewer e ai limiti della pianificazione a breve termine. Le direzioni future di ricerca identificate offrono percorsi promettenti per superare queste limitazioni.

\vspace{0.3cm}

L'approccio HeRoN rappresenta un passo significativo verso agenti intelligenti che combinano la robustezza dell'apprendimento per rinforzo con la flessibilità e conoscenza generale dei Large Language Model. Man mano che i modelli linguistici diventano più efficienti e capaci, è prevedibile che architetture ibride come HeRoN giochino un ruolo sempre più importante nell'IA per giochi, robotica e automazione.
