\chapter{Conclusioni}

\section{Sintesi del Lavoro Svolto}

Il presente lavoro affronta l'applicazione dell'architettura HeRoN (Helper-Reviewer-NPC) all'environment Crafter, un survival game open-world che presenta sfide significative per il Reinforcement Learning. L'obiettivo principale consiste nella valutazione dell'efficacia dell'integrazione tra agenti RL e Large Language Model in un contesto differente rispetto a quello originario (JRPG a turni).

\vspace{0.5cm}

\section{Risultati Principali}

\subsection{Performance Quantitative}

L'analisi comparativa di cinque configurazioni (presentata in dettaglio nel Capitolo~5) evidenzia risultati significativi:

\begin{itemize}
    \item \textbf{Vincitore complessivo}: HeRoN Initial ottiene il miglior reward shaped in training (8.02) e testing (5.21), coverage massima (50\%, 11/22 achievement) mantenuta in entrambe le fasi, e achievement medio più alto
    \item \textbf{Coverage}: In training, HeRoN Initial e HeRoN Random raggiungono parità con coverage massima (50\%, 11/22 achievement). In testing, tutte e tre le varianti HeRoN raggiungono 50\% coverage, mentre DQN Baseline raggiunge solo 36.4\% (8/22)
    \item \textbf{Performance intermedie}: HeRoN Final presenta performance intermedie in training (reward 3.84, coverage 36.4\%), ma raggiunge 50\% coverage in testing, dimostrando buona capacità di generalizzazione
    \item \textbf{Generalizzazione eccellente}: In testing (1500 episodi su 5 run), tutte le varianti HeRoN mantengono o migliorano la coverage, con HeRoN Initial che conserva il vantaggio su tutte le metriche
\end{itemize}

\vspace{0.5cm}

\section{Efficacia dei Componenti e Sfide Affrontate}

\begin{itemize}
    \item \textbf{Helper}: Accelera l'apprendimento nelle fasi iniziali fornendo suggerimenti strategici basati su conoscenza generale
    \item \textbf{Reward Shaping}: Cruciale per facilitare l'apprendimento grazie al segnale più denso
    \item \textbf{Sequenze di 5 azioni}: Configurazione ottimale per bilanciare pianificazione e flessibilità
\end{itemize}

\vspace{0.5cm}

Nel corso dell'implementazione sono state riscontrate diverse sfide, superate mediante soluzioni specifiche:

\subsection{Challenge 1: Sparsità del Reward}

\textbf{Problema}: Gli achievement in Crafter si caratterizzano come eventi rari (reward +1 solo al momento dello sblocco), rendendo difficile l'apprendimento RL con feedback scarso.

\textbf{Soluzione}: Reward shaping multi-componente che fornisce feedback denso:

\begin{itemize}
    \item Raccolta risorse (+0.1 per risorsa)
    \item Gestione salute (+0.02 se health, food o drink $>$ 5)
    \item Crafting strumenti (+0.3 per tool creato)
    \item Penalty morte (-1.0)
\end{itemize}

\textbf{Risultato}: Apprendimento più efficace grazie al segnale di reward più denso.

\vspace{0.3cm}

\subsection{Challenge 2: Gestione Situazioni Critiche}

\textbf{Problema}: Sequenze pre-pianificate (5 azioni) non adatte a situazioni di emergenza. 

\textbf{Soluzione}: Sistema di re-planning multi-livello:

\begin{itemize}
    \item \textbf{Salute critica (health $\leq$ 5)}: Se la salute scende sotto soglia critica, la sequenza viene interrotta e l'Helper si consulta per suggerire azioni di emergenza (mangiare, bere, dormire)
    \item \textbf{Salute bassa (health < 30\%)}: Se la salute è bassa ma non critica, si consulta l'Helper per bilanciare l'esplorazione con la gestione della sopravvivenza
\end{itemize}

\vspace{0.3cm}

\subsection{Challenge 3: LLM Hallucinations e Action Typos}

\textbf{Problema}: Helper LLM genera azioni inesistenti, causando errori e comportamento subottimale.

\textbf{Soluzione}: Sistema di correzione e validazione:
\begin{itemize}
    \item TYPO\_MAP con 13 correzioni comuni (place\_rock → place\_stone)
    \item Fuzzy matching con Levenshtein distance < 2 → auto-correct
\end{itemize}

\vspace{0.3cm}

\subsection{Sintesi Soluzioni}

Le sfide sono state affrontate con soluzioni specifiche:

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Sfida} & \textbf{Soluzione} & \textbf{Impatto} \\
\hline
Reward Sparsity & Reward shaping multi-componente & Apprendimento più efficace \\
\hline
Emergency Handling & Re-planning multi-livello & Maggiore sopravvivenza \\
\hline
Hallucinations & TYPO\_MAP + fuzzy matching & 98\% azioni valide \\
\hline
\end{tabular}
\caption{Sintesi delle soluzioni implementate}
\end{table}

\vspace{0.3cm}


\subsection{Limitazioni}

Nonostante i risultati positivi, il progetto presenta alcune limitazioni:

\begin{enumerate}
    \item \textbf{Assenza di informazioni visive}: Il progetto lavora esclusivamente su uno stato vettoriale di 43 dimensioni, senza accesso a osservazioni visive (immagini RGB). Questo limita la possibilità di apprendere strategie basate su percezione visiva, come fanno molti agenti RL avanzati.
    \item \textbf{Pianificazione a breve termine}: Sequenze di 5 azioni limitano la capacità di perseguire obiettivi molto distanti (es. collect\_diamond richiede 50+ azioni coordinate)
    \item \textbf{Coverage incompleta}: Le varianti HeRoN raggiungono 11 achievement su 22 (50\% coverage) sia in training che in testing, mentre DQN Baseline raggiunge 8/22 (36.4\%) in testing. Gli achievement più avanzati (es. diamanti, crafting complesso) richiedono catene di azioni molto lunghe non ancora supportate dalla pianificazione a breve termine
\end{enumerate}

\vspace{0.5cm}

\subsection{Lavori Futuri}

Il progetto apre diverse direzioni di ricerca futura:

\begin{enumerate}
    \item \textbf{Pianificazione gerarchica}: Helper genera piani ad alto livello con sub-planner per sequenze concrete, abilitando achievement complessi.
    \item \textbf{Memory augmentation}: Memoria episodica per strategie di successo.
    \item \textbf{Multi-agent learning}: Condivisione esperienze tra agenti con Helper centralizzato.
\end{enumerate}

\vspace{0.3cm}

\subsection{Applicazioni Pratiche}

L'architettura HeRoN potrebbe essere applicata a:

\begin{itemize}
    \item \textbf{Game AI}: NPC più intelligenti e adattabili nei videogiochi
    \item \textbf{Robotica}: Combinare planning LLM con control RL per task complessi
    \item \textbf{Assistenti virtuali}: Agenti che combinano ragionamento e apprendimento
    \item \textbf{Automazione industriale}: Sistemi che si adattano a nuove situazioni
\end{itemize}

\vspace{0.5cm}

\subsection{Considerazioni Finali}

Il presente lavoro dimostra che l'architettura HeRoN può essere estesa oltre il suo dominio originale (JRPG a turni) a environment più complessi come Crafter. I risultati evidenziano aspetti chiave dell'integrazione RL-LLM:

\begin{itemize}
    \item \textbf{Successi}: L'architettura HeRoN Initial emerge come vincitore complessivo, ottimizzando simultaneamente reward, coverage e achievement medio. La semplicità e consistenza della guidance LLM nei primi 100 step di ogni episodio si rivela la scelta ottimale.
    \item \textbf{Efficacia di HeRoN}: L'esperienza su Crafter evidenzia che l'efficacia dipende fortemente da tre fattori chiave:
    \begin{itemize}
        \item Strategia di attivazione LLM (la strategia Initial risulta ottimale rispetto a Random e Final)
        \item Qualità del reward shaping multi-componente per guidare l'apprendimento
        \item Robustezza del sistema di re-planning per gestire situazioni critiche
    \end{itemize}
    Il successo in domini diversi (JRPG a turni vs survival open-world) conferma la generalizzabilità dell'approccio.
    \item \textbf{Vantaggi osservati}: Velocità di apprendimento significativamente migliorata, performance finale superiore su tutte le metriche principali, capacità di pianificazione strategica e coverage achievement migliorata del 37\% rispetto a DQN baseline (50.0\% vs 36.4\% in testing). In testing, HeRoN Random ottiene una media di 240 unlock per run, contro i 75 di DQN Base, triplicando le prestazioni esplorative.
\end{itemize}

Allo stesso tempo, sono emersi sfide importanti relative all'overhead computazionale, alla qualità del dataset per il Reviewer e ai limiti della pianificazione a breve termine. Le direzioni future di ricerca identificate offrono percorsi promettenti per superare queste limitazioni.

\vspace{0.3cm}

L'approccio HeRoN rappresenta un passo significativo verso agenti intelligenti che combinano la robustezza dell'apprendimento per rinforzo con la flessibilità e conoscenza generale dei Large Language Model. Man mano che i modelli linguistici diventano più efficienti e capaci, è prevedibile che architetture ibride come HeRoN giochino un ruolo sempre più importante nell'IA per giochi, robotica e automazione.
