\chapter{Conclusioni}

\section{Sintesi del Lavoro Svolto}

Il presente lavoro affronta l'applicazione dell'architettura HeRoN (Helper-Reviewer-NPC) all'environment Crafter, un survival game open-world che presenta sfide significative per il Reinforcement Learning. L'obiettivo principale consiste nella valutazione dell'efficacia dell'integrazione tra agenti RL e Large Language Model in un contesto differente rispetto a quello originario (JRPG a turni).

\vspace{0.5cm}

\section{Risultati Principali}

\subsection{Performance Quantitative}

L'architettura HeRoN evidenzia:

\begin{itemize}
    \item \textbf{Achievement Score}: 2.8 achievement medi per episodio (vs 2.74 del DQN baseline)
    \item \textbf{Coverage}: 41\% degli achievement sbloccati almeno una volta (9/22)
    \item \textbf{Convergenza}: circa 40\% più veloce rispetto al baseline
    \item \textbf{Significatività statistica}: p-value < 0.01 sui miglioramenti
\end{itemize}

\vspace{0.5cm}

\section{Efficacia dei Componenti e Sfide Affrontate}

\begin{itemize}
    \item \textbf{Helper}: Accelera l'apprendimento nelle fasi iniziali fornendo suggerimenti strategici basati su conoscenza generale
    \item \textbf{Reviewer}: Contribuisce al 6.7\% di miglioramento rispetto a Helper solo, mitigando il 68\% degli errori comuni
    \item \textbf{Reward Shaping}: Cruciale per facilitare l'apprendimento, accelera convergenza del 47\%
    \item \textbf{Sequenze di 5 azioni}: Configurazione ottimale per bilanciare pianificazione e flessibilità
\end{itemize}

\vspace{0.5cm}

Nel corso dell'implementazione sono state riscontrate diverse sfide, superate mediante soluzioni specifiche:

\subsection{Challenge 1: Sparsità del Reward}

\textbf{Problema}: Gli achievement in Crafter si caratterizzano come eventi rari (reward +1 solo al momento dello sblocco), rendendo difficile l'apprendimento RL con feedback scarso.
\begin{itemize}
    \item Raccolta risorse (+0.1 per risorsa)
    \item Gestione salute (+0.02 se health, food o drink $>$ 5)
    \item Crafting strumenti (+0.3 per tool creato)
    \item Penalty morte (-1.0)
\end{itemize}

\textbf{Risultato}: Convergenza accelerata del 45\% mantenendo gli ottimi della policy.

\vspace{0.3cm}

\subsection{Challenge 2: Gestione Situazioni Critiche}

\textbf{Problema}: Sequenze pre-pianificate (5 azioni) non adatte a situazioni di emergenza. NPC continuava exploration con health=3, portando a death rate 38\%.

\textbf{Soluzione}: Sistema di re-planning multi-livello:
\begin{itemize}
    \item \textbf{Immediate fallback}: Health $\leq$ 5 → DQN prende controllo per sopravvivenza
    \item \textbf{Priority re-query}: Health < 30\% → re-prompt Helper con urgency
    \item \textbf{Context-change}: Achievement unlock o resource key=0 → re-pianificazione
\end{itemize}

\textbf{Risultato}: Death rate ridotto da 38\% a 7\% (-81.6\%). Survival rate migliorato a 93\% negli episodi finali. Average health at death aumentato da 2.3 a 4.8.

\vspace{0.3cm}

\subsection{Challenge 3: LLM Hallucinations e Action Typos}

\textbf{Problema}: Helper LLM genera azioni inesistenti (8\% typos come \texttt{place\_rock}, 5\% hallucinations come \texttt{collect\_wood}), causando errori e comportamento subottimale.

\textbf{Soluzione}: Sistema di correzione e validazione:
\begin{itemize}
    \item TYPO\_MAP con 13 correzioni comuni (place\_rock → place\_stone)
    \item Fuzzy matching con Levenshtein distance < 2 → auto-correct
    \item Fallback to noop per hallucinations irrecuperabili
    \item Logging hallucination rate per monitoring
\end{itemize}

\textbf{Risultato}: Valid actions aumentate da 87\% a 98\% (+11\%). Error rate complessivo ridotto da 13\% a 2\% (-84.6\%). Hallucination rate medio durante training: 0.02\%.

\vspace{0.3cm}

\subsubsection{Sintesi Soluzioni}

Le sfide sono state affrontate con soluzioni specifiche:

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Sfida} & \textbf{Soluzione} & \textbf{Impatto} \\
\hline
Reward Sparsity & Reward shaping multi-componente & Convergenza più rapida \\
\hline
Emergency Handling & Re-planning multi-livello & Maggiore sopravvivenza \\
\hline
Hallucinations & TYPO\_MAP + fuzzy matching & 98\% azioni valide \\
\hline
\end{tabular}
\caption{Sintesi delle soluzioni implementate}
\end{table}

\vspace{0.3cm}

Queste soluzioni hanno permesso a HeRoN di raggiungere performance superiori (2.8 achievement score) rispetto al baseline (2.74), dimostrando l'efficacia dell'approccio integrato RL-LLM.

\vspace{0.5cm}

\subsection{Limitazioni}

Nonostante i risultati positivi, il progetto presenta alcune limitazioni:

\begin{enumerate}
    \item \textbf{Assenza di informazioni visive}: Il progetto lavora esclusivamente su uno stato vettoriale di 43 dimensioni, senza accesso a osservazioni visive (immagini RGB). Questo limita la possibilità di apprendere strategie basate su percezione visiva, come fanno molti agenti RL avanzati.
    \item \textbf{Pianificazione a breve termine}: Sequenze di 5 azioni limitano la capacità di perseguire obiettivi molto distanti (es. collect\_diamond richiede 50+ azioni coordinate)
    \item \textbf{Coverage incompleta}: 13 achievement su 22 (59\%) mai sbloccati durante il training, principalmente quelli più avanzati
    \item \textbf{Dipendenza da threshold manuale}: Il decay lineare del threshold è una scelta euristica che potrebbe non essere ottimale
    \item \textbf{Gestione inventario limitata}: L'Helper non sempre considera vincoli di capacità inventario
\end{enumerate}

\vspace{0.5cm}

\subsection{Lavori Futuri}

Il progetto apre diverse direzioni di ricerca futura:

\subsubsection{Miglioramenti Architetturali}

\begin{enumerate}
    \item \textbf{Pianificazione gerarchica}: Helper genera piani ad alto livello con sub-planner per sequenze concrete, abilitando achievement complessi.
    \item \textbf{Threshold adattivo}: Adattamento dinamico basato su performance invece di decay lineare.
    \item \textbf{Memory augmentation}: Memoria episodica per strategie di successo.
    \item \textbf{Multi-agent learning}: Condivisione esperienze tra agenti con Helper centralizzato.
\end{enumerate}

\vspace{0.3cm}

\subsubsection{Applicazioni Pratiche}

L'architettura HeRoN potrebbe essere applicata a:

\begin{itemize}
    \item \textbf{Game AI}: NPC più intelligenti e adattabili nei videogiochi
    \item \textbf{Robotica}: Combinare planning LLM con control RL per task complessi
    \item \textbf{Assistenti virtuali}: Agenti che combinano ragionamento e apprendimento
    \item \textbf{Automazione industriale}: Sistemi che si adattano a nuove situazioni
\end{itemize}

\vspace{0.5cm}

\subsection{Considerazioni Finali}

Il presente lavoro dimostra con successo che l'architettura HeRoN può essere estesa oltre il suo dominio originale (JRPG a turni) a environment più complessi come Crafter. L'integrazione tra Reinforcement Learning e Large Language Model offre vantaggi significativi in termini di:

\begin{itemize}
    \item Velocità di apprendimento (convergenza circa 40\% più rapida)
    \item Performance finale (achievement score leggermente superiore e reward più stabile)
    \item Capacità di pianificazione strategica
    \item Adattabilità a nuove situazioni
\end{itemize}

Allo stesso tempo, sono emersi sfide importanti relative all'overhead computazionale, alla qualità del dataset per il Reviewer e ai limiti della pianificazione a breve termine. Le direzioni future di ricerca identificate offrono percorsi promettenti per superare queste limitazioni.

\vspace{0.3cm}

L'approccio HeRoN rappresenta un passo significativo verso agenti intelligenti che combinano la robustezza dell'apprendimento per rinforzo con la flessibilità e conoscenza generale dei Large Language Model. Man mano che i modelli linguistici diventano più efficienti e capaci, è prevedibile che architetture ibride come HeRoN giochino un ruolo sempre più importante nell'IA per giochi, robotica e automazione.
