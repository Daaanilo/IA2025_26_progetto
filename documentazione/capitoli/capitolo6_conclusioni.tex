\chapter{Conclusioni}

\section{Sintesi del Lavoro Svolto}

Il presente lavoro affronta l'applicazione dell'architettura HeRoN (Helper-Reviewer-NPC) all'environment Crafter, un survival game open-world che presenta sfide significative per il Reinforcement Learning. L'obiettivo principale consiste nella valutazione dell'efficacia dell'integrazione tra agenti RL e Large Language Model in un contesto differente rispetto a quello originario (JRPG a turni).

\vspace{0.5cm}

\section{Risultati Principali}

\subsection{Performance Quantitative}

L'analisi comparativa di cinque configurazioni (presentata in dettaglio nel Capitolo~5) evidenzia risultati significativi:

\begin{itemize}
    \item \textbf{Vincitore complessivo}: HeRoN Initial con strategia fixed-window ottiene il miglior reward shaped (8.02), coverage massima (50\%, 11/22 achievement), achievement medio più alto (2.65) e maggior numero di unlock totali (802)
    \item \textbf{Coverage}: HeRoN Initial e HeRoN Random raggiungono parità con coverage massima (50\%, 11/22 achievement), includendo crafting base e combat. DQN Baseline raggiunge solo 18.2\% (4/22)
    \item \textbf{Esplorazione accelerata}: La guidance LLM fixed-window accelera significativamente l'esplorazione, con HeRoN Initial che supera DQN baseline del +331\% su reward e +546\% su achievement medio
    \item \textbf{Performance intermedie}: HeRoN Final con threshold decay k=0.01 presenta performance intermedie (reward 3.84, coverage 36.4\%, 228 unlock), superiori a DQN baseline ma inferiori a HeRoN Initial
    \item \textbf{Generalizzazione}: In testing (senza LLM), HeRoN Initial mantiene vantaggio su DQN baseline (+192\% achievement medio, +344\% reward), confermando apprendimento robusto
\end{itemize}

\vspace{0.5cm}

\section{Efficacia dei Componenti e Sfide Affrontate}

\begin{itemize}
    \item \textbf{Helper}: Accelera l'apprendimento nelle fasi iniziali fornendo suggerimenti strategici basati su conoscenza generale
    \item \textbf{Reviewer}: Contribuisce al 6.7\% di miglioramento rispetto a Helper solo, mitigando il 68\% degli errori comuni
    \item \textbf{Reward Shaping}: Cruciale per facilitare l'apprendimento grazie al segnale più denso
    \item \textbf{Sequenze di 5 azioni}: Configurazione ottimale per bilanciare pianificazione e flessibilità
\end{itemize}

\vspace{0.5cm}

Nel corso dell'implementazione sono state riscontrate diverse sfide, superate mediante soluzioni specifiche:

\subsection{Challenge 1: Sparsità del Reward}

\textbf{Problema}: Gli achievement in Crafter si caratterizzano come eventi rari (reward +1 solo al momento dello sblocco), rendendo difficile l'apprendimento RL con feedback scarso.
\begin{itemize}
    \item Raccolta risorse (+0.1 per risorsa)
    \item Gestione salute (+0.02 se health, food o drink $>$ 5)
    \item Crafting strumenti (+0.3 per tool creato)
    \item Penalty morte (-1.0)
\end{itemize}

\textbf{Risultato}: Apprendimento più efficace grazie al segnale di reward più denso.

\vspace{0.3cm}

\subsection{Challenge 2: Gestione Situazioni Critiche}

\textbf{Problema}: Sequenze pre-pianificate (5 azioni) non adatte a situazioni di emergenza. NPC continuava exploration con health=3, portando a death rate 38\%.

\textbf{Soluzione}: Sistema di re-planning multi-livello:
\begin{itemize}
    \item \textbf{Immediate fallback}: Health $\leq$ 5 → DQN prende controllo per sopravvivenza
    \item \textbf{Priority re-query}: Health < 30\% → re-prompt Helper con urgency
    \item \textbf{Context-change}: Achievement unlock o resource key=0 → re-pianificazione
\end{itemize}

\textbf{Risultato}: Death rate ridotto da 38\% a 7\% (-81.6\%). Survival rate migliorato a 93\% negli episodi finali. Average health at death aumentato da 2.3 a 4.8.

\vspace{0.3cm}

\subsection{Challenge 3: LLM Hallucinations e Action Typos}

\textbf{Problema}: Helper LLM genera azioni inesistenti (8\% typos come \texttt{place\_rock}, 5\% hallucinations come \texttt{collect\_wood}), causando errori e comportamento subottimale.

\textbf{Soluzione}: Sistema di correzione e validazione:
\begin{itemize}
    \item TYPO\_MAP con 13 correzioni comuni (place\_rock → place\_stone)
    \item Fuzzy matching con Levenshtein distance < 2 → auto-correct
    \item Fallback to noop per hallucinations irrecuperabili
    \item Logging hallucination rate per monitoring
\end{itemize}

\textbf{Risultato}: Valid actions aumentate da 87\% a 98\% (+11\%). Error rate complessivo ridotto da 13\% a 2\% (-84.6\%). Hallucination rate medio durante training: 0.02\%.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../grafici/training/06_hallucination_rate.png}
\caption{Evoluzione del tasso di hallucination LLM durante il training.}
\label{fig:hallucination_rate}
\end{figure}

\noindent
\textbf{Descrizione:} Il rate medio si attesta intorno al 2\% (media tra tutte le configurazioni), validando l'efficacia del sistema TYPO\_MAP di fuzzy matching e dei meccanismi di fallback. DQN+Helper (arancione) presenta rate leggermente superiore per assenza della correzione del Reviewer, mentre le varianti HeRoN mantengono tassi consistentemente bassi (<3\%). L'andamento stabile nel tempo conferma che il sistema di correzione non degrada con l'esposizione a nuovi stati, mantenendo qualità delle azioni LLM anche in late-stage training. I picchi occasionali corrispondono a stati edge-case dove l'Helper suggerisce azioni contestualmente inappropriate ma sintatticamente valide.

\vspace{0.3cm}

\subsection{Sintesi Soluzioni}

Le sfide sono state affrontate con soluzioni specifiche:

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\rowcolor{gray!20}
\textbf{Sfida} & \textbf{Soluzione} & \textbf{Impatto} \\
\hline
Reward Sparsity & Reward shaping multi-componente & Apprendimento più efficace \\
\hline
Emergency Handling & Re-planning multi-livello & Maggiore sopravvivenza \\
\hline
Hallucinations & TYPO\_MAP + fuzzy matching & 98\% azioni valide \\
\hline
\end{tabular}
\caption{Sintesi delle soluzioni implementate}
\end{table}

\vspace{0.3cm}

Queste soluzioni hanno permesso a HeRoN Initial di raggiungere performance superiori (2.65 achievement medio, 8.02 reward shaped) rispetto al DQN baseline (0.41 achievement medio, 1.86 reward shaped), con miglioramento del +546\% su achievement e +331\% su reward, dimostrando l'efficacia dell'approccio integrato RL-LLM.

\vspace{0.5cm}

\subsection{Limitazioni}

Nonostante i risultati positivi, il progetto presenta alcune limitazioni:

\begin{enumerate}
    \item \textbf{Assenza di informazioni visive}: Il progetto lavora esclusivamente su uno stato vettoriale di 43 dimensioni, senza accesso a osservazioni visive (immagini RGB). Questo limita la possibilità di apprendere strategie basate su percezione visiva, come fanno molti agenti RL avanzati.
    \item \textbf{Pianificazione a breve termine}: Sequenze di 5 azioni limitano la capacità di perseguire obiettivi molto distanti (es. collect\_diamond richiede 50+ azioni coordinate)
    \item \textbf{Coverage incompleta}: HeRoN Initial raggiunge 11 achievement su 22 (50\% coverage), mentre DQN Baseline solo 4/22 (18.2\%). Gli achievement più avanzati richiedono catene complesse non ancora supportate
    \item \textbf{Dipendenza da threshold manuale}: Il decay lineare del threshold è una scelta euristica che potrebbe non essere ottimale
    \item \textbf{Gestione inventario limitata}: L'Helper non sempre considera vincoli di capacità inventario
\end{enumerate}

\vspace{0.5cm}

\subsection{Lavori Futuri}

Il progetto apre diverse direzioni di ricerca futura:

\begin{enumerate}
    \item \textbf{Pianificazione gerarchica}: Helper genera piani ad alto livello con sub-planner per sequenze concrete, abilitando achievement complessi.
    \item \textbf{Threshold adattivo}: Adattamento dinamico basato su performance invece di decay lineare.
    \item \textbf{Memory augmentation}: Memoria episodica per strategie di successo.
    \item \textbf{Multi-agent learning}: Condivisione esperienze tra agenti con Helper centralizzato.
\end{enumerate}

\vspace{0.3cm}

\subsection{Applicazioni Pratiche}

L'architettura HeRoN potrebbe essere applicata a:

\begin{itemize}
    \item \textbf{Game AI}: NPC più intelligenti e adattabili nei videogiochi
    \item \textbf{Robotica}: Combinare planning LLM con control RL per task complessi
    \item \textbf{Assistenti virtuali}: Agenti che combinano ragionamento e apprendimento
    \item \textbf{Automazione industriale}: Sistemi che si adattano a nuove situazioni
\end{itemize}

\vspace{0.5cm}

\subsection{Considerazioni Finali}

Il presente lavoro dimostra che l'architettura HeRoN può essere estesa oltre il suo dominio originale (JRPG a turni) a environment più complessi come Crafter. I risultati evidenziano aspetti chiave dell'integrazione RL-LLM:

\begin{itemize}
    \item \textbf{Successi}: L'architettura HeRoN Initial con strategia fixed-window emerge come vincitore complessivo, ottimizzando simultaneamente reward (+331\% vs baseline), coverage (50\% vs 18.2\%) e achievement medio (+546\% vs baseline). La semplicità e consistenza della guidance LLM nei primi 100 step di ogni episodio si rivela la scelta ottimale.
    \item \textbf{Criticità}: La scelta del meccanismo di attivazione LLM è cruciale - la strategia fixed-window (Initial) supera significativamente sia l'attivazione stocastica (Random) che il decay adattivo (Final). Su Crafter, il DQN baseline puro senza LLM raggiunge solo 18.2\% coverage (4/22 achievement), confermando che la guidance LLM è fondamentale per esplorazione efficace.
    \item \textbf{Generalizzazione}: I risultati di testing confermano che la policy appresa con guidance LLM mantiene performance superiori (+192\% achievement, +344\% reward per HeRoN Initial vs baseline) anche senza LLM in fase di inference, dimostrando apprendimento robusto e generalizzabile.
    \item \textbf{Efficacia di HeRoN}: L'esperienza su Crafter evidenzia che l'efficacia dipende fortemente da: (1) strategia fixed-window per guidance consistente, (2) quality del reward shaping multi-componente, (3) robustezza del sistema di re-planning e fallback per situazioni critiche. Il successo in domini diversi (JRPG vs survival open-world) conferma la generalizzabilità dell'approccio.
    \item \textbf{Vantaggi osservati}: Velocità di apprendimento significativamente migliorata, performance finale superiore su tutte le metriche principali, capacità di pianificazione strategica e coverage achievement triplicata rispetto a baseline puro.
\end{itemize}

Allo stesso tempo, sono emersi sfide importanti relative all'overhead computazionale, alla qualità del dataset per il Reviewer e ai limiti della pianificazione a breve termine. Le direzioni future di ricerca identificate offrono percorsi promettenti per superare queste limitazioni.

\vspace{0.3cm}

L'approccio HeRoN rappresenta un passo significativo verso agenti intelligenti che combinano la robustezza dell'apprendimento per rinforzo con la flessibilità e conoscenza generale dei Large Language Model. Man mano che i modelli linguistici diventano più efficienti e capaci, è prevedibile che architetture ibride come HeRoN giochino un ruolo sempre più importante nell'IA per giochi, robotica e automazione.
